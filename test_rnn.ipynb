{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IQ TEST SEQUENCER BOT - RNN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import random\n",
    "from keras.layers import Bidirectional\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  stem               options            category    id\n",
      "0           7,9,-1,5,?        [4, 2, -1, -3]            sequence     0\n",
      "1        3,2,5/3,3/2,?  [1/4, 7/5, 3/4, 2/5]            sequence     1\n",
      "2           1,2,5,26,?    [34, 841, 677, 37]            sequence     2\n",
      "3            2,12,30,?      [50, 65, 75, 56]            sequence     3\n",
      "4        2,1,2/3,1/2,?  [3/4, 1/4, 2/5, 5/6]            sequence     4\n",
      "...                ...                   ...                 ...   ...\n",
      "1071  20 22 25 30 37 ?                    []  sequence-reasoning  1090\n",
      "1072        0 1 3 10 ?                    []  sequence-reasoning  1091\n",
      "1073       5 15 10 215                    []  sequence-reasoning  1092\n",
      "1074        1 2 5 29 ?    [34, 841, 866, 37]  sequence-reasoning  1093\n",
      "1075         2 12 30 ?      [50, 65, 75, 56]  sequence-reasoning  1094\n",
      "\n",
      "[1076 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "seqdataIn = pd.read_json('data/seq-public.json', orient='records')\n",
    "print(seqdataIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     answer                                               hint\n",
      "0       [4]      A<sub>n+1</sub>=2<sup>5-n</sup>-A<sub>n</sub>\n",
      "1       [2]  3/1, 4/2, 5/3, 6/4\\n\\nA<sub>n+1</sub>=A<sub>n<...\n",
      "2       [3]        A<sub>n+1</sub>=A<sub>n</sub><sup>2</sup>+1\n",
      "3       [4]                           A<sub>n</sub>=2n\\*(2n-1)\n",
      "4       [3]             4/2, 4/4, 4/6, 4/8\\n\\nA<sub>n</sub>=2n\n",
      "...     ...                                                ...\n",
      "1090     48                A<sub>n+1</sub>-A<sub>n</sub>=P(n);\n",
      "1091    102           A<sub>n\\*2</sub>=A<sub>n\\*2-1</sub>^2+2;\n",
      "1092   -115   A<sub>n</sub>=A<sub>n-2</sub>^2-A<sub>n-1</sub>;\n",
      "1093    [3]  A<sub>n</sub>=A<sub>n-1</sub>^2+A<sub>n-2</sub...\n",
      "1094    [4]                      A<sub>n</sub>=(2\\*n-1)\\*2\\*n;\n",
      "\n",
      "[1076 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "seqDataAns =pd.read_json('data/seq-public.answer.json',orient='index')\n",
    "print(seqDataAns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             [4, 2, -1, -3]\n",
      "1       [1/4, 7/5, 3/4, 2/5]\n",
      "2         [34, 841, 677, 37]\n",
      "3           [50, 65, 75, 56]\n",
      "4       [3/4, 1/4, 2/5, 5/6]\n",
      "                ...         \n",
      "1071                      []\n",
      "1072                      []\n",
      "1073                      []\n",
      "1074      [34, 841, 866, 37]\n",
      "1075        [50, 65, 75, 56]\n",
      "Name: options, Length: 1076, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(seqdataIn['options'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Preprocess hints for printing - (not given with question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                   A_{n+1}=2^{5-n}-A_{n}\n",
      "1       3/1, 4/2, 5/3, 6/4 |  | A_{n+1}=A_{n}+1 |  | B...\n",
      "2                                     A_{n+1}=A_{n}^{2}+1\n",
      "3                                         A_{n}=2n*(2n-1)\n",
      "4                        4/2, 4/4, 4/6, 4/8 |  | A_{n}=2n\n",
      "                              ...                        \n",
      "1090                                  A_{n+1}-A_{n}=P(n);\n",
      "1091                               A_{n*2}=A_{n*2-1}^2+2;\n",
      "1092                             A_{n}=A_{n-2}^2-A_{n-1};\n",
      "1093                           A_{n}=A_{n-1}^2+A_{n-2}^2;\n",
      "1094                                   A_{n}=(2*n-1)*2*n;\n",
      "Name: hint, Length: 1076, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def hint2txt(h):\n",
    "    return h.replace('<sub>','_{').replace('</sub>','}').replace('<sup>','^{').replace('</sup>','}').replace('\\n',' | ').replace('\\*','*')\n",
    "seqDataAns['hint'] = seqDataAns['hint'].map(lambda x: hint2txt(x))\n",
    "print(seqDataAns['hint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: The sequences are recursive and can involve exponents and the index number. \n",
    "#### The answers also correspond to the index in the answer choices not the literal answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fibonacci LSTM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n",
      "[2, 3, 5, 8, 13, 21, 34, 55, 89, 144]\n"
     ]
    }
   ],
   "source": [
    "#define fibonacci function\n",
    "def fib(x):\n",
    "    if x == 0:\n",
    "        return 1\n",
    "    elif x == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fib(x-1)+fib(x-2)\n",
    "\n",
    "#returns n fibonacci numbers (starting with the 'start'th number) \n",
    "def fibSeq(n,start=0):\n",
    "    s = []\n",
    "    for i in range(start,start+n):\n",
    "        s.append(fib(i))\n",
    "    return s\n",
    "\n",
    "print(fibSeq(10))\n",
    "print(fibSeq(10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]] => [2]\n",
      "[[1 2]] => [3]\n",
      "[[2 3]] => [5]\n",
      "[[3 5]] => [8]\n",
      "[[5 8]] => [13]\n",
      "[[ 8 13]] => [21]\n",
      "[[13 21]] => [34]\n",
      "[[21 34]] => [55]\n",
      "[[34 55]] => [89]\n",
      "[[55 89]] => [144]\n",
      "[[ 89 144]] => [233]\n",
      "[[144 233]] => [377]\n",
      "[[233 377]] => [610]\n",
      "[[377 610]] => [987]\n",
      "[[610 987]] => [1597]\n",
      "[[ 987 1597]] => [2584]\n",
      "[[1597 2584]] => [4181]\n",
      "[[2584 4181]] => [6765]\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/time-series-forecasting-with-recurrent-neural-networks-74674e289816\n",
    "# https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras/\n",
    "\n",
    "# Test data preprocessing for RNN \n",
    "lookback = 2 #for fibonacci specifically only looks back to the previous 2 values\n",
    "testSeq = fibSeq(20)\n",
    "generator = TimeseriesGenerator(testSeq, testSeq, length=lookback, batch_size=1)\n",
    "# print each sample\n",
    "for i in range(len(generator)):\n",
    "\tx, y = generator[i]\n",
    "\tprint('%s => %s' % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 5, 8, 13], [121393, 196418, 317811, 514229, 832040], [233, 377, 610, 987, 1597], [13, 21, 34, 55, 89], [377, 610, 987, 1597, 2584], [17711, 28657, 46368, 75025, 121393], [46368, 75025, 121393, 196418, 317811], [987, 1597, 2584, 4181, 6765], [89, 144, 233, 377, 610], [121393, 196418, 317811, 514229, 832040]]\n",
      "[[6765, 10946, 17711, 28657, 46368]]\n"
     ]
    }
   ],
   "source": [
    "# make train and test data sets\n",
    "m = 30\n",
    "seqLen = 5\n",
    "masterSeq = fibSeq(m) #get first 50 fib numbers\n",
    "#make set of specific length \n",
    "def fibSet(x,l):\n",
    "    s = []\n",
    "    for i in range(x):\n",
    "        t = random.randint(0,m-l)\n",
    "        s.append(masterSeq[t:t+l])\n",
    "    return s\n",
    "train_fib = fibSet(10,seqLen)\n",
    "test_fib = fibSet(1,seqLen)\n",
    "print(train_fib)\n",
    "print(test_fib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data for lstm\n",
    "fib_look = 2 #for fibonacci specifically only looks back to the previous 2 values\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "#train\n",
    "for t in train_fib:\n",
    "    train_gen = TimeseriesGenerator(t, t, length=fib_look, batch_size=1)\n",
    "    for i in range(len(train_gen)):\n",
    "        x, y = train_gen[i]\n",
    "        X_train.append(x)\n",
    "        y_train.append(y)\n",
    "\n",
    "X_train = np.squeeze(np.asarray(X_train))\n",
    "y_train = np.squeeze(np.asarray(y_train))\n",
    "#reshape to [# samples, #time steps, #features] -> [10*segments, 2, 1]\n",
    "X_train = X_train.reshape((X_train.shape[0],fib_look,1))\n",
    "\n",
    "#test\n",
    "for t in test_fib:\n",
    "    test_gen = TimeseriesGenerator(t, t, length=fib_look, batch_size=1)\n",
    "    for i in range(len(test_gen)):\n",
    "        x, y = test_gen[i]\n",
    "        X_test.append(x)\n",
    "        y_test.append(y)\n",
    "\n",
    "X_test = np.squeeze(np.asarray(X_test))\n",
    "y_test = np.squeeze(np.asarray(y_test))\n",
    "#reshape to [# samples, #time steps, #features] -> [10*segments, 2, 1]\n",
    "X_test = X_test.reshape((X_test.shape[0],fib_look,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train[[[     2]\n",
      "  [     3]]\n",
      "\n",
      " [[     3]\n",
      "  [     5]]\n",
      "\n",
      " [[     5]\n",
      "  [     8]]\n",
      "\n",
      " [[121393]\n",
      "  [196418]]\n",
      "\n",
      " [[196418]\n",
      "  [317811]]\n",
      "\n",
      " [[317811]\n",
      "  [514229]]\n",
      "\n",
      " [[   233]\n",
      "  [   377]]\n",
      "\n",
      " [[   377]\n",
      "  [   610]]\n",
      "\n",
      " [[   610]\n",
      "  [   987]]\n",
      "\n",
      " [[    13]\n",
      "  [    21]]\n",
      "\n",
      " [[    21]\n",
      "  [    34]]\n",
      "\n",
      " [[    34]\n",
      "  [    55]]\n",
      "\n",
      " [[   377]\n",
      "  [   610]]\n",
      "\n",
      " [[   610]\n",
      "  [   987]]\n",
      "\n",
      " [[   987]\n",
      "  [  1597]]\n",
      "\n",
      " [[ 17711]\n",
      "  [ 28657]]\n",
      "\n",
      " [[ 28657]\n",
      "  [ 46368]]\n",
      "\n",
      " [[ 46368]\n",
      "  [ 75025]]\n",
      "\n",
      " [[ 46368]\n",
      "  [ 75025]]\n",
      "\n",
      " [[ 75025]\n",
      "  [121393]]\n",
      "\n",
      " [[121393]\n",
      "  [196418]]\n",
      "\n",
      " [[   987]\n",
      "  [  1597]]\n",
      "\n",
      " [[  1597]\n",
      "  [  2584]]\n",
      "\n",
      " [[  2584]\n",
      "  [  4181]]\n",
      "\n",
      " [[    89]\n",
      "  [   144]]\n",
      "\n",
      " [[   144]\n",
      "  [   233]]\n",
      "\n",
      " [[   233]\n",
      "  [   377]]\n",
      "\n",
      " [[121393]\n",
      "  [196418]]\n",
      "\n",
      " [[196418]\n",
      "  [317811]]\n",
      "\n",
      " [[317811]\n",
      "  [514229]]]\n",
      "y train[     5      8     13 317811 514229 832040    610    987   1597     34\n",
      "     55     89    987   1597   2584  46368  75025 121393 121393 196418\n",
      " 317811   2584   4181   6765    233    377    610 317811 514229 832040]\n",
      "X test[[[ 6765]\n",
      "  [10946]]\n",
      "\n",
      " [[10946]\n",
      "  [17711]]\n",
      "\n",
      " [[17711]\n",
      "  [28657]]]\n",
      "y test[17711 28657 46368]\n"
     ]
    }
   ],
   "source": [
    "print(\"X train\" + str(X_train))\n",
    "print(\"y train\" + str(y_train))\n",
    "print(\"X test\" + str(X_test))\n",
    "print(\"y test\" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make Simple LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(fib_look, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse',metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 38000226304.0000 - mse: 38000226304.0000 - val_loss: 88859.2266 - val_mse: 88859.2266\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 37789618176.0000 - mse: 37789618176.0000 - val_loss: 88257.6016 - val_mse: 88257.6016\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 37611683840.0000 - mse: 37611683840.0000 - val_loss: 87700.2422 - val_mse: 87700.2422\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 37432868864.0000 - mse: 37432868864.0000 - val_loss: 87168.6250 - val_mse: 87168.6250\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 37236871168.0000 - mse: 37236871168.0000 - val_loss: 86650.9141 - val_mse: 86650.9141\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 37057490944.0000 - mse: 37057490944.0000 - val_loss: 86139.8047 - val_mse: 86139.8047\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 36877082624.0000 - mse: 36877082624.0000 - val_loss: 85634.2578 - val_mse: 85634.2578\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 36695633920.0000 - mse: 36695633920.0000 - val_loss: 85129.6484 - val_mse: 85129.6484\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 36513136640.0000 - mse: 36513136640.0000 - val_loss: 84620.9766 - val_mse: 84620.9766\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 36329549824.0000 - mse: 36329549824.0000 - val_loss: 84104.8984 - val_mse: 84104.8984\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 36144861184.0000 - mse: 36144861184.0000 - val_loss: 83577.5234 - val_mse: 83577.5234\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 35959013376.0000 - mse: 35959013376.0000 - val_loss: 83034.4297 - val_mse: 83034.4297\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 35415064576.0000 - mse: 35415064576.0000 - val_loss: 82463.1953 - val_mse: 82463.1953\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 35237920768.0000 - mse: 35237920768.0000 - val_loss: 81848.6797 - val_mse: 81848.6797\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 35059339264.0000 - mse: 35059339264.0000 - val_loss: 81181.0000 - val_mse: 81181.0000\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 34879275008.0000 - mse: 34879275008.0000 - val_loss: 80494.8203 - val_mse: 80494.8203\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 34697715712.0000 - mse: 34697715712.0000 - val_loss: 79790.4922 - val_mse: 79790.4922\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 34514616320.0000 - mse: 34514616320.0000 - val_loss: 79061.7188 - val_mse: 79061.7188\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 34329952256.0000 - mse: 34329952256.0000 - val_loss: 78321.1250 - val_mse: 78321.1250\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 34143688704.0000 - mse: 34143688704.0000 - val_loss: 77592.0859 - val_mse: 77592.0859\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 32606365696.0000 - mse: 32606365696.0000 - val_loss: 76840.5391 - val_mse: 76840.5391\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 32406511616.0000 - mse: 32406511616.0000 - val_loss: 76120.8750 - val_mse: 76120.8750\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 32204445696.0000 - mse: 32204445696.0000 - val_loss: 75425.0625 - val_mse: 75425.0625\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 32000256000.0000 - mse: 32000256000.0000 - val_loss: 74736.4062 - val_mse: 74736.4062\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 31794022400.0000 - mse: 31794022400.0000 - val_loss: 74039.4141 - val_mse: 74039.4141\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 31585812480.0000 - mse: 31585812480.0000 - val_loss: 73336.2500 - val_mse: 73336.2500\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 31375671296.0000 - mse: 31375671296.0000 - val_loss: 72609.5234 - val_mse: 72609.5234\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 31163635712.0000 - mse: 31163635712.0000 - val_loss: 71838.0625 - val_mse: 71838.0625\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 30949736448.0000 - mse: 30949736448.0000 - val_loss: 70997.2578 - val_mse: 70997.2578\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 29806807040.0000 - mse: 29806807040.0000 - val_loss: 70039.1016 - val_mse: 70039.1016\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 29581246464.0000 - mse: 29581246464.0000 - val_loss: 68943.4609 - val_mse: 68943.4609\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 29350576128.0000 - mse: 29350576128.0000 - val_loss: 67783.3047 - val_mse: 67783.3047\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 29115824128.0000 - mse: 29115824128.0000 - val_loss: 66524.6953 - val_mse: 66524.6953\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 28877551616.0000 - mse: 28877551616.0000 - val_loss: 65215.2695 - val_mse: 65215.2695\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 28636100608.0000 - mse: 28636100608.0000 - val_loss: 63842.2031 - val_mse: 63842.2031\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 27197962240.0000 - mse: 27197962240.0000 - val_loss: 62395.9258 - val_mse: 62395.9258\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 26942765056.0000 - mse: 26942765056.0000 - val_loss: 60943.6367 - val_mse: 60943.6367\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 25686804480.0000 - mse: 25686804480.0000 - val_loss: 59447.0117 - val_mse: 59447.0117\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 25419948032.0000 - mse: 25419948032.0000 - val_loss: 57887.1445 - val_mse: 57887.1445\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 25147049984.0000 - mse: 25147049984.0000 - val_loss: 56340.2812 - val_mse: 56340.2812\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 23638079488.0000 - mse: 23638079488.0000 - val_loss: 54807.8125 - val_mse: 54807.8125\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 23344799744.0000 - mse: 23344799744.0000 - val_loss: 53277.8867 - val_mse: 53277.8867\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 21954940928.0000 - mse: 21954940928.0000 - val_loss: 51705.8320 - val_mse: 51705.8320\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 20375824384.0000 - mse: 20375824384.0000 - val_loss: 50179.1875 - val_mse: 50179.1875\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 20047882240.0000 - mse: 20047882240.0000 - val_loss: 48749.2695 - val_mse: 48749.2695\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 19709427712.0000 - mse: 19709427712.0000 - val_loss: 47420.3398 - val_mse: 47420.3398\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 19363366912.0000 - mse: 19363366912.0000 - val_loss: 46178.7461 - val_mse: 46178.7461\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 19011493888.0000 - mse: 19011493888.0000 - val_loss: 45003.9258 - val_mse: 45003.9258\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 18655111168.0000 - mse: 18655111168.0000 - val_loss: 43873.5117 - val_mse: 43873.5117\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 18295212032.0000 - mse: 18295212032.0000 - val_loss: 42766.7227 - val_mse: 42766.7227\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 17808406528.0000 - mse: 17808406528.0000 - val_loss: 41660.8242 - val_mse: 41660.8242\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 17440528384.0000 - mse: 17440528384.0000 - val_loss: 40528.7500 - val_mse: 40528.7500\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 17069829120.0000 - mse: 17069829120.0000 - val_loss: 39334.4844 - val_mse: 39334.4844\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 16697203712.0000 - mse: 16697203712.0000 - val_loss: 38088.8867 - val_mse: 38088.8867\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 16323301376.0000 - mse: 16323301376.0000 - val_loss: 36727.5898 - val_mse: 36727.5898\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 15948337152.0000 - mse: 15948337152.0000 - val_loss: 35154.2969 - val_mse: 35154.2969\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 15000821760.0000 - mse: 15000821760.0000 - val_loss: 33244.6914 - val_mse: 33244.6914\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 14621299712.0000 - mse: 14621299712.0000 - val_loss: 31189.3066 - val_mse: 31189.3066\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 14242362368.0000 - mse: 14242362368.0000 - val_loss: 28924.2188 - val_mse: 28924.2188\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10290642944.0000 - mse: 10290642944.0000 - val_loss: 26200.3359 - val_mse: 26200.3359\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9943267328.0000 - mse: 9943267328.0000 - val_loss: 23612.3535 - val_mse: 23612.3535\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9121667072.0000 - mse: 9121667072.0000 - val_loss: 21306.6348 - val_mse: 21306.6348\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 8772640768.0000 - mse: 8772640768.0000 - val_loss: 19337.7090 - val_mse: 19337.7090\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6808510464.0000 - mse: 6808510464.0000 - val_loss: 17427.8203 - val_mse: 17427.8203\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4704304640.0000 - mse: 4704304640.0000 - val_loss: 15443.3408 - val_mse: 15443.3408\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4415388160.0000 - mse: 4415388160.0000 - val_loss: 13544.0742 - val_mse: 13544.0742\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4130093824.0000 - mse: 4130093824.0000 - val_loss: 11870.9033 - val_mse: 11870.9033\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3851157248.0000 - mse: 3851157248.0000 - val_loss: 10467.4990 - val_mse: 10467.4990\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3580203264.0000 - mse: 3580203264.0000 - val_loss: 9295.4541 - val_mse: 9295.4541\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3318319360.0000 - mse: 3318319360.0000 - val_loss: 8289.3428 - val_mse: 8289.3428\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3066270976.0000 - mse: 3066270976.0000 - val_loss: 7383.1968 - val_mse: 7383.1968\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2824604416.0000 - mse: 2824604416.0000 - val_loss: 6565.1743 - val_mse: 6565.1743\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2593707264.0000 - mse: 2593707264.0000 - val_loss: 5818.2754 - val_mse: 5818.2754\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2373777664.0000 - mse: 2373777664.0000 - val_loss: 5129.9243 - val_mse: 5129.9243\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1893540736.0000 - mse: 1893540736.0000 - val_loss: 4509.2261 - val_mse: 4509.2261\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1705067904.0000 - mse: 1705067904.0000 - val_loss: 3951.3105 - val_mse: 3951.3105\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1527327872.0000 - mse: 1527327872.0000 - val_loss: 3448.2451 - val_mse: 3448.2451\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1360818304.0000 - mse: 1360818304.0000 - val_loss: 2990.3098 - val_mse: 2990.3098\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1205691264.0000 - mse: 1205691264.0000 - val_loss: 2569.9006 - val_mse: 2569.9006\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1061924864.0000 - mse: 1061924864.0000 - val_loss: 2187.9260 - val_mse: 2187.9260\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 929387328.0000 - mse: 929387328.0000 - val_loss: 1846.8859 - val_mse: 1846.8859\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 807864320.0000 - mse: 807864320.0000 - val_loss: 1541.2330 - val_mse: 1541.2330\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 697070592.0000 - mse: 697070592.0000 - val_loss: 1268.9874 - val_mse: 1268.9874\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 596665664.0000 - mse: 596665664.0000 - val_loss: 1029.5018 - val_mse: 1029.5018\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 506256000.0000 - mse: 506256000.0000 - val_loss: 822.5146 - val_mse: 822.5146\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 216880256.0000 - mse: 216880256.0000 - val_loss: 647.7973 - val_mse: 647.7973\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 164784976.0000 - mse: 164784976.0000 - val_loss: 500.3153 - val_mse: 500.3153\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 121130144.0000 - mse: 121130144.0000 - val_loss: 377.7061 - val_mse: 377.7061\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 85449032.0000 - mse: 85449032.0000 - val_loss: 277.5776 - val_mse: 277.5776\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 57128468.0000 - mse: 57128468.0000 - val_loss: 197.6839 - val_mse: 197.6839\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 34282844.0000 - mse: 34282844.0000 - val_loss: 135.7718 - val_mse: 135.7718\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 18728010.0000 - mse: 18728010.0000 - val_loss: 89.5393 - val_mse: 89.5393\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8345437.5000 - mse: 8345437.5000 - val_loss: 56.8955 - val_mse: 56.8955\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2388083.0000 - mse: 2388083.0000 - val_loss: 35.7934 - val_mse: 35.7934\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 91858.2109 - mse: 91858.2109 - val_loss: 24.1818 - val_mse: 24.1818\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 691665.1250 - mse: 691665.1250 - val_loss: 19.9466 - val_mse: 19.9466\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3431034.2500 - mse: 3431034.2500 - val_loss: 20.9041 - val_mse: 20.9041\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7579598.5000 - mse: 7579598.5000 - val_loss: 24.9387 - val_mse: 24.9387\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12469851.0000 - mse: 12469851.0000 - val_loss: 30.2629 - val_mse: 30.2629\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 17537588.0000 - mse: 17537588.0000 - val_loss: 35.6017 - val_mse: 35.6017\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 22341882.0000 - mse: 22341882.0000 - val_loss: 40.1821 - val_mse: 40.1821\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 26562928.0000 - mse: 26562928.0000 - val_loss: 43.6080 - val_mse: 43.6080\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 29991400.0000 - mse: 29991400.0000 - val_loss: 45.7286 - val_mse: 45.7286\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 32513730.0000 - mse: 32513730.0000 - val_loss: 46.5473 - val_mse: 46.5473\n",
      "Epoch 105/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step - loss: 34095036.0000 - mse: 34095036.0000 - val_loss: 46.1603 - val_mse: 46.1603\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 34762004.0000 - mse: 34762004.0000 - val_loss: 44.7202 - val_mse: 44.7202\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 34586956.0000 - mse: 34586956.0000 - val_loss: 42.4149 - val_mse: 42.4149\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 33673388.0000 - mse: 33673388.0000 - val_loss: 39.4551 - val_mse: 39.4551\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 32142014.0000 - mse: 32142014.0000 - val_loss: 36.0541 - val_mse: 36.0541\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 30121862.0000 - mse: 30121862.0000 - val_loss: 32.4404 - val_mse: 32.4404\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 27740906.0000 - mse: 27740906.0000 - val_loss: 28.7869 - val_mse: 28.7869\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 25186912.0000 - mse: 25186912.0000 - val_loss: 25.1962 - val_mse: 25.1962\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 22566786.0000 - mse: 22566786.0000 - val_loss: 21.7767 - val_mse: 21.7767\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 19885436.0000 - mse: 19885436.0000 - val_loss: 18.6460 - val_mse: 18.6460\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 17227826.0000 - mse: 17227826.0000 - val_loss: 15.8967 - val_mse: 15.8967\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 14665521.0000 - mse: 14665521.0000 - val_loss: 13.5953 - val_mse: 13.5953\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12254897.0000 - mse: 12254897.0000 - val_loss: 11.7834 - val_mse: 11.7834\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10038099.0000 - mse: 10038099.0000 - val_loss: 10.4792 - val_mse: 10.4792\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8043965.5000 - mse: 8043965.5000 - val_loss: 9.6802 - val_mse: 9.6802\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6289049.5000 - mse: 6289049.5000 - val_loss: 9.3655 - val_mse: 9.3655\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4779205.5000 - mse: 4779205.5000 - val_loss: 9.5003 - val_mse: 9.5003\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3511310.7500 - mse: 3511310.7500 - val_loss: 10.0376 - val_mse: 10.0376\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2474934.2500 - mse: 2474934.2500 - val_loss: 10.9224 - val_mse: 10.9224\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1654053.3750 - mse: 1654053.3750 - val_loss: 12.0949 - val_mse: 12.0949\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1028439.8125 - mse: 1028439.8125 - val_loss: 13.4924 - val_mse: 13.4924\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 575411.8125 - mse: 575411.8125 - val_loss: 15.0529 - val_mse: 15.0529\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 270804.4375 - mse: 270804.4375 - val_loss: 16.7156 - val_mse: 16.7156\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 90184.2266 - mse: 90184.2266 - val_loss: 18.4241 - val_mse: 18.4241\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9755.1025 - mse: 9755.1025 - val_loss: 20.1274 - val_mse: 20.1274\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7062.7153 - mse: 7062.7153 - val_loss: 21.7804 - val_mse: 21.7804\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 61502.8711 - mse: 61502.8711 - val_loss: 23.3435 - val_mse: 23.3435\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 154725.2031 - mse: 154725.2031 - val_loss: 24.7856 - val_mse: 24.7856\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 270837.3438 - mse: 270837.3438 - val_loss: 26.0812 - val_mse: 26.0812\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 396495.1562 - mse: 396495.1562 - val_loss: 27.2129 - val_mse: 27.2129\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 520890.9688 - mse: 520890.9688 - val_loss: 28.1677 - val_mse: 28.1677\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 635667.6875 - mse: 635667.6875 - val_loss: 28.9407 - val_mse: 28.9407\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 734707.2500 - mse: 734707.2500 - val_loss: 29.5299 - val_mse: 29.5299\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 813978.6875 - mse: 813978.6875 - val_loss: 29.9397 - val_mse: 29.9397\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 871222.3125 - mse: 871222.3125 - val_loss: 30.1773 - val_mse: 30.1773\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 905753.6875 - mse: 905753.6875 - val_loss: 30.2528 - val_mse: 30.2528\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 918123.3125 - mse: 918123.3125 - val_loss: 30.1795 - val_mse: 30.1795\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 909814.4375 - mse: 909814.4375 - val_loss: 29.9718 - val_mse: 29.9718\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 883118.1875 - mse: 883118.1875 - val_loss: 29.6458 - val_mse: 29.6458\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 840784.6875 - mse: 840784.6875 - val_loss: 29.2173 - val_mse: 29.2173\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 785892.3125 - mse: 785892.3125 - val_loss: 28.7034 - val_mse: 28.7034\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 721507.2500 - mse: 721507.2500 - val_loss: 28.1199 - val_mse: 28.1199\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 650809.8125 - mse: 650809.8125 - val_loss: 27.4822 - val_mse: 27.4822\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 576612.2500 - mse: 576612.2500 - val_loss: 26.8054 - val_mse: 26.8054\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 501624.5312 - mse: 501624.5312 - val_loss: 26.1023 - val_mse: 26.1023\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 428093.8750 - mse: 428093.8750 - val_loss: 25.3857 - val_mse: 25.3857\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 357995.2812 - mse: 357995.2812 - val_loss: 24.6666 - val_mse: 24.6666\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 292822.2500 - mse: 292822.2500 - val_loss: 23.9545 - val_mse: 23.9545\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 233699.5469 - mse: 233699.5469 - val_loss: 23.2570 - val_mse: 23.2570\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 181378.4375 - mse: 181378.4375 - val_loss: 22.5818 - val_mse: 22.5818\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 136238.7969 - mse: 136238.7969 - val_loss: 21.9347 - val_mse: 21.9347\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 98373.4609 - mse: 98373.4609 - val_loss: 21.3197 - val_mse: 21.3197\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 67587.2266 - mse: 67587.2266 - val_loss: 20.7406 - val_mse: 20.7406\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 43485.3555 - mse: 43485.3555 - val_loss: 20.2000 - val_mse: 20.2000\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 25499.7188 - mse: 25499.7188 - val_loss: 19.6993 - val_mse: 19.6993\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12933.6299 - mse: 12933.6299 - val_loss: 19.2395 - val_mse: 19.2395\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5023.0508 - mse: 5023.0508 - val_loss: 18.8212 - val_mse: 18.8212\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 985.8335 - mse: 985.8335 - val_loss: 18.4440 - val_mse: 18.4440\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 25.4125 - mse: 25.4125 - val_loss: 18.1073 - val_mse: 18.1073\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1387.8926 - mse: 1387.8926 - val_loss: 17.8103 - val_mse: 17.8103\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4380.0259 - mse: 4380.0259 - val_loss: 17.5513 - val_mse: 17.5513\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8379.1553 - mse: 8379.1553 - val_loss: 17.3291 - val_mse: 17.3291\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12847.0576 - mse: 12847.0576 - val_loss: 17.1421 - val_mse: 17.1421\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 17348.7715 - mse: 17348.7715 - val_loss: 16.9884 - val_mse: 16.9884\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 21528.8809 - mse: 21528.8809 - val_loss: 16.8661 - val_mse: 16.8661\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 25135.4004 - mse: 25135.4004 - val_loss: 16.7734 - val_mse: 16.7734\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 27995.4629 - mse: 27995.4629 - val_loss: 16.7083 - val_mse: 16.7083\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 30010.5781 - mse: 30010.5781 - val_loss: 16.6687 - val_mse: 16.6687\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 31146.6328 - mse: 31146.6328 - val_loss: 16.6525 - val_mse: 16.6525\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 31433.6035 - mse: 31433.6035 - val_loss: 16.6581 - val_mse: 16.6581\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 30929.5859 - mse: 30929.5859 - val_loss: 16.6828 - val_mse: 16.6828\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 29742.3535 - mse: 29742.3535 - val_loss: 16.7252 - val_mse: 16.7252\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 27980.3926 - mse: 27980.3926 - val_loss: 16.7829 - val_mse: 16.7829\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 25775.7285 - mse: 25775.7285 - val_loss: 16.8540 - val_mse: 16.8540\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 23257.2344 - mse: 23257.2344 - val_loss: 16.9363 - val_mse: 16.9363\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 20558.0625 - mse: 20558.0625 - val_loss: 17.0283 - val_mse: 17.0283\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 17787.0078 - mse: 17787.0078 - val_loss: 17.1278 - val_mse: 17.1278\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 15047.8564 - mse: 15047.8564 - val_loss: 17.2330 - val_mse: 17.2330\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12433.6494 - mse: 12433.6494 - val_loss: 17.3423 - val_mse: 17.3423\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10009.7188 - mse: 10009.7188 - val_loss: 17.4542 - val_mse: 17.4542\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7822.1030 - mse: 7822.1030 - val_loss: 17.5666 - val_mse: 17.5666\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5910.4634 - mse: 5910.4634 - val_loss: 17.6786 - val_mse: 17.6786\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4288.8340 - mse: 4288.8340 - val_loss: 17.7884 - val_mse: 17.7884\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2955.5479 - mse: 2955.5479 - val_loss: 17.8953 - val_mse: 17.8953\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1905.0394 - mse: 1905.0394 - val_loss: 17.9979 - val_mse: 17.9979\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1117.6034 - mse: 1117.6034 - val_loss: 18.0958 - val_mse: 18.0958\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 566.1449 - mse: 566.1449 - val_loss: 18.1874 - val_mse: 18.1874\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 219.1584 - mse: 219.1584 - val_loss: 18.2725 - val_mse: 18.2725\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 43.6685 - mse: 43.6685 - val_loss: 18.3506 - val_mse: 18.3506\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.1672 - mse: 4.1672 - val_loss: 18.4210 - val_mse: 18.4210\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 67.1958 - mse: 67.1958 - val_loss: 18.4839 - val_mse: 18.4839\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 201.7553 - mse: 201.7553 - val_loss: 18.5384 - val_mse: 18.5384\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 378.9379 - mse: 378.9379 - val_loss: 18.5855 - val_mse: 18.5855\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 575.2782 - mse: 575.2782 - val_loss: 18.6248 - val_mse: 18.6248\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 770.4042 - mse: 770.4042 - val_loss: 18.6561 - val_mse: 18.6561\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 949.1213 - mse: 949.1213 - val_loss: 18.6804 - val_mse: 18.6804\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1099.0659 - mse: 1099.0659 - val_loss: 18.6973 - val_mse: 18.6973\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1213.6180 - mse: 1213.6180 - val_loss: 18.7078 - val_mse: 18.7078\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1288.5956 - mse: 1288.5956 - val_loss: 18.7120 - val_mse: 18.7120\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1324.7290 - mse: 1324.7290 - val_loss: 18.7107 - val_mse: 18.7107\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1321.9292 - mse: 1321.9292 - val_loss: 18.7041 - val_mse: 18.7041\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1285.7778 - mse: 1285.7778 - val_loss: 18.6932 - val_mse: 18.6932\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1219.1613 - mse: 1219.1613 - val_loss: 18.6783 - val_mse: 18.6783\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1129.2543 - mse: 1129.2543 - val_loss: 18.6602 - val_mse: 18.6602\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1023.9141 - mse: 1023.9141 - val_loss: 18.6392 - val_mse: 18.6392\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 904.6287 - mse: 904.6287 - val_loss: 18.6159 - val_mse: 18.6159\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 783.5722 - mse: 783.5722 - val_loss: 18.5910 - val_mse: 18.5910\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 660.4838 - mse: 660.4838 - val_loss: 18.5650 - val_mse: 18.5650\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 543.5884 - mse: 543.5884 - val_loss: 18.5379 - val_mse: 18.5379\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 434.6289 - mse: 434.6289 - val_loss: 18.5108 - val_mse: 18.5108\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 335.8123 - mse: 335.8123 - val_loss: 18.4834 - val_mse: 18.4834\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 249.9346 - mse: 249.9346 - val_loss: 18.4567 - val_mse: 18.4567\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 178.1871 - mse: 178.1871 - val_loss: 18.4306 - val_mse: 18.4306\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 119.4158 - mse: 119.4158 - val_loss: 18.4054 - val_mse: 18.4054\n",
      "Epoch 219/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step - loss: 74.2547 - mse: 74.2547 - val_loss: 18.3815 - val_mse: 18.3815\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 41.4928 - mse: 41.4928 - val_loss: 18.3591 - val_mse: 18.3591\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 19.5474 - mse: 19.5474 - val_loss: 18.3382 - val_mse: 18.3382\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.2429 - mse: 7.2429 - val_loss: 18.3191 - val_mse: 18.3191\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.5029 - mse: 2.5029 - val_loss: 18.3018 - val_mse: 18.3018\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.7672 - mse: 3.7672 - val_loss: 18.2862 - val_mse: 18.2862\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.2867 - mse: 9.2867 - val_loss: 18.2727 - val_mse: 18.2727\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 17.6160 - mse: 17.6160 - val_loss: 18.2611 - val_mse: 18.2611\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 27.1299 - mse: 27.1299 - val_loss: 18.2516 - val_mse: 18.2516\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 37.2556 - mse: 37.2556 - val_loss: 18.2436 - val_mse: 18.2436\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 46.4023 - mse: 46.4023 - val_loss: 18.2375 - val_mse: 18.2375\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 54.2959 - mse: 54.2959 - val_loss: 18.2333 - val_mse: 18.2333\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 60.5295 - mse: 60.5295 - val_loss: 18.2307 - val_mse: 18.2307\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 64.5629 - mse: 64.5629 - val_loss: 18.2298 - val_mse: 18.2298\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 66.6310 - mse: 66.6310 - val_loss: 18.2303 - val_mse: 18.2303\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 66.6168 - mse: 66.6168 - val_loss: 18.2320 - val_mse: 18.2320\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 64.7828 - mse: 64.7828 - val_loss: 18.2350 - val_mse: 18.2350\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 61.5216 - mse: 61.5216 - val_loss: 18.2389 - val_mse: 18.2389\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 57.0187 - mse: 57.0187 - val_loss: 18.2437 - val_mse: 18.2437\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 51.6107 - mse: 51.6107 - val_loss: 18.2491 - val_mse: 18.2491\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 45.4368 - mse: 45.4368 - val_loss: 18.2549 - val_mse: 18.2549\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 39.0541 - mse: 39.0541 - val_loss: 18.2616 - val_mse: 18.2616\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 32.9848 - mse: 32.9848 - val_loss: 18.2683 - val_mse: 18.2683\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 26.9849 - mse: 26.9849 - val_loss: 18.2751 - val_mse: 18.2751\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 21.5125 - mse: 21.5125 - val_loss: 18.2820 - val_mse: 18.2820\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 16.7365 - mse: 16.7365 - val_loss: 18.2889 - val_mse: 18.2889\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 12.5958 - mse: 12.5958 - val_loss: 18.2958 - val_mse: 18.2958\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.1787 - mse: 9.1787 - val_loss: 18.3026 - val_mse: 18.3026\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 6.4867 - mse: 6.4867 - val_loss: 18.3088 - val_mse: 18.3088\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.4821 - mse: 4.4821 - val_loss: 18.3151 - val_mse: 18.3151\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.1332 - mse: 3.1332 - val_loss: 18.3208 - val_mse: 18.3208\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3226 - mse: 2.3226 - val_loss: 18.3261 - val_mse: 18.3261\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.9678 - mse: 1.9678 - val_loss: 18.3311 - val_mse: 18.3311\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.9731 - mse: 1.9731 - val_loss: 18.3357 - val_mse: 18.3357\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.2444 - mse: 2.2444 - val_loss: 18.3396 - val_mse: 18.3396\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.6752 - mse: 2.6752 - val_loss: 18.3430 - val_mse: 18.3430\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.2274 - mse: 3.2274 - val_loss: 18.3460 - val_mse: 18.3460\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.7427 - mse: 3.7427 - val_loss: 18.3487 - val_mse: 18.3487\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.2613 - mse: 4.2613 - val_loss: 18.3509 - val_mse: 18.3509\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.7477 - mse: 4.7477 - val_loss: 18.3527 - val_mse: 18.3527\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.0383 - mse: 5.0383 - val_loss: 18.3544 - val_mse: 18.3544\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.2261 - mse: 5.2261 - val_loss: 18.3560 - val_mse: 18.3560\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.3094 - mse: 5.3094 - val_loss: 18.3574 - val_mse: 18.3574\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.2961 - mse: 5.2961 - val_loss: 18.3578 - val_mse: 18.3578\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.1862 - mse: 5.1862 - val_loss: 18.3573 - val_mse: 18.3573\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.0351 - mse: 5.0351 - val_loss: 18.3562 - val_mse: 18.3562\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.7025 - mse: 4.7025 - val_loss: 18.3549 - val_mse: 18.3549\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.3558 - mse: 4.3558 - val_loss: 18.3534 - val_mse: 18.3534\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.0115 - mse: 4.0115 - val_loss: 18.3516 - val_mse: 18.3516\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.6665 - mse: 3.6665 - val_loss: 18.3502 - val_mse: 18.3502\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.2779 - mse: 3.2779 - val_loss: 18.3483 - val_mse: 18.3483\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9302 - mse: 2.9302 - val_loss: 18.3467 - val_mse: 18.3467\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.6023 - mse: 2.6023 - val_loss: 18.3454 - val_mse: 18.3454\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.3505 - mse: 2.3505 - val_loss: 18.3441 - val_mse: 18.3441\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1019 - mse: 2.1019 - val_loss: 18.3426 - val_mse: 18.3426\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9135 - mse: 1.9135 - val_loss: 18.3416 - val_mse: 18.3416\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.7941 - mse: 1.7941 - val_loss: 18.3405 - val_mse: 18.3405\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6839 - mse: 1.6839 - val_loss: 18.3393 - val_mse: 18.3393\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6335 - mse: 1.6335 - val_loss: 18.3383 - val_mse: 18.3383\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5916 - mse: 1.5916 - val_loss: 18.3367 - val_mse: 18.3367\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5791 - mse: 1.5791 - val_loss: 18.3352 - val_mse: 18.3352\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5881 - mse: 1.5881 - val_loss: 18.3335 - val_mse: 18.3335\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6056 - mse: 1.6056 - val_loss: 18.3315 - val_mse: 18.3315\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6322 - mse: 1.6322 - val_loss: 18.3298 - val_mse: 18.3298\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - mse: 1.6536 - val_loss: 18.3284 - val_mse: 18.3284\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6789 - mse: 1.6789 - val_loss: 18.3272 - val_mse: 18.3272\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6945 - mse: 1.6945 - val_loss: 18.3260 - val_mse: 18.3260\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7030 - mse: 1.7030 - val_loss: 18.3250 - val_mse: 18.3250\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6944 - mse: 1.6944 - val_loss: 18.3244 - val_mse: 18.3244\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6860 - mse: 1.6860 - val_loss: 18.3236 - val_mse: 18.3236\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6778 - mse: 1.6778 - val_loss: 18.3228 - val_mse: 18.3228\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6699 - mse: 1.6699 - val_loss: 18.3222 - val_mse: 18.3222\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6624 - mse: 1.6624 - val_loss: 18.3213 - val_mse: 18.3213\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6403 - mse: 1.6403 - val_loss: 18.3204 - val_mse: 18.3204\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6147 - mse: 1.6147 - val_loss: 18.3193 - val_mse: 18.3193\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5936 - mse: 1.5936 - val_loss: 18.3182 - val_mse: 18.3182\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5657 - mse: 1.5657 - val_loss: 18.3171 - val_mse: 18.3171\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5344 - mse: 1.5344 - val_loss: 18.3156 - val_mse: 18.3156\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5111 - mse: 1.5111 - val_loss: 18.3145 - val_mse: 18.3145\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4882 - mse: 1.4882 - val_loss: 18.3134 - val_mse: 18.3134\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4664 - mse: 1.4664 - val_loss: 18.3122 - val_mse: 18.3122\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4511 - mse: 1.4511 - val_loss: 18.3109 - val_mse: 18.3109\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4301 - mse: 1.4301 - val_loss: 18.3097 - val_mse: 18.3097\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4170 - mse: 1.4170 - val_loss: 18.3085 - val_mse: 18.3085\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4075 - mse: 1.4075 - val_loss: 18.3074 - val_mse: 18.3074\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4019 - mse: 1.4019 - val_loss: 18.3062 - val_mse: 18.3062\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3936 - mse: 1.3936 - val_loss: 18.3048 - val_mse: 18.3048\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3884 - mse: 1.3884 - val_loss: 18.3035 - val_mse: 18.3035\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3885 - mse: 1.3885 - val_loss: 18.3022 - val_mse: 18.3022\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3812 - mse: 1.3812 - val_loss: 18.3005 - val_mse: 18.3005\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3848 - mse: 1.3848 - val_loss: 18.2989 - val_mse: 18.2989\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3797 - mse: 1.3797 - val_loss: 18.2974 - val_mse: 18.2974\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3756 - mse: 1.3756 - val_loss: 18.2960 - val_mse: 18.2960\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3718 - mse: 1.3718 - val_loss: 18.2944 - val_mse: 18.2944\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3670 - mse: 1.3670 - val_loss: 18.2929 - val_mse: 18.2929\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3685 - mse: 1.3685 - val_loss: 18.2913 - val_mse: 18.2913\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3713 - mse: 1.3713 - val_loss: 18.2900 - val_mse: 18.2900\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3680 - mse: 1.3680 - val_loss: 18.2886 - val_mse: 18.2886\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3656 - mse: 1.3656 - val_loss: 18.2873 - val_mse: 18.2873\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3633 - mse: 1.3633 - val_loss: 18.2858 - val_mse: 18.2858\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3611 - mse: 1.3611 - val_loss: 18.2844 - val_mse: 18.2844\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3589 - mse: 1.3589 - val_loss: 18.2833 - val_mse: 18.2833\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3576 - mse: 1.3576 - val_loss: 18.2821 - val_mse: 18.2821\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3503 - mse: 1.3503 - val_loss: 18.2808 - val_mse: 18.2808\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3483 - mse: 1.3483 - val_loss: 18.2797 - val_mse: 18.2797\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3422 - mse: 1.3422 - val_loss: 18.2786 - val_mse: 18.2786\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3414 - mse: 1.3414 - val_loss: 18.2776 - val_mse: 18.2776\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3407 - mse: 1.3407 - val_loss: 18.2768 - val_mse: 18.2768\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3392 - mse: 1.3392 - val_loss: 18.2758 - val_mse: 18.2758\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3376 - mse: 1.3376 - val_loss: 18.2751 - val_mse: 18.2751\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3367 - mse: 1.3367 - val_loss: 18.2744 - val_mse: 18.2744\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3360 - mse: 1.3360 - val_loss: 18.2738 - val_mse: 18.2738\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3350 - mse: 1.3350 - val_loss: 18.2733 - val_mse: 18.2733\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3336 - mse: 1.3336 - val_loss: 18.2728 - val_mse: 18.2728\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3340 - mse: 1.3340 - val_loss: 18.2727 - val_mse: 18.2727\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3339 - mse: 1.3339 - val_loss: 18.2721 - val_mse: 18.2721\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3240 - mse: 1.3240 - val_loss: 18.2718 - val_mse: 18.2718\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3243 - mse: 1.3243 - val_loss: 18.2718 - val_mse: 18.2718\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3232 - mse: 1.3232 - val_loss: 18.2719 - val_mse: 18.2719\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3240 - mse: 1.3240 - val_loss: 18.2719 - val_mse: 18.2719\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3196 - mse: 1.3196 - val_loss: 18.2721 - val_mse: 18.2721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3190 - mse: 1.3190 - val_loss: 18.2721 - val_mse: 18.2721\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3180 - mse: 1.3180 - val_loss: 18.2724 - val_mse: 18.2724\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3172 - mse: 1.3172 - val_loss: 18.2727 - val_mse: 18.2727\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3164 - mse: 1.3164 - val_loss: 18.2729 - val_mse: 18.2729\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3118 - mse: 1.3118 - val_loss: 18.2731 - val_mse: 18.2731\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3117 - mse: 1.3117 - val_loss: 18.2733 - val_mse: 18.2733\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3120 - mse: 1.3120 - val_loss: 18.2737 - val_mse: 18.2737\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3113 - mse: 1.3113 - val_loss: 18.2737 - val_mse: 18.2737\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3107 - mse: 1.3107 - val_loss: 18.2737 - val_mse: 18.2737\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.3101 - mse: 1.3101 - val_loss: 18.2737 - val_mse: 18.2737\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3095 - mse: 1.3095 - val_loss: 18.2734 - val_mse: 18.2734\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3089 - mse: 1.3089 - val_loss: 18.2730 - val_mse: 18.2730\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3084 - mse: 1.3084 - val_loss: 18.2720 - val_mse: 18.2720\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3079 - mse: 1.3079 - val_loss: 18.2713 - val_mse: 18.2713\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3074 - mse: 1.3074 - val_loss: 18.2700 - val_mse: 18.2700\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3069 - mse: 1.3069 - val_loss: 18.2686 - val_mse: 18.2686\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.3064 - mse: 1.3064 - val_loss: 18.2672 - val_mse: 18.2672\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3060 - mse: 1.3060 - val_loss: 18.2651 - val_mse: 18.2651\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3056 - mse: 1.3056 - val_loss: 18.2630 - val_mse: 18.2630\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3052 - mse: 1.3052 - val_loss: 18.2607 - val_mse: 18.2607\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3047 - mse: 1.3047 - val_loss: 18.2579 - val_mse: 18.2579\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3043 - mse: 1.3043 - val_loss: 18.2549 - val_mse: 18.2549\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3038 - mse: 1.3038 - val_loss: 18.2517 - val_mse: 18.2517\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3034 - mse: 1.3034 - val_loss: 18.2483 - val_mse: 18.2483\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3029 - mse: 1.3029 - val_loss: 18.2447 - val_mse: 18.2447\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3025 - mse: 1.3025 - val_loss: 18.2410 - val_mse: 18.2410\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3021 - mse: 1.3021 - val_loss: 18.2371 - val_mse: 18.2371\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3017 - mse: 1.3017 - val_loss: 18.2328 - val_mse: 18.2328\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3012 - mse: 1.3012 - val_loss: 18.2287 - val_mse: 18.2287\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3008 - mse: 1.3008 - val_loss: 18.2242 - val_mse: 18.2242\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3004 - mse: 1.3004 - val_loss: 18.2196 - val_mse: 18.2196\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2999 - mse: 1.2999 - val_loss: 18.2154 - val_mse: 18.2154\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2995 - mse: 1.2995 - val_loss: 18.2107 - val_mse: 18.2107\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2990 - mse: 1.2990 - val_loss: 18.2060 - val_mse: 18.2060\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2986 - mse: 1.2986 - val_loss: 18.2013 - val_mse: 18.2013\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2982 - mse: 1.2982 - val_loss: 18.1968 - val_mse: 18.1968\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2978 - mse: 1.2978 - val_loss: 18.1921 - val_mse: 18.1921\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2974 - mse: 1.2974 - val_loss: 18.1874 - val_mse: 18.1874\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2970 - mse: 1.2970 - val_loss: 18.1826 - val_mse: 18.1826\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2965 - mse: 1.2965 - val_loss: 18.1779 - val_mse: 18.1779\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2961 - mse: 1.2961 - val_loss: 18.1735 - val_mse: 18.1735\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2957 - mse: 1.2957 - val_loss: 18.1687 - val_mse: 18.1687\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2953 - mse: 1.2953 - val_loss: 18.1643 - val_mse: 18.1643\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2949 - mse: 1.2949 - val_loss: 18.1597 - val_mse: 18.1597\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2945 - mse: 1.2945 - val_loss: 18.1551 - val_mse: 18.1551\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2941 - mse: 1.2941 - val_loss: 18.1508 - val_mse: 18.1508\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2937 - mse: 1.2937 - val_loss: 18.1462 - val_mse: 18.1462\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2934 - mse: 1.2934 - val_loss: 18.1419 - val_mse: 18.1419\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2930 - mse: 1.2930 - val_loss: 18.1374 - val_mse: 18.1374\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2926 - mse: 1.2926 - val_loss: 18.1332 - val_mse: 18.1332\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2922 - mse: 1.2922 - val_loss: 18.1289 - val_mse: 18.1289\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2918 - mse: 1.2918 - val_loss: 18.1248 - val_mse: 18.1248\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2914 - mse: 1.2914 - val_loss: 18.1205 - val_mse: 18.1205\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2910 - mse: 1.2910 - val_loss: 18.1164 - val_mse: 18.1164\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2906 - mse: 1.2906 - val_loss: 18.1121 - val_mse: 18.1121\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2903 - mse: 1.2903 - val_loss: 18.1081 - val_mse: 18.1081\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2899 - mse: 1.2899 - val_loss: 18.1040 - val_mse: 18.1040\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2895 - mse: 1.2895 - val_loss: 18.0999 - val_mse: 18.0999\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2892 - mse: 1.2892 - val_loss: 18.0960 - val_mse: 18.0960\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2888 - mse: 1.2888 - val_loss: 18.0918 - val_mse: 18.0918\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2885 - mse: 1.2885 - val_loss: 18.0879 - val_mse: 18.0879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2881 - mse: 1.2881 - val_loss: 18.0840 - val_mse: 18.0840\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2878 - mse: 1.2878 - val_loss: 18.0800 - val_mse: 18.0800\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2874 - mse: 1.2874 - val_loss: 18.0762 - val_mse: 18.0762\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2870 - mse: 1.2870 - val_loss: 18.0722 - val_mse: 18.0722\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2867 - mse: 1.2867 - val_loss: 18.0681 - val_mse: 18.0681\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2863 - mse: 1.2863 - val_loss: 18.0644 - val_mse: 18.0644\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2860 - mse: 1.2860 - val_loss: 18.0603 - val_mse: 18.0603\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2856 - mse: 1.2856 - val_loss: 18.0565 - val_mse: 18.0565\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2852 - mse: 1.2852 - val_loss: 18.0526 - val_mse: 18.0526\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2849 - mse: 1.2849 - val_loss: 18.0487 - val_mse: 18.0487\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2845 - mse: 1.2845 - val_loss: 18.0449 - val_mse: 18.0449\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2841 - mse: 1.2841 - val_loss: 18.0411 - val_mse: 18.0411\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2838 - mse: 1.2838 - val_loss: 18.0372 - val_mse: 18.0372\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2835 - mse: 1.2835 - val_loss: 18.0333 - val_mse: 18.0333\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2831 - mse: 1.2831 - val_loss: 18.0295 - val_mse: 18.0295\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2827 - mse: 1.2827 - val_loss: 18.0255 - val_mse: 18.0255\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2824 - mse: 1.2824 - val_loss: 18.0215 - val_mse: 18.0215\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2820 - mse: 1.2820 - val_loss: 18.0176 - val_mse: 18.0176\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2817 - mse: 1.2817 - val_loss: 18.0139 - val_mse: 18.0139\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2813 - mse: 1.2813 - val_loss: 18.0101 - val_mse: 18.0101\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2810 - mse: 1.2810 - val_loss: 18.0061 - val_mse: 18.0061\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2806 - mse: 1.2806 - val_loss: 18.0022 - val_mse: 18.0022\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2804 - mse: 1.2804 - val_loss: 17.9984 - val_mse: 17.9984\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2799 - mse: 1.2799 - val_loss: 17.9944 - val_mse: 17.9944\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2797 - mse: 1.2797 - val_loss: 17.9903 - val_mse: 17.9903\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2794 - mse: 1.2794 - val_loss: 17.9865 - val_mse: 17.9865\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2791 - mse: 1.2791 - val_loss: 17.9826 - val_mse: 17.9826\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2787 - mse: 1.2787 - val_loss: 17.9786 - val_mse: 17.9786\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2784 - mse: 1.2784 - val_loss: 17.9746 - val_mse: 17.9746\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2780 - mse: 1.2780 - val_loss: 17.9707 - val_mse: 17.9707\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2776 - mse: 1.2776 - val_loss: 17.9669 - val_mse: 17.9669\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2773 - mse: 1.2773 - val_loss: 17.9629 - val_mse: 17.9629\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2770 - mse: 1.2770 - val_loss: 17.9589 - val_mse: 17.9589\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2767 - mse: 1.2767 - val_loss: 17.9549 - val_mse: 17.9549\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2764 - mse: 1.2764 - val_loss: 17.9509 - val_mse: 17.9509\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2761 - mse: 1.2761 - val_loss: 17.9467 - val_mse: 17.9467\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2758 - mse: 1.2758 - val_loss: 17.9429 - val_mse: 17.9429\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2754 - mse: 1.2754 - val_loss: 17.9389 - val_mse: 17.9389\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2751 - mse: 1.2751 - val_loss: 17.9349 - val_mse: 17.9349\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2747 - mse: 1.2747 - val_loss: 17.9309 - val_mse: 17.9309\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2744 - mse: 1.2744 - val_loss: 17.9268 - val_mse: 17.9268\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2741 - mse: 1.2741 - val_loss: 17.9226 - val_mse: 17.9226\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2738 - mse: 1.2738 - val_loss: 17.9188 - val_mse: 17.9188\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2735 - mse: 1.2735 - val_loss: 17.9147 - val_mse: 17.9147\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2732 - mse: 1.2732 - val_loss: 17.9109 - val_mse: 17.9109\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2729 - mse: 1.2729 - val_loss: 17.9066 - val_mse: 17.9066\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2725 - mse: 1.2725 - val_loss: 17.9027 - val_mse: 17.9027\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2722 - mse: 1.2722 - val_loss: 17.8986 - val_mse: 17.8986\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2718 - mse: 1.2718 - val_loss: 17.8946 - val_mse: 17.8946\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2716 - mse: 1.2716 - val_loss: 17.8906 - val_mse: 17.8906\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2713 - mse: 1.2713 - val_loss: 17.8865 - val_mse: 17.8865\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2710 - mse: 1.2710 - val_loss: 17.8825 - val_mse: 17.8825\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2707 - mse: 1.2707 - val_loss: 17.8786 - val_mse: 17.8786\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2704 - mse: 1.2704 - val_loss: 17.8744 - val_mse: 17.8744\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2700 - mse: 1.2700 - val_loss: 17.8703 - val_mse: 17.8703\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2698 - mse: 1.2698 - val_loss: 17.8665 - val_mse: 17.8665\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2732 - mse: 1.2732 - val_loss: 17.8623 - val_mse: 17.8623\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2729 - mse: 1.2729 - val_loss: 17.8585 - val_mse: 17.8585\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2726 - mse: 1.2726 - val_loss: 17.8543 - val_mse: 17.8543\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2723 - mse: 1.2723 - val_loss: 17.8504 - val_mse: 17.8504\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2720 - mse: 1.2720 - val_loss: 17.8462 - val_mse: 17.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2717 - mse: 1.2717 - val_loss: 17.8424 - val_mse: 17.8424\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2714 - mse: 1.2714 - val_loss: 17.8382 - val_mse: 17.8382\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2711 - mse: 1.2711 - val_loss: 17.8341 - val_mse: 17.8341\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2708 - mse: 1.2708 - val_loss: 17.8302 - val_mse: 17.8302\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2705 - mse: 1.2705 - val_loss: 17.8261 - val_mse: 17.8261\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2702 - mse: 1.2702 - val_loss: 17.8219 - val_mse: 17.8219\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2699 - mse: 1.2699 - val_loss: 17.8180 - val_mse: 17.8180\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2696 - mse: 1.2696 - val_loss: 17.8141 - val_mse: 17.8141\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2656 - mse: 1.2656 - val_loss: 17.8100 - val_mse: 17.8100\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2653 - mse: 1.2653 - val_loss: 17.8059 - val_mse: 17.8059\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2650 - mse: 1.2650 - val_loss: 17.8019 - val_mse: 17.8019\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2647 - mse: 1.2647 - val_loss: 17.7979 - val_mse: 17.7979\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2644 - mse: 1.2644 - val_loss: 17.7937 - val_mse: 17.7937\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2641 - mse: 1.2641 - val_loss: 17.7899 - val_mse: 17.7899\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2638 - mse: 1.2638 - val_loss: 17.7859 - val_mse: 17.7859\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2635 - mse: 1.2635 - val_loss: 17.7820 - val_mse: 17.7820\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2632 - mse: 1.2632 - val_loss: 17.7777 - val_mse: 17.7777\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2629 - mse: 1.2629 - val_loss: 17.7737 - val_mse: 17.7737\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2626 - mse: 1.2626 - val_loss: 17.7698 - val_mse: 17.7698\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2624 - mse: 1.2624 - val_loss: 17.7657 - val_mse: 17.7657\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2621 - mse: 1.2621 - val_loss: 17.7616 - val_mse: 17.7616\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2618 - mse: 1.2618 - val_loss: 17.7577 - val_mse: 17.7577\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2615 - mse: 1.2615 - val_loss: 17.7536 - val_mse: 17.7536\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2612 - mse: 1.2612 - val_loss: 17.7497 - val_mse: 17.7497\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2609 - mse: 1.2609 - val_loss: 17.7455 - val_mse: 17.7455\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2606 - mse: 1.2606 - val_loss: 17.7416 - val_mse: 17.7416\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2603 - mse: 1.2603 - val_loss: 17.7375 - val_mse: 17.7375\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2601 - mse: 1.2601 - val_loss: 17.7334 - val_mse: 17.7334\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2635 - mse: 1.2635 - val_loss: 17.7296 - val_mse: 17.7296\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2632 - mse: 1.2632 - val_loss: 17.7254 - val_mse: 17.7254\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2630 - mse: 1.2630 - val_loss: 17.7215 - val_mse: 17.7215\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2627 - mse: 1.2627 - val_loss: 17.7175 - val_mse: 17.7175\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2624 - mse: 1.2624 - val_loss: 17.7133 - val_mse: 17.7133\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2622 - mse: 1.2622 - val_loss: 17.7095 - val_mse: 17.7095\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2619 - mse: 1.2619 - val_loss: 17.7054 - val_mse: 17.7054\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2616 - mse: 1.2616 - val_loss: 17.7013 - val_mse: 17.7013\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2613 - mse: 1.2613 - val_loss: 17.6975 - val_mse: 17.6975\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2610 - mse: 1.2610 - val_loss: 17.6933 - val_mse: 17.6933\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2608 - mse: 1.2608 - val_loss: 17.6895 - val_mse: 17.6895\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2568 - mse: 1.2568 - val_loss: 17.6855 - val_mse: 17.6855\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2564 - mse: 1.2564 - val_loss: 17.6812 - val_mse: 17.6812\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2561 - mse: 1.2561 - val_loss: 17.6771 - val_mse: 17.6771\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2559 - mse: 1.2559 - val_loss: 17.6734 - val_mse: 17.6734\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2556 - mse: 1.2556 - val_loss: 17.6692 - val_mse: 17.6692\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2554 - mse: 1.2554 - val_loss: 17.6655 - val_mse: 17.6655\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2551 - mse: 1.2551 - val_loss: 17.6613 - val_mse: 17.6613\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2548 - mse: 1.2548 - val_loss: 17.6572 - val_mse: 17.6572\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2546 - mse: 1.2546 - val_loss: 17.6532 - val_mse: 17.6532\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2543 - mse: 1.2543 - val_loss: 17.6493 - val_mse: 17.6493\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2540 - mse: 1.2540 - val_loss: 17.6452 - val_mse: 17.6452\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2538 - mse: 1.2538 - val_loss: 17.6412 - val_mse: 17.6412\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2535 - mse: 1.2535 - val_loss: 17.6372 - val_mse: 17.6372\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2532 - mse: 1.2532 - val_loss: 17.6331 - val_mse: 17.6331\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2530 - mse: 1.2530 - val_loss: 17.6294 - val_mse: 17.6294\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2527 - mse: 1.2527 - val_loss: 17.6252 - val_mse: 17.6252\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2524 - mse: 1.2524 - val_loss: 17.6212 - val_mse: 17.6212\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2522 - mse: 1.2522 - val_loss: 17.6174 - val_mse: 17.6174\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2557 - mse: 1.2557 - val_loss: 17.6132 - val_mse: 17.6132\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2554 - mse: 1.2554 - val_loss: 17.6091 - val_mse: 17.6091\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2551 - mse: 1.2551 - val_loss: 17.6052 - val_mse: 17.6052\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2548 - mse: 1.2548 - val_loss: 17.6012 - val_mse: 17.6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2546 - mse: 1.2546 - val_loss: 17.5971 - val_mse: 17.5971\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2543 - mse: 1.2543 - val_loss: 17.5932 - val_mse: 17.5932\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2540 - mse: 1.2540 - val_loss: 17.5893 - val_mse: 17.5893\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2538 - mse: 1.2538 - val_loss: 17.5852 - val_mse: 17.5852\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2535 - mse: 1.2535 - val_loss: 17.5814 - val_mse: 17.5814\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2533 - mse: 1.2533 - val_loss: 17.5772 - val_mse: 17.5772\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2530 - mse: 1.2530 - val_loss: 17.5735 - val_mse: 17.5735\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2490 - mse: 1.2490 - val_loss: 17.5694 - val_mse: 17.5694\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2487 - mse: 1.2487 - val_loss: 17.5654 - val_mse: 17.5654\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2484 - mse: 1.2484 - val_loss: 17.5614 - val_mse: 17.5614\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2482 - mse: 1.2482 - val_loss: 17.5575 - val_mse: 17.5575\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2479 - mse: 1.2479 - val_loss: 17.5534 - val_mse: 17.5534\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2477 - mse: 1.2477 - val_loss: 17.5493 - val_mse: 17.5493\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2474 - mse: 1.2474 - val_loss: 17.5455 - val_mse: 17.5455\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2472 - mse: 1.2472 - val_loss: 17.5415 - val_mse: 17.5415\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2468 - mse: 1.2468 - val_loss: 17.5374 - val_mse: 17.5374\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2466 - mse: 1.2466 - val_loss: 17.5335 - val_mse: 17.5335\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2463 - mse: 1.2463 - val_loss: 17.5296 - val_mse: 17.5296\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2461 - mse: 1.2461 - val_loss: 17.5255 - val_mse: 17.5255\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2458 - mse: 1.2458 - val_loss: 17.5218 - val_mse: 17.5218\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2456 - mse: 1.2456 - val_loss: 17.5176 - val_mse: 17.5176\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2454 - mse: 1.2454 - val_loss: 17.5135 - val_mse: 17.5135\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2451 - mse: 1.2451 - val_loss: 17.5095 - val_mse: 17.5095\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2448 - mse: 1.2448 - val_loss: 17.5059 - val_mse: 17.5059\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2445 - mse: 1.2445 - val_loss: 17.5018 - val_mse: 17.5018\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2481 - mse: 1.2481 - val_loss: 17.4980 - val_mse: 17.4980\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2479 - mse: 1.2479 - val_loss: 17.4938 - val_mse: 17.4938\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2476 - mse: 1.2476 - val_loss: 17.4897 - val_mse: 17.4897\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2474 - mse: 1.2474 - val_loss: 17.4859 - val_mse: 17.4859\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2470 - mse: 1.2470 - val_loss: 17.4822 - val_mse: 17.4822\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2468 - mse: 1.2468 - val_loss: 17.4781 - val_mse: 17.4781\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2465 - mse: 1.2465 - val_loss: 17.4740 - val_mse: 17.4740\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2463 - mse: 1.2463 - val_loss: 17.4700 - val_mse: 17.4700\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2460 - mse: 1.2460 - val_loss: 17.4660 - val_mse: 17.4660\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2458 - mse: 1.2458 - val_loss: 17.4622 - val_mse: 17.4622\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2455 - mse: 1.2455 - val_loss: 17.4583 - val_mse: 17.4583\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2415 - mse: 1.2415 - val_loss: 17.4544 - val_mse: 17.4544\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2413 - mse: 1.2413 - val_loss: 17.4504 - val_mse: 17.4504\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2411 - mse: 1.2411 - val_loss: 17.4463 - val_mse: 17.4463\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2409 - mse: 1.2409 - val_loss: 17.4423 - val_mse: 17.4423\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2406 - mse: 1.2406 - val_loss: 17.4387 - val_mse: 17.4387\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2404 - mse: 1.2404 - val_loss: 17.4345 - val_mse: 17.4345\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2401 - mse: 1.2401 - val_loss: 17.4307 - val_mse: 17.4307\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2399 - mse: 1.2399 - val_loss: 17.4269 - val_mse: 17.4269\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2396 - mse: 1.2396 - val_loss: 17.4229 - val_mse: 17.4229\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2394 - mse: 1.2394 - val_loss: 17.4188 - val_mse: 17.4188\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2391 - mse: 1.2391 - val_loss: 17.4151 - val_mse: 17.4151\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2389 - mse: 1.2389 - val_loss: 17.4111 - val_mse: 17.4111\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2386 - mse: 1.2386 - val_loss: 17.4070 - val_mse: 17.4070\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2384 - mse: 1.2384 - val_loss: 17.4034 - val_mse: 17.4034\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2381 - mse: 1.2381 - val_loss: 17.3994 - val_mse: 17.3994\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2379 - mse: 1.2379 - val_loss: 17.3954 - val_mse: 17.3954\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2376 - mse: 1.2376 - val_loss: 17.3915 - val_mse: 17.3915\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2411 - mse: 1.2411 - val_loss: 17.3877 - val_mse: 17.3877\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2409 - mse: 1.2409 - val_loss: 17.3837 - val_mse: 17.3837\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2407 - mse: 1.2407 - val_loss: 17.3798 - val_mse: 17.3798\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2404 - mse: 1.2404 - val_loss: 17.3760 - val_mse: 17.3760\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2402 - mse: 1.2402 - val_loss: 17.3720 - val_mse: 17.3720\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2399 - mse: 1.2399 - val_loss: 17.3680 - val_mse: 17.3680\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2397 - mse: 1.2397 - val_loss: 17.3644 - val_mse: 17.3644\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2395 - mse: 1.2395 - val_loss: 17.3604 - val_mse: 17.3604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2392 - mse: 1.2392 - val_loss: 17.3565 - val_mse: 17.3565\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2390 - mse: 1.2390 - val_loss: 17.3524 - val_mse: 17.3524\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2350 - mse: 1.2350 - val_loss: 17.3487 - val_mse: 17.3487\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2348 - mse: 1.2348 - val_loss: 17.3448 - val_mse: 17.3448\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2345 - mse: 1.2345 - val_loss: 17.3408 - val_mse: 17.3408\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2343 - mse: 1.2343 - val_loss: 17.3371 - val_mse: 17.3371\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2340 - mse: 1.2340 - val_loss: 17.3331 - val_mse: 17.3331\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 17.3292 - val_mse: 17.3292\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 17.3251 - val_mse: 17.3251\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2334 - mse: 1.2334 - val_loss: 17.3216 - val_mse: 17.3216\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2332 - mse: 1.2332 - val_loss: 17.3175 - val_mse: 17.3175\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2329 - mse: 1.2329 - val_loss: 17.3138 - val_mse: 17.3138\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2326 - mse: 1.2326 - val_loss: 17.3099 - val_mse: 17.3099\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2324 - mse: 1.2324 - val_loss: 17.3060 - val_mse: 17.3060\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2323 - mse: 1.2323 - val_loss: 17.3021 - val_mse: 17.3021\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2320 - mse: 1.2320 - val_loss: 17.2984 - val_mse: 17.2984\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2318 - mse: 1.2318 - val_loss: 17.2945 - val_mse: 17.2945\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2316 - mse: 1.2316 - val_loss: 17.2905 - val_mse: 17.2905\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2350 - mse: 1.2350 - val_loss: 17.2868 - val_mse: 17.2868\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2348 - mse: 1.2348 - val_loss: 17.2829 - val_mse: 17.2829\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2346 - mse: 1.2346 - val_loss: 17.2790 - val_mse: 17.2790\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2343 - mse: 1.2343 - val_loss: 17.2751 - val_mse: 17.2751\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2341 - mse: 1.2341 - val_loss: 17.2714 - val_mse: 17.2714\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2339 - mse: 1.2339 - val_loss: 17.2674 - val_mse: 17.2674\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2336 - mse: 1.2336 - val_loss: 17.2635 - val_mse: 17.2635\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2334 - mse: 1.2334 - val_loss: 17.2596 - val_mse: 17.2596\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2332 - mse: 1.2332 - val_loss: 17.2558 - val_mse: 17.2558\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2329 - mse: 1.2329 - val_loss: 17.2521 - val_mse: 17.2521\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2290 - mse: 1.2290 - val_loss: 17.2483 - val_mse: 17.2483\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2287 - mse: 1.2287 - val_loss: 17.2445 - val_mse: 17.2445\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2285 - mse: 1.2285 - val_loss: 17.2405 - val_mse: 17.2405\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2283 - mse: 1.2283 - val_loss: 17.2367 - val_mse: 17.2367\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2281 - mse: 1.2281 - val_loss: 17.2331 - val_mse: 17.2331\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2279 - mse: 1.2279 - val_loss: 17.2292 - val_mse: 17.2292\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2277 - mse: 1.2277 - val_loss: 17.2253 - val_mse: 17.2253\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2275 - mse: 1.2275 - val_loss: 17.2213 - val_mse: 17.2213\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2272 - mse: 1.2272 - val_loss: 17.2176 - val_mse: 17.2176\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2270 - mse: 1.2270 - val_loss: 17.2139 - val_mse: 17.2139\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2268 - mse: 1.2268 - val_loss: 17.2100 - val_mse: 17.2100\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2265 - mse: 1.2265 - val_loss: 17.2063 - val_mse: 17.2063\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2263 - mse: 1.2263 - val_loss: 17.2023 - val_mse: 17.2023\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2261 - mse: 1.2261 - val_loss: 17.1986 - val_mse: 17.1986\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2258 - mse: 1.2258 - val_loss: 17.1947 - val_mse: 17.1947\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2294 - mse: 1.2294 - val_loss: 17.1911 - val_mse: 17.1911\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2291 - mse: 1.2291 - val_loss: 17.1872 - val_mse: 17.1872\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2289 - mse: 1.2289 - val_loss: 17.1833 - val_mse: 17.1833\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2287 - mse: 1.2287 - val_loss: 17.1796 - val_mse: 17.1796\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2285 - mse: 1.2285 - val_loss: 17.1759 - val_mse: 17.1759\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2282 - mse: 1.2282 - val_loss: 17.1722 - val_mse: 17.1722\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2280 - mse: 1.2280 - val_loss: 17.1682 - val_mse: 17.1682\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2277 - mse: 1.2277 - val_loss: 17.1644 - val_mse: 17.1644\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2276 - mse: 1.2276 - val_loss: 17.1607 - val_mse: 17.1607\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2236 - mse: 1.2236 - val_loss: 17.1570 - val_mse: 17.1570\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2234 - mse: 1.2234 - val_loss: 17.1533 - val_mse: 17.1533\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2232 - mse: 1.2232 - val_loss: 17.1495 - val_mse: 17.1495\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2229 - mse: 1.2229 - val_loss: 17.1456 - val_mse: 17.1456\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2228 - mse: 1.2228 - val_loss: 17.1418 - val_mse: 17.1418\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2225 - mse: 1.2225 - val_loss: 17.1382 - val_mse: 17.1382\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2224 - mse: 1.2224 - val_loss: 17.1343 - val_mse: 17.1343\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2221 - mse: 1.2221 - val_loss: 17.1304 - val_mse: 17.1304\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2219 - mse: 1.2219 - val_loss: 17.1269 - val_mse: 17.1269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2217 - mse: 1.2217 - val_loss: 17.1231 - val_mse: 17.1231\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2215 - mse: 1.2215 - val_loss: 17.1194 - val_mse: 17.1194\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2213 - mse: 1.2213 - val_loss: 17.1157 - val_mse: 17.1157\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2210 - mse: 1.2210 - val_loss: 17.1118 - val_mse: 17.1118\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2208 - mse: 1.2208 - val_loss: 17.1080 - val_mse: 17.1080\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2206 - mse: 1.2206 - val_loss: 17.1042 - val_mse: 17.1042\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2241 - mse: 1.2241 - val_loss: 17.1007 - val_mse: 17.1007\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2239 - mse: 1.2239 - val_loss: 17.0971 - val_mse: 17.0971\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2237 - mse: 1.2237 - val_loss: 17.0931 - val_mse: 17.0931\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2234 - mse: 1.2234 - val_loss: 17.0895 - val_mse: 17.0895\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2232 - mse: 1.2232 - val_loss: 17.0857 - val_mse: 17.0857\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2230 - mse: 1.2230 - val_loss: 17.0822 - val_mse: 17.0822\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2228 - mse: 1.2228 - val_loss: 17.0784 - val_mse: 17.0784\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2226 - mse: 1.2226 - val_loss: 17.0746 - val_mse: 17.0746\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2224 - mse: 1.2224 - val_loss: 17.0710 - val_mse: 17.0710\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2184 - mse: 1.2184 - val_loss: 17.0671 - val_mse: 17.0671\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2182 - mse: 1.2182 - val_loss: 17.0634 - val_mse: 17.0634\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2181 - mse: 1.2181 - val_loss: 17.0596 - val_mse: 17.0596\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2179 - mse: 1.2179 - val_loss: 17.0562 - val_mse: 17.0562\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2176 - mse: 1.2176 - val_loss: 17.0524 - val_mse: 17.0524\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2175 - mse: 1.2175 - val_loss: 17.0487 - val_mse: 17.0487\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2172 - mse: 1.2172 - val_loss: 17.0450 - val_mse: 17.0450\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2170 - mse: 1.2170 - val_loss: 17.0413 - val_mse: 17.0413\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2168 - mse: 1.2168 - val_loss: 17.0376 - val_mse: 17.0376\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2166 - mse: 1.2166 - val_loss: 17.0338 - val_mse: 17.0338\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2164 - mse: 1.2164 - val_loss: 17.0303 - val_mse: 17.0303\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2162 - mse: 1.2162 - val_loss: 17.0266 - val_mse: 17.0266\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2159 - mse: 1.2159 - val_loss: 17.0230 - val_mse: 17.0230\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2158 - mse: 1.2158 - val_loss: 17.0192 - val_mse: 17.0192\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2193 - mse: 1.2193 - val_loss: 17.0154 - val_mse: 17.0154\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2191 - mse: 1.2191 - val_loss: 17.0120 - val_mse: 17.0120\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2189 - mse: 1.2189 - val_loss: 17.0082 - val_mse: 17.0082\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2187 - mse: 1.2187 - val_loss: 17.0045 - val_mse: 17.0045\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2185 - mse: 1.2185 - val_loss: 17.0007 - val_mse: 17.0007\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2182 - mse: 1.2182 - val_loss: 16.9972 - val_mse: 16.9972\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2180 - mse: 1.2180 - val_loss: 16.9935 - val_mse: 16.9935\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2178 - mse: 1.2178 - val_loss: 16.9899 - val_mse: 16.9899\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2176 - mse: 1.2176 - val_loss: 16.9864 - val_mse: 16.9864\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2137 - mse: 1.2137 - val_loss: 16.9826 - val_mse: 16.9826\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2134 - mse: 1.2134 - val_loss: 16.9788 - val_mse: 16.9788\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2133 - mse: 1.2133 - val_loss: 16.9755 - val_mse: 16.9755\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2131 - mse: 1.2131 - val_loss: 16.9717 - val_mse: 16.9717\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2129 - mse: 1.2129 - val_loss: 16.9679 - val_mse: 16.9679\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2127 - mse: 1.2127 - val_loss: 16.9645 - val_mse: 16.9645\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2124 - mse: 1.2124 - val_loss: 16.9607 - val_mse: 16.9607\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2122 - mse: 1.2122 - val_loss: 16.9572 - val_mse: 16.9572\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2120 - mse: 1.2120 - val_loss: 16.9535 - val_mse: 16.9535\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2118 - mse: 1.2118 - val_loss: 16.9499 - val_mse: 16.9499\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2116 - mse: 1.2116 - val_loss: 16.9461 - val_mse: 16.9461\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2114 - mse: 1.2114 - val_loss: 16.9428 - val_mse: 16.9428\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2112 - mse: 1.2112 - val_loss: 16.9390 - val_mse: 16.9390\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2111 - mse: 1.2111 - val_loss: 16.9356 - val_mse: 16.9356\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2146 - mse: 1.2146 - val_loss: 16.9318 - val_mse: 16.9318\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2144 - mse: 1.2144 - val_loss: 16.9282 - val_mse: 16.9282\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2142 - mse: 1.2142 - val_loss: 16.9247 - val_mse: 16.9247\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2140 - mse: 1.2140 - val_loss: 16.9211 - val_mse: 16.9211\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2138 - mse: 1.2138 - val_loss: 16.9174 - val_mse: 16.9174\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2136 - mse: 1.2136 - val_loss: 16.9140 - val_mse: 16.9140\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2134 - mse: 1.2134 - val_loss: 16.9103 - val_mse: 16.9103\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2132 - mse: 1.2132 - val_loss: 16.9066 - val_mse: 16.9066\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2130 - mse: 1.2130 - val_loss: 16.9031 - val_mse: 16.9031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2091 - mse: 1.2091 - val_loss: 16.8995 - val_mse: 16.8995\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2088 - mse: 1.2088 - val_loss: 16.8959 - val_mse: 16.8959\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2086 - mse: 1.2086 - val_loss: 16.8922 - val_mse: 16.8922\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2084 - mse: 1.2084 - val_loss: 16.8888 - val_mse: 16.8888\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2083 - mse: 1.2083 - val_loss: 16.8851 - val_mse: 16.8851\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2081 - mse: 1.2081 - val_loss: 16.8815 - val_mse: 16.8815\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2079 - mse: 1.2079 - val_loss: 16.8781 - val_mse: 16.8781\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2077 - mse: 1.2077 - val_loss: 16.8744 - val_mse: 16.8744\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2075 - mse: 1.2075 - val_loss: 16.8708 - val_mse: 16.8708\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2073 - mse: 1.2073 - val_loss: 16.8673 - val_mse: 16.8673\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2071 - mse: 1.2071 - val_loss: 16.8639 - val_mse: 16.8639\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2069 - mse: 1.2069 - val_loss: 16.8604 - val_mse: 16.8604\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2068 - mse: 1.2068 - val_loss: 16.8567 - val_mse: 16.8567\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2065 - mse: 1.2065 - val_loss: 16.8531 - val_mse: 16.8531\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2100 - mse: 1.2100 - val_loss: 16.8497 - val_mse: 16.8497\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2099 - mse: 1.2099 - val_loss: 16.8460 - val_mse: 16.8460\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2097 - mse: 1.2097 - val_loss: 16.8425 - val_mse: 16.8425\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2096 - mse: 1.2096 - val_loss: 16.8389 - val_mse: 16.8389\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2094 - mse: 1.2094 - val_loss: 16.8354 - val_mse: 16.8354\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2092 - mse: 1.2092 - val_loss: 16.8318 - val_mse: 16.8318\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2090 - mse: 1.2090 - val_loss: 16.8284 - val_mse: 16.8284\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2088 - mse: 1.2088 - val_loss: 16.8247 - val_mse: 16.8247\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2086 - mse: 1.2086 - val_loss: 16.8215 - val_mse: 16.8215\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2047 - mse: 1.2047 - val_loss: 16.8177 - val_mse: 16.8177\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2045 - mse: 1.2045 - val_loss: 16.8144 - val_mse: 16.8144\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2043 - mse: 1.2043 - val_loss: 16.8109 - val_mse: 16.8109\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2041 - mse: 1.2041 - val_loss: 16.8072 - val_mse: 16.8072\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2039 - mse: 1.2039 - val_loss: 16.8037 - val_mse: 16.8037\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2037 - mse: 1.2037 - val_loss: 16.8004 - val_mse: 16.8004\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2035 - mse: 1.2035 - val_loss: 16.7968 - val_mse: 16.7968\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2033 - mse: 1.2033 - val_loss: 16.7934 - val_mse: 16.7934\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2031 - mse: 1.2031 - val_loss: 16.7898 - val_mse: 16.7898\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2029 - mse: 1.2029 - val_loss: 16.7862 - val_mse: 16.7862\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2027 - mse: 1.2027 - val_loss: 16.7830 - val_mse: 16.7830\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2026 - mse: 1.2026 - val_loss: 16.7793 - val_mse: 16.7793\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2024 - mse: 1.2024 - val_loss: 16.7759 - val_mse: 16.7759\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2060 - mse: 1.2060 - val_loss: 16.7722 - val_mse: 16.7722\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2057 - mse: 1.2057 - val_loss: 16.7689 - val_mse: 16.7689\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2056 - mse: 1.2056 - val_loss: 16.7652 - val_mse: 16.7652\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2054 - mse: 1.2054 - val_loss: 16.7620 - val_mse: 16.7620\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2051 - mse: 1.2051 - val_loss: 16.7584 - val_mse: 16.7584\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2050 - mse: 1.2050 - val_loss: 16.7551 - val_mse: 16.7551\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2047 - mse: 1.2047 - val_loss: 16.7516 - val_mse: 16.7516\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2047 - mse: 1.2047 - val_loss: 16.7479 - val_mse: 16.7479\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2007 - mse: 1.2007 - val_loss: 16.7445 - val_mse: 16.7445\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2005 - mse: 1.2005 - val_loss: 16.7411 - val_mse: 16.7411\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2003 - mse: 1.2003 - val_loss: 16.7377 - val_mse: 16.7377\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2002 - mse: 1.2002 - val_loss: 16.7342 - val_mse: 16.7342\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2000 - mse: 1.2000 - val_loss: 16.7307 - val_mse: 16.7307\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1998 - mse: 1.1998 - val_loss: 16.7275 - val_mse: 16.7275\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1996 - mse: 1.1996 - val_loss: 16.7241 - val_mse: 16.7241\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1995 - mse: 1.1995 - val_loss: 16.7207 - val_mse: 16.7207\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1993 - mse: 1.1993 - val_loss: 16.7171 - val_mse: 16.7171\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1991 - mse: 1.1991 - val_loss: 16.7136 - val_mse: 16.7136\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1989 - mse: 1.1989 - val_loss: 16.7103 - val_mse: 16.7103\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1987 - mse: 1.1987 - val_loss: 16.7067 - val_mse: 16.7067\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1985 - mse: 1.1985 - val_loss: 16.7034 - val_mse: 16.7034\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2021 - mse: 1.2021 - val_loss: 16.7000 - val_mse: 16.7000\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2019 - mse: 1.2019 - val_loss: 16.6965 - val_mse: 16.6965\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2017 - mse: 1.2017 - val_loss: 16.6930 - val_mse: 16.6930\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2015 - mse: 1.2015 - val_loss: 16.6897 - val_mse: 16.6897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2013 - mse: 1.2013 - val_loss: 16.6863 - val_mse: 16.6863\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2012 - mse: 1.2012 - val_loss: 16.6829 - val_mse: 16.6829\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2010 - mse: 1.2010 - val_loss: 16.6796 - val_mse: 16.6796\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2009 - mse: 1.2009 - val_loss: 16.6761 - val_mse: 16.6761\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1969 - mse: 1.1969 - val_loss: 16.6727 - val_mse: 16.6727\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1967 - mse: 1.1967 - val_loss: 16.6693 - val_mse: 16.6693\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1965 - mse: 1.1965 - val_loss: 16.6660 - val_mse: 16.6660\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1963 - mse: 1.1963 - val_loss: 16.6624 - val_mse: 16.6624\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1962 - mse: 1.1962 - val_loss: 16.6590 - val_mse: 16.6590\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1960 - mse: 1.1960 - val_loss: 16.6557 - val_mse: 16.6557\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1958 - mse: 1.1958 - val_loss: 16.6524 - val_mse: 16.6524\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1957 - mse: 1.1957 - val_loss: 16.6490 - val_mse: 16.6490\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1955 - mse: 1.1955 - val_loss: 16.6456 - val_mse: 16.6456\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1953 - mse: 1.1953 - val_loss: 16.6424 - val_mse: 16.6424\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1951 - mse: 1.1951 - val_loss: 16.6388 - val_mse: 16.6388\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1950 - mse: 1.1950 - val_loss: 16.6356 - val_mse: 16.6356\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1986 - mse: 1.1986 - val_loss: 16.6322 - val_mse: 16.6322\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1984 - mse: 1.1984 - val_loss: 16.6290 - val_mse: 16.6290\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1983 - mse: 1.1983 - val_loss: 16.6255 - val_mse: 16.6255\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1981 - mse: 1.1981 - val_loss: 16.6220 - val_mse: 16.6220\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1979 - mse: 1.1979 - val_loss: 16.6187 - val_mse: 16.6187\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1977 - mse: 1.1977 - val_loss: 16.6154 - val_mse: 16.6154\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1975 - mse: 1.1975 - val_loss: 16.6120 - val_mse: 16.6120\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1974 - mse: 1.1974 - val_loss: 16.6087 - val_mse: 16.6087\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1934 - mse: 1.1934 - val_loss: 16.6054 - val_mse: 16.6054\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1933 - mse: 1.1933 - val_loss: 16.6021 - val_mse: 16.6021\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1930 - mse: 1.1930 - val_loss: 16.5988 - val_mse: 16.5988\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1929 - mse: 1.1929 - val_loss: 16.5955 - val_mse: 16.5955\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1927 - mse: 1.1927 - val_loss: 16.5920 - val_mse: 16.5920\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1926 - mse: 1.1926 - val_loss: 16.5889 - val_mse: 16.5889\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1924 - mse: 1.1924 - val_loss: 16.5854 - val_mse: 16.5854\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1922 - mse: 1.1922 - val_loss: 16.5820 - val_mse: 16.5820\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1920 - mse: 1.1920 - val_loss: 16.5788 - val_mse: 16.5788\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1919 - mse: 1.1919 - val_loss: 16.5754 - val_mse: 16.5754\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1917 - mse: 1.1917 - val_loss: 16.5722 - val_mse: 16.5722\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1915 - mse: 1.1915 - val_loss: 16.5689 - val_mse: 16.5689\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1951 - mse: 1.1951 - val_loss: 16.5655 - val_mse: 16.5655\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1949 - mse: 1.1949 - val_loss: 16.5622 - val_mse: 16.5622\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1947 - mse: 1.1947 - val_loss: 16.5591 - val_mse: 16.5591\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1945 - mse: 1.1945 - val_loss: 16.5557 - val_mse: 16.5557\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1944 - mse: 1.1944 - val_loss: 16.5526 - val_mse: 16.5526\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1942 - mse: 1.1942 - val_loss: 16.5491 - val_mse: 16.5491\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1940 - mse: 1.1940 - val_loss: 16.5457 - val_mse: 16.5457\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1939 - mse: 1.1939 - val_loss: 16.5426 - val_mse: 16.5426\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1900 - mse: 1.1900 - val_loss: 16.5393 - val_mse: 16.5393\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1898 - mse: 1.1898 - val_loss: 16.5359 - val_mse: 16.5359\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1897 - mse: 1.1897 - val_loss: 16.5327 - val_mse: 16.5327\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1895 - mse: 1.1895 - val_loss: 16.5296 - val_mse: 16.5296\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1893 - mse: 1.1893 - val_loss: 16.5262 - val_mse: 16.5262\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1891 - mse: 1.1891 - val_loss: 16.5228 - val_mse: 16.5228\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1889 - mse: 1.1889 - val_loss: 16.5196 - val_mse: 16.5196\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1888 - mse: 1.1888 - val_loss: 16.5164 - val_mse: 16.5164\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1886 - mse: 1.1886 - val_loss: 16.5132 - val_mse: 16.5132\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1885 - mse: 1.1885 - val_loss: 16.5097 - val_mse: 16.5097\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1883 - mse: 1.1883 - val_loss: 16.5067 - val_mse: 16.5067\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1881 - mse: 1.1881 - val_loss: 16.5033 - val_mse: 16.5033\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1917 - mse: 1.1917 - val_loss: 16.5002 - val_mse: 16.5002\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1915 - mse: 1.1915 - val_loss: 16.4968 - val_mse: 16.4968\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1913 - mse: 1.1913 - val_loss: 16.4937 - val_mse: 16.4937\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1911 - mse: 1.1911 - val_loss: 16.4903 - val_mse: 16.4903\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1910 - mse: 1.1910 - val_loss: 16.4871 - val_mse: 16.4871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1909 - mse: 1.1909 - val_loss: 16.4839 - val_mse: 16.4839\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1907 - mse: 1.1907 - val_loss: 16.4808 - val_mse: 16.4808\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1905 - mse: 1.1905 - val_loss: 16.4775 - val_mse: 16.4775\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1867 - mse: 1.1867 - val_loss: 16.4743 - val_mse: 16.4743\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1865 - mse: 1.1865 - val_loss: 16.4710 - val_mse: 16.4710\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1863 - mse: 1.1863 - val_loss: 16.4678 - val_mse: 16.4678\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1862 - mse: 1.1862 - val_loss: 16.4646 - val_mse: 16.4646\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1860 - mse: 1.1860 - val_loss: 16.4614 - val_mse: 16.4614\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1858 - mse: 1.1858 - val_loss: 16.4582 - val_mse: 16.4582\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1857 - mse: 1.1857 - val_loss: 16.4548 - val_mse: 16.4548\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1855 - mse: 1.1855 - val_loss: 16.4519 - val_mse: 16.4519\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1853 - mse: 1.1853 - val_loss: 16.4485 - val_mse: 16.4485\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1851 - mse: 1.1851 - val_loss: 16.4454 - val_mse: 16.4454\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1850 - mse: 1.1850 - val_loss: 16.4421 - val_mse: 16.4421\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1849 - mse: 1.1849 - val_loss: 16.4390 - val_mse: 16.4390\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1884 - mse: 1.1884 - val_loss: 16.4357 - val_mse: 16.4357\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1882 - mse: 1.1882 - val_loss: 16.4326 - val_mse: 16.4326\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1880 - mse: 1.1880 - val_loss: 16.4293 - val_mse: 16.4293\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1879 - mse: 1.1879 - val_loss: 16.4263 - val_mse: 16.4263\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1877 - mse: 1.1877 - val_loss: 16.4230 - val_mse: 16.4230\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1876 - mse: 1.1876 - val_loss: 16.4199 - val_mse: 16.4199\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1874 - mse: 1.1874 - val_loss: 16.4166 - val_mse: 16.4166\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1872 - mse: 1.1872 - val_loss: 16.4135 - val_mse: 16.4135\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1833 - mse: 1.1833 - val_loss: 16.4103 - val_mse: 16.4103\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1831 - mse: 1.1831 - val_loss: 16.4072 - val_mse: 16.4072\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1830 - mse: 1.1830 - val_loss: 16.4040 - val_mse: 16.4040\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1828 - mse: 1.1828 - val_loss: 16.4010 - val_mse: 16.4010\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1827 - mse: 1.1827 - val_loss: 16.3977 - val_mse: 16.3977\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1826 - mse: 1.1826 - val_loss: 16.3947 - val_mse: 16.3947\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1824 - mse: 1.1824 - val_loss: 16.3914 - val_mse: 16.3914\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1822 - mse: 1.1822 - val_loss: 16.3884 - val_mse: 16.3884\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1821 - mse: 1.1821 - val_loss: 16.3851 - val_mse: 16.3851\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1819 - mse: 1.1819 - val_loss: 16.3821 - val_mse: 16.3821\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1817 - mse: 1.1817 - val_loss: 16.3790 - val_mse: 16.3790\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1816 - mse: 1.1816 - val_loss: 16.3758 - val_mse: 16.3758\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1851 - mse: 1.1851 - val_loss: 16.3725 - val_mse: 16.3725\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1850 - mse: 1.1850 - val_loss: 16.3694 - val_mse: 16.3694\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1848 - mse: 1.1848 - val_loss: 16.3662 - val_mse: 16.3662\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1848 - mse: 1.1848 - val_loss: 16.3633 - val_mse: 16.3633\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1846 - mse: 1.1846 - val_loss: 16.3602 - val_mse: 16.3602\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1844 - mse: 1.1844 - val_loss: 16.3571 - val_mse: 16.3571\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1842 - mse: 1.1842 - val_loss: 16.3541 - val_mse: 16.3541\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1841 - mse: 1.1841 - val_loss: 16.3508 - val_mse: 16.3508\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1802 - mse: 1.1802 - val_loss: 16.3475 - val_mse: 16.3475\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1801 - mse: 1.1801 - val_loss: 16.3445 - val_mse: 16.3445\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1799 - mse: 1.1799 - val_loss: 16.3414 - val_mse: 16.3414\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1797 - mse: 1.1797 - val_loss: 16.3384 - val_mse: 16.3384\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 16.3352 - val_mse: 16.3352\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1794 - mse: 1.1794 - val_loss: 16.3322 - val_mse: 16.3322\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 16.3291 - val_mse: 16.3291\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 16.3260 - val_mse: 16.3260\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1789 - mse: 1.1789 - val_loss: 16.3229 - val_mse: 16.3229\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1787 - mse: 1.1787 - val_loss: 16.3198 - val_mse: 16.3198\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1786 - mse: 1.1786 - val_loss: 16.3168 - val_mse: 16.3168\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1821 - mse: 1.1821 - val_loss: 16.3136 - val_mse: 16.3136\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1820 - mse: 1.1820 - val_loss: 16.3107 - val_mse: 16.3107\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1818 - mse: 1.1818 - val_loss: 16.3074 - val_mse: 16.3074\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1816 - mse: 1.1816 - val_loss: 16.3045 - val_mse: 16.3045\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1815 - mse: 1.1815 - val_loss: 16.3016 - val_mse: 16.3016\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1813 - mse: 1.1813 - val_loss: 16.2983 - val_mse: 16.2983\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1812 - mse: 1.1812 - val_loss: 16.2953 - val_mse: 16.2953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1773 - mse: 1.1773 - val_loss: 16.2922 - val_mse: 16.2922\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1771 - mse: 1.1771 - val_loss: 16.2890 - val_mse: 16.2890\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1770 - mse: 1.1770 - val_loss: 16.2861 - val_mse: 16.2861\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1769 - mse: 1.1769 - val_loss: 16.2832 - val_mse: 16.2832\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1767 - mse: 1.1767 - val_loss: 16.2801 - val_mse: 16.2801\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1765 - mse: 1.1765 - val_loss: 16.2770 - val_mse: 16.2770\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1764 - mse: 1.1764 - val_loss: 16.2738 - val_mse: 16.2738\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1762 - mse: 1.1762 - val_loss: 16.2709 - val_mse: 16.2709\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1761 - mse: 1.1761 - val_loss: 16.2677 - val_mse: 16.2677\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1759 - mse: 1.1759 - val_loss: 16.2648 - val_mse: 16.2648\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1758 - mse: 1.1758 - val_loss: 16.2619 - val_mse: 16.2619\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1794 - mse: 1.1794 - val_loss: 16.2587 - val_mse: 16.2587\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1792 - mse: 1.1792 - val_loss: 16.2558 - val_mse: 16.2558\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1791 - mse: 1.1791 - val_loss: 16.2526 - val_mse: 16.2526\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1789 - mse: 1.1789 - val_loss: 16.2498 - val_mse: 16.2498\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1787 - mse: 1.1787 - val_loss: 16.2467 - val_mse: 16.2467\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1785 - mse: 1.1785 - val_loss: 16.2436 - val_mse: 16.2436\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1784 - mse: 1.1784 - val_loss: 16.2408 - val_mse: 16.2408\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1745 - mse: 1.1745 - val_loss: 16.2376 - val_mse: 16.2376\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1743 - mse: 1.1743 - val_loss: 16.2347 - val_mse: 16.2347\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1743 - mse: 1.1743 - val_loss: 16.2315 - val_mse: 16.2315\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1741 - mse: 1.1741 - val_loss: 16.2285 - val_mse: 16.2285\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1739 - mse: 1.1739 - val_loss: 16.2256 - val_mse: 16.2256\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.1738 - mse: 1.1738 - val_loss: 16.2226 - val_mse: 16.2226\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1737 - mse: 1.1737 - val_loss: 16.2196 - val_mse: 16.2196\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1735 - mse: 1.1735 - val_loss: 16.2166 - val_mse: 16.2166\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1734 - mse: 1.1734 - val_loss: 16.2137 - val_mse: 16.2137\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1732 - mse: 1.1732 - val_loss: 16.2106 - val_mse: 16.2106\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1730 - mse: 1.1730 - val_loss: 16.2078 - val_mse: 16.2078\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1766 - mse: 1.1766 - val_loss: 16.2048 - val_mse: 16.2048\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1764 - mse: 1.1764 - val_loss: 16.2017 - val_mse: 16.2017\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1763 - mse: 1.1763 - val_loss: 16.1989 - val_mse: 16.1989\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1761 - mse: 1.1761 - val_loss: 16.1957 - val_mse: 16.1957\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1759 - mse: 1.1759 - val_loss: 16.1929 - val_mse: 16.1929\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1758 - mse: 1.1758 - val_loss: 16.1899 - val_mse: 16.1899\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1756 - mse: 1.1756 - val_loss: 16.1869 - val_mse: 16.1869\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1717 - mse: 1.1717 - val_loss: 16.1841 - val_mse: 16.1841\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1716 - mse: 1.1716 - val_loss: 16.1809 - val_mse: 16.1809\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1714 - mse: 1.1714 - val_loss: 16.1781 - val_mse: 16.1781\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1712 - mse: 1.1712 - val_loss: 16.1751 - val_mse: 16.1751\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1711 - mse: 1.1711 - val_loss: 16.1721 - val_mse: 16.1721\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1709 - mse: 1.1709 - val_loss: 16.1692 - val_mse: 16.1692\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1707 - mse: 1.1707 - val_loss: 16.1662 - val_mse: 16.1662\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1706 - mse: 1.1706 - val_loss: 16.1634 - val_mse: 16.1634\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1704 - mse: 1.1704 - val_loss: 16.1604 - val_mse: 16.1604\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1702 - mse: 1.1702 - val_loss: 16.1575 - val_mse: 16.1575\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1739 - mse: 1.1739 - val_loss: 16.1547 - val_mse: 16.1547\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1737 - mse: 1.1737 - val_loss: 16.1517 - val_mse: 16.1517\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1736 - mse: 1.1736 - val_loss: 16.1488 - val_mse: 16.1488\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1735 - mse: 1.1735 - val_loss: 16.1458 - val_mse: 16.1458\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1733 - mse: 1.1733 - val_loss: 16.1428 - val_mse: 16.1428\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1731 - mse: 1.1731 - val_loss: 16.1400 - val_mse: 16.1400\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1729 - mse: 1.1729 - val_loss: 16.1371 - val_mse: 16.1371\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1690 - mse: 1.1690 - val_loss: 16.1341 - val_mse: 16.1341\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1683 - mse: 1.1683 - val_loss: 16.1314 - val_mse: 16.1314\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1682 - mse: 1.1682 - val_loss: 16.1282 - val_mse: 16.1282\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1680 - mse: 1.1680 - val_loss: 16.1255 - val_mse: 16.1255\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1678 - mse: 1.1678 - val_loss: 16.1224 - val_mse: 16.1224\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1676 - mse: 1.1676 - val_loss: 16.1196 - val_mse: 16.1196\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1674 - mse: 1.1674 - val_loss: 16.1168 - val_mse: 16.1168\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1672 - mse: 1.1672 - val_loss: 16.1141 - val_mse: 16.1141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1671 - mse: 1.1671 - val_loss: 16.1110 - val_mse: 16.1110\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1669 - mse: 1.1669 - val_loss: 16.1082 - val_mse: 16.1082\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1667 - mse: 1.1667 - val_loss: 16.1051 - val_mse: 16.1051\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1702 - mse: 1.1702 - val_loss: 16.1024 - val_mse: 16.1024\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1701 - mse: 1.1701 - val_loss: 16.0997 - val_mse: 16.0997\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1699 - mse: 1.1699 - val_loss: 16.0965 - val_mse: 16.0965\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1696 - mse: 1.1696 - val_loss: 16.0938 - val_mse: 16.0938\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1694 - mse: 1.1694 - val_loss: 16.0910 - val_mse: 16.0910\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1692 - mse: 1.1692 - val_loss: 16.0882 - val_mse: 16.0882\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1689 - mse: 1.1689 - val_loss: 16.0853 - val_mse: 16.0853\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1649 - mse: 1.1649 - val_loss: 16.0826 - val_mse: 16.0826\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1646 - mse: 1.1646 - val_loss: 16.0797 - val_mse: 16.0797\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1643 - mse: 1.1643 - val_loss: 16.0771 - val_mse: 16.0771\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1640 - mse: 1.1640 - val_loss: 16.0743 - val_mse: 16.0743\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1637 - mse: 1.1637 - val_loss: 16.0717 - val_mse: 16.0717\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1633 - mse: 1.1633 - val_loss: 16.0695 - val_mse: 16.0695\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 16.0674 - val_mse: 16.0674\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1627 - mse: 1.1627 - val_loss: 16.0658 - val_mse: 16.0658\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1624 - mse: 1.1624 - val_loss: 16.0644 - val_mse: 16.0644\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1622 - mse: 1.1622 - val_loss: 16.0631 - val_mse: 16.0631\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1658 - mse: 1.1658 - val_loss: 16.0621 - val_mse: 16.0621\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1656 - mse: 1.1656 - val_loss: 16.0612 - val_mse: 16.0612\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1654 - mse: 1.1654 - val_loss: 16.0601 - val_mse: 16.0601\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1652 - mse: 1.1652 - val_loss: 16.0594 - val_mse: 16.0594\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1651 - mse: 1.1651 - val_loss: 16.0585 - val_mse: 16.0585\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1649 - mse: 1.1649 - val_loss: 16.0579 - val_mse: 16.0579\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1647 - mse: 1.1647 - val_loss: 16.0570 - val_mse: 16.0570\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1608 - mse: 1.1608 - val_loss: 16.0562 - val_mse: 16.0562\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1606 - mse: 1.1606 - val_loss: 16.0554 - val_mse: 16.0554\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1604 - mse: 1.1604 - val_loss: 16.0544 - val_mse: 16.0544\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1601 - mse: 1.1601 - val_loss: 16.0529 - val_mse: 16.0529\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1598 - mse: 1.1598 - val_loss: 16.0518 - val_mse: 16.0518\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1597 - mse: 1.1597 - val_loss: 16.0501 - val_mse: 16.0501\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1595 - mse: 1.1595 - val_loss: 16.0482 - val_mse: 16.0482\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1593 - mse: 1.1593 - val_loss: 16.0460 - val_mse: 16.0460\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1591 - mse: 1.1591 - val_loss: 16.0437 - val_mse: 16.0437\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1589 - mse: 1.1589 - val_loss: 16.0411 - val_mse: 16.0411\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1625 - mse: 1.1625 - val_loss: 16.0380 - val_mse: 16.0380\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1622 - mse: 1.1622 - val_loss: 16.0351 - val_mse: 16.0351\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1620 - mse: 1.1620 - val_loss: 16.0318 - val_mse: 16.0318\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1619 - mse: 1.1619 - val_loss: 16.0283 - val_mse: 16.0283\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1617 - mse: 1.1617 - val_loss: 16.0249 - val_mse: 16.0249\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1615 - mse: 1.1615 - val_loss: 16.0212 - val_mse: 16.0212\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1613 - mse: 1.1613 - val_loss: 16.0175 - val_mse: 16.0175\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1573 - mse: 1.1573 - val_loss: 16.0136 - val_mse: 16.0136\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1571 - mse: 1.1571 - val_loss: 16.0100 - val_mse: 16.0100\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1569 - mse: 1.1569 - val_loss: 16.0066 - val_mse: 16.0066\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1567 - mse: 1.1567 - val_loss: 16.0028 - val_mse: 16.0028\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1565 - mse: 1.1565 - val_loss: 15.9994 - val_mse: 15.9994\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1563 - mse: 1.1563 - val_loss: 15.9959 - val_mse: 15.9959\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1561 - mse: 1.1561 - val_loss: 15.9925 - val_mse: 15.9925\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "history = model.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [3]]]\n",
      "Prediction: [2.4599965 3.0220253 5.0503144]\n",
      "Actual: [2 3 5]\n"
     ]
    }
   ],
   "source": [
    "#try on prediction\n",
    "print(X_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(model.predict(X_test))))\n",
    "print(\"Actual: \" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make Bidirectional LSTM\n",
    "bimodel = Sequential()\n",
    "bimodel.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(fib_look, 1)))\n",
    "bimodel.add(Dense(1))\n",
    "bimodel.compile(optimizer='adam', loss='mse',metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 53374070784.0000 - mse: 53374070784.0000 - val_loss: 123757.5000 - val_mse: 123757.5000\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 53166927872.0000 - mse: 53166927872.0000 - val_loss: 123164.1328 - val_mse: 123164.1328\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 52961595392.0000 - mse: 52961595392.0000 - val_loss: 122587.2188 - val_mse: 122587.2188\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 52758085632.0000 - mse: 52758085632.0000 - val_loss: 122024.4766 - val_mse: 122024.4766\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 52556402688.0000 - mse: 52556402688.0000 - val_loss: 121473.6641 - val_mse: 121473.6641\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 52356562944.0000 - mse: 52356562944.0000 - val_loss: 120931.9453 - val_mse: 120931.9453\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 51969654784.0000 - mse: 51969654784.0000 - val_loss: 120559.8438 - val_mse: 120559.8438\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 51831975936.0000 - mse: 51831975936.0000 - val_loss: 120180.8750 - val_mse: 120180.8750\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 51713802240.0000 - mse: 51713802240.0000 - val_loss: 119802.4766 - val_mse: 119802.4766\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 51593293824.0000 - mse: 51593293824.0000 - val_loss: 119427.2422 - val_mse: 119427.2422\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 51470888960.0000 - mse: 51470888960.0000 - val_loss: 119051.0000 - val_mse: 119051.0000\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 51346997248.0000 - mse: 51346997248.0000 - val_loss: 118671.0859 - val_mse: 118671.0859\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 51221856256.0000 - mse: 51221856256.0000 - val_loss: 118286.6484 - val_mse: 118286.6484\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 50749521920.0000 - mse: 50749521920.0000 - val_loss: 117881.0391 - val_mse: 117881.0391\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 50634518528.0000 - mse: 50634518528.0000 - val_loss: 117469.7891 - val_mse: 117469.7891\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 50518798336.0000 - mse: 50518798336.0000 - val_loss: 117062.3125 - val_mse: 117062.3125\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 50402492416.0000 - mse: 50402492416.0000 - val_loss: 116648.0859 - val_mse: 116648.0859\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 50285727744.0000 - mse: 50285727744.0000 - val_loss: 116227.0234 - val_mse: 116227.0234\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 50168623104.0000 - mse: 50168623104.0000 - val_loss: 115790.7500 - val_mse: 115790.7500\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 49089523712.0000 - mse: 49089523712.0000 - val_loss: 115341.3984 - val_mse: 115341.3984\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 48913649664.0000 - mse: 48913649664.0000 - val_loss: 114887.6484 - val_mse: 114887.6484\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 48621703168.0000 - mse: 48621703168.0000 - val_loss: 114432.9375 - val_mse: 114432.9375\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 48494333952.0000 - mse: 48494333952.0000 - val_loss: 113981.0391 - val_mse: 113981.0391\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 48364732416.0000 - mse: 48364732416.0000 - val_loss: 113533.6328 - val_mse: 113533.6328\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 48233394176.0000 - mse: 48233394176.0000 - val_loss: 113089.5391 - val_mse: 113089.5391\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 48100638720.0000 - mse: 48100638720.0000 - val_loss: 112647.0234 - val_mse: 112647.0234\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 47859204096.0000 - mse: 47859204096.0000 - val_loss: 112204.4766 - val_mse: 112204.4766\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 47729152000.0000 - mse: 47729152000.0000 - val_loss: 111759.9141 - val_mse: 111759.9141\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 47523115008.0000 - mse: 47523115008.0000 - val_loss: 111309.4766 - val_mse: 111309.4766\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 47396077568.0000 - mse: 47396077568.0000 - val_loss: 110848.7578 - val_mse: 110848.7578\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 47268249600.0000 - mse: 47268249600.0000 - val_loss: 110371.2266 - val_mse: 110371.2266\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 47139737600.0000 - mse: 47139737600.0000 - val_loss: 109868.4922 - val_mse: 109868.4922\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 47010607104.0000 - mse: 47010607104.0000 - val_loss: 109329.8516 - val_mse: 109329.8516\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 46880911360.0000 - mse: 46880911360.0000 - val_loss: 108741.8125 - val_mse: 108741.8125\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 46750715904.0000 - mse: 46750715904.0000 - val_loss: 108084.8984 - val_mse: 108084.8984\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 46620082176.0000 - mse: 46620082176.0000 - val_loss: 107337.4609 - val_mse: 107337.4609\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 46489026560.0000 - mse: 46489026560.0000 - val_loss: 106502.4297 - val_mse: 106502.4297\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 46357594112.0000 - mse: 46357594112.0000 - val_loss: 105550.0391 - val_mse: 105550.0391\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46225797120.0000 - mse: 46225797120.0000 - val_loss: 104422.1641 - val_mse: 104422.1641\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46093660160.0000 - mse: 46093660160.0000 - val_loss: 103073.9609 - val_mse: 103073.9609\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 45961007104.0000 - mse: 45961007104.0000 - val_loss: 101541.1953 - val_mse: 101541.1953\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 43552591872.0000 - mse: 43552591872.0000 - val_loss: 99816.0000 - val_mse: 99816.0000\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 43448107008.0000 - mse: 43448107008.0000 - val_loss: 97907.0625 - val_mse: 97907.0625\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 42327453696.0000 - mse: 42327453696.0000 - val_loss: 95893.0234 - val_mse: 95893.0234\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 40691515392.0000 - mse: 40691515392.0000 - val_loss: 93768.1484 - val_mse: 93768.1484\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 37738405888.0000 - mse: 37738405888.0000 - val_loss: 91532.2109 - val_mse: 91532.2109\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 37599354880.0000 - mse: 37599354880.0000 - val_loss: 89248.0625 - val_mse: 89248.0625\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 37440983040.0000 - mse: 37440983040.0000 - val_loss: 86950.5703 - val_mse: 86950.5703\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 34601254912.0000 - mse: 34601254912.0000 - val_loss: 84636.4297 - val_mse: 84636.4297\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 34416058368.0000 - mse: 34416058368.0000 - val_loss: 82286.7578 - val_mse: 82286.7578\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 34217732096.0000 - mse: 34217732096.0000 - val_loss: 79902.7891 - val_mse: 79902.7891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 34008279040.0000 - mse: 34008279040.0000 - val_loss: 77501.5781 - val_mse: 77501.5781\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 33311029248.0000 - mse: 33311029248.0000 - val_loss: 75063.2734 - val_mse: 75063.2734\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 33083129856.0000 - mse: 33083129856.0000 - val_loss: 72536.8906 - val_mse: 72536.8906\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 32847693824.0000 - mse: 32847693824.0000 - val_loss: 69888.4453 - val_mse: 69888.4453\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 32606240768.0000 - mse: 32606240768.0000 - val_loss: 67185.3750 - val_mse: 67185.3750\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 32359606272.0000 - mse: 32359606272.0000 - val_loss: 64499.4688 - val_mse: 64499.4688\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 30925150208.0000 - mse: 30925150208.0000 - val_loss: 61784.6758 - val_mse: 61784.6758\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 28111570944.0000 - mse: 28111570944.0000 - val_loss: 58883.8945 - val_mse: 58883.8945\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 26444193792.0000 - mse: 26444193792.0000 - val_loss: 55942.9258 - val_mse: 55942.9258\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 26146600960.0000 - mse: 26146600960.0000 - val_loss: 52971.3438 - val_mse: 52971.3438\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 25836820480.0000 - mse: 25836820480.0000 - val_loss: 49959.6680 - val_mse: 49959.6680\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 24444690432.0000 - mse: 24444690432.0000 - val_loss: 46859.1875 - val_mse: 46859.1875\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 24116193280.0000 - mse: 24116193280.0000 - val_loss: 43601.4805 - val_mse: 43601.4805\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 23779964928.0000 - mse: 23779964928.0000 - val_loss: 40124.5820 - val_mse: 40124.5820\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 23437879296.0000 - mse: 23437879296.0000 - val_loss: 36426.4805 - val_mse: 36426.4805\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 20917977088.0000 - mse: 20917977088.0000 - val_loss: 32683.9688 - val_mse: 32683.9688\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 20565032960.0000 - mse: 20565032960.0000 - val_loss: 29100.8281 - val_mse: 29100.8281\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 20204273664.0000 - mse: 20204273664.0000 - val_loss: 25650.0059 - val_mse: 25650.0059\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 19838402560.0000 - mse: 19838402560.0000 - val_loss: 22116.6621 - val_mse: 22116.6621\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 18986620928.0000 - mse: 18986620928.0000 - val_loss: 18356.5605 - val_mse: 18356.5605\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 18610425856.0000 - mse: 18610425856.0000 - val_loss: 14078.0938 - val_mse: 14078.0938\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 17315461120.0000 - mse: 17315461120.0000 - val_loss: 9534.9746 - val_mse: 9534.9746\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 16925875200.0000 - mse: 16925875200.0000 - val_loss: 5415.7944 - val_mse: 5415.7944\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7937810944.0000 - mse: 7937810944.0000 - val_loss: 2819.9795 - val_mse: 2819.9795\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7155949568.0000 - mse: 7155949568.0000 - val_loss: 1450.6387 - val_mse: 1450.6387\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6805757440.0000 - mse: 6805757440.0000 - val_loss: 756.0071 - val_mse: 756.0071\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 6450282496.0000 - mse: 6450282496.0000 - val_loss: 431.5502 - val_mse: 431.5502\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 6094646784.0000 - mse: 6094646784.0000 - val_loss: 346.7180 - val_mse: 346.7180\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5742174720.0000 - mse: 5742174720.0000 - val_loss: 469.4990 - val_mse: 469.4990\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5336139264.0000 - mse: 5336139264.0000 - val_loss: 795.6832 - val_mse: 795.6832\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 162466672.0000 - mse: 162466672.0000 - val_loss: 1088.5597 - val_mse: 1088.5597\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 21740240.0000 - mse: 21740240.0000 - val_loss: 1349.3326 - val_mse: 1349.3326\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 36945876.0000 - mse: 36945876.0000 - val_loss: 1556.4995 - val_mse: 1556.4995\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 51400892.0000 - mse: 51400892.0000 - val_loss: 1705.3317 - val_mse: 1705.3317\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 63364500.0000 - mse: 63364500.0000 - val_loss: 1799.9528 - val_mse: 1799.9528\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 71933528.0000 - mse: 71933528.0000 - val_loss: 1848.1279 - val_mse: 1848.1279\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 76801376.0000 - mse: 76801376.0000 - val_loss: 1857.8741 - val_mse: 1857.8741\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 78070520.0000 - mse: 78070520.0000 - val_loss: 1836.2587 - val_mse: 1836.2587\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 76116696.0000 - mse: 76116696.0000 - val_loss: 1789.3346 - val_mse: 1789.3346\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 71487720.0000 - mse: 71487720.0000 - val_loss: 1722.3778 - val_mse: 1722.3778\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 64821452.0000 - mse: 64821452.0000 - val_loss: 1640.1093 - val_mse: 1640.1093\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 56783836.0000 - mse: 56783836.0000 - val_loss: 1546.8229 - val_mse: 1546.8229\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 48018092.0000 - mse: 48018092.0000 - val_loss: 1446.3892 - val_mse: 1446.3892\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 39107540.0000 - mse: 39107540.0000 - val_loss: 1342.2588 - val_mse: 1342.2588\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 30549168.0000 - mse: 30549168.0000 - val_loss: 1237.4242 - val_mse: 1237.4242\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 22737110.0000 - mse: 22737110.0000 - val_loss: 1134.3937 - val_mse: 1134.3937\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 15954636.0000 - mse: 15954636.0000 - val_loss: 1035.1891 - val_mse: 1035.1891\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10374013.0000 - mse: 10374013.0000 - val_loss: 941.3501 - val_mse: 941.3501\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6063538.5000 - mse: 6063538.5000 - val_loss: 853.9592 - val_mse: 853.9592\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2999698.7500 - mse: 2999698.7500 - val_loss: 773.6785 - val_mse: 773.6785\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1083459.1250 - mse: 1083459.1250 - val_loss: 700.8028 - val_mse: 700.8028\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 159209.4375 - mse: 159209.4375 - val_loss: 635.3109 - val_mse: 635.3109\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 34944.6211 - mse: 34944.6211 - val_loss: 576.9240 - val_mse: 576.9240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 501957.7812 - mse: 501957.7812 - val_loss: 525.1611 - val_mse: 525.1611\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1352565.7500 - mse: 1352565.7500 - val_loss: 479.3820 - val_mse: 479.3820\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2394981.2500 - mse: 2394981.2500 - val_loss: 438.8370 - val_mse: 438.8370\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3464554.2500 - mse: 3464554.2500 - val_loss: 402.6953 - val_mse: 402.6953\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4430477.5000 - mse: 4430477.5000 - val_loss: 370.0992 - val_mse: 370.0992\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5199366.5000 - mse: 5199366.5000 - val_loss: 340.2493 - val_mse: 340.2493\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5714645.5000 - mse: 5714645.5000 - val_loss: 312.5865 - val_mse: 312.5865\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5953634.5000 - mse: 5953634.5000 - val_loss: 288.3913 - val_mse: 288.3913\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5922208.5000 - mse: 5922208.5000 - val_loss: 283.7536 - val_mse: 283.7536\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5649075.5000 - mse: 5649075.5000 - val_loss: 281.1768 - val_mse: 281.1768\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5178343.5000 - mse: 5178343.5000 - val_loss: 279.5122 - val_mse: 279.5122\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4564039.5000 - mse: 4564039.5000 - val_loss: 278.3622 - val_mse: 278.3622\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3862976.2500 - mse: 3862976.2500 - val_loss: 277.4065 - val_mse: 277.4065\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3130442.2500 - mse: 3130442.2500 - val_loss: 276.2671 - val_mse: 276.2671\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2415937.2500 - mse: 2415937.2500 - val_loss: 274.4538 - val_mse: 274.4538\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1759979.1250 - mse: 1759979.1250 - val_loss: 271.2885 - val_mse: 271.2885\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1192787.8750 - mse: 1192787.8750 - val_loss: 265.7136 - val_mse: 265.7136\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 733421.3125 - mse: 733421.3125 - val_loss: 255.7428 - val_mse: 255.7428\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 389940.6562 - mse: 389940.6562 - val_loss: 237.8683 - val_mse: 237.8683\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 160786.4219 - mse: 160786.4219 - val_loss: 217.6342 - val_mse: 217.6342\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 36087.3594 - mse: 36087.3594 - val_loss: 209.0691 - val_mse: 209.0691\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 108.3117 - mse: 108.3117 - val_loss: 183.2979 - val_mse: 183.2979\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 11716979.0000 - mse: 11716979.0000 - val_loss: 179.6418 - val_mse: 179.6418\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 173431.1719 - mse: 173431.1719 - val_loss: 160.7588 - val_mse: 160.7588\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 383500.9688 - mse: 383500.9688 - val_loss: 129.2334 - val_mse: 129.2334\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7982531.5000 - mse: 7982531.5000 - val_loss: 107.6970 - val_mse: 107.6970\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 366373.5938 - mse: 366373.5938 - val_loss: 86.3100 - val_mse: 86.3100\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1311508.3750 - mse: 1311508.3750 - val_loss: 66.5461 - val_mse: 66.5461\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1551633.1250 - mse: 1551633.1250 - val_loss: 50.5036 - val_mse: 50.5036\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 56310476.0000 - mse: 56310476.0000 - val_loss: 61.2450 - val_mse: 61.2450\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2304468.2500 - mse: 2304468.2500 - val_loss: 71.3676 - val_mse: 71.3676\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2799999.2500 - mse: 2799999.2500 - val_loss: 78.9170 - val_mse: 78.9170\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3150470.0000 - mse: 3150470.0000 - val_loss: 82.7886 - val_mse: 82.7886\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3332934.7500 - mse: 3332934.7500 - val_loss: 82.7282 - val_mse: 82.7282\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3345067.0000 - mse: 3345067.0000 - val_loss: 79.1472 - val_mse: 79.1472\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3201556.0000 - mse: 3201556.0000 - val_loss: 72.8130 - val_mse: 72.8130\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2929715.2500 - mse: 2929715.2500 - val_loss: 64.5652 - val_mse: 64.5652\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 580370.0625 - mse: 580370.0625 - val_loss: 60.0810 - val_mse: 60.0810\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 616064.3125 - mse: 616064.3125 - val_loss: 55.7654 - val_mse: 55.7654\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 581217.5000 - mse: 581217.5000 - val_loss: 51.4223 - val_mse: 51.4223\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 497283.2500 - mse: 497283.2500 - val_loss: 46.9360 - val_mse: 46.9360\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 386413.2500 - mse: 386413.2500 - val_loss: 42.1990 - val_mse: 42.1990\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 269450.3125 - mse: 269450.3125 - val_loss: 37.1265 - val_mse: 37.1265\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 163607.4219 - mse: 163607.4219 - val_loss: 31.9003 - val_mse: 31.9003\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 80813.7188 - mse: 80813.7188 - val_loss: 27.7060 - val_mse: 27.7060\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 27072.9121 - mse: 27072.9121 - val_loss: 24.1158 - val_mse: 24.1158\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2593.3242 - mse: 2593.3242 - val_loss: 20.5921 - val_mse: 20.5921\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2872.3210 - mse: 2872.3210 - val_loss: 17.2748 - val_mse: 17.2748\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 11700876.0000 - mse: 11700876.0000 - val_loss: 13.8366 - val_mse: 13.8366\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 18980456.0000 - mse: 18980456.0000 - val_loss: 12.8833 - val_mse: 12.8833\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 16238405.0000 - mse: 16238405.0000 - val_loss: 12.8778 - val_mse: 12.8778\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12990448.0000 - mse: 12990448.0000 - val_loss: 13.7905 - val_mse: 13.7905\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9652351.0000 - mse: 9652351.0000 - val_loss: 15.3689 - val_mse: 15.3689\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6558918.5000 - mse: 6558918.5000 - val_loss: 17.8440 - val_mse: 17.8440\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3955038.7500 - mse: 3955038.7500 - val_loss: 20.8185 - val_mse: 20.8185\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1991624.0000 - mse: 1991624.0000 - val_loss: 29.1675 - val_mse: 29.1675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 712350.0000 - mse: 712350.0000 - val_loss: 38.1288 - val_mse: 38.1288\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 94401.4141 - mse: 94401.4141 - val_loss: 46.3267 - val_mse: 46.3267\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 35001.8008 - mse: 35001.8008 - val_loss: 52.2399 - val_mse: 52.2399\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 389438.7812 - mse: 389438.7812 - val_loss: 56.4896 - val_mse: 56.4896\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 993399.1875 - mse: 993399.1875 - val_loss: 59.1174 - val_mse: 59.1174\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1686127.5000 - mse: 1686127.5000 - val_loss: 59.9323 - val_mse: 59.9323\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2330039.5000 - mse: 2330039.5000 - val_loss: 58.7048 - val_mse: 58.7048\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2823798.7500 - mse: 2823798.7500 - val_loss: 55.2494 - val_mse: 55.2494\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3107663.2500 - mse: 3107663.2500 - val_loss: 49.5918 - val_mse: 49.5918\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3162770.7500 - mse: 3162770.7500 - val_loss: 42.6859 - val_mse: 42.6859\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3004708.2500 - mse: 3004708.2500 - val_loss: 35.6517 - val_mse: 35.6517\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2674651.2500 - mse: 2674651.2500 - val_loss: 28.5846 - val_mse: 28.5846\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1680216.5000 - mse: 1680216.5000 - val_loss: 21.4377 - val_mse: 21.4377\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 562016.5625 - mse: 562016.5625 - val_loss: 15.3809 - val_mse: 15.3809\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3165553.7500 - mse: 3165553.7500 - val_loss: 15.0020 - val_mse: 15.0020\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 205048.7656 - mse: 205048.7656 - val_loss: 13.3569 - val_mse: 13.3569\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 122607.6172 - mse: 122607.6172 - val_loss: 11.4855 - val_mse: 11.4855\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 58508.4219 - mse: 58508.4219 - val_loss: 9.6321 - val_mse: 9.6321\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 17800.8867 - mse: 17800.8867 - val_loss: 8.0367 - val_mse: 8.0367\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 943.3271 - mse: 943.3271 - val_loss: 7.3632 - val_mse: 7.3632\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4323.5537 - mse: 4323.5537 - val_loss: 6.7592 - val_mse: 6.7592\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 22079.8496 - mse: 22079.8496 - val_loss: 6.2284 - val_mse: 6.2284\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46944.5195 - mse: 46944.5195 - val_loss: 5.7874 - val_mse: 5.7874\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 71996.1797 - mse: 71996.1797 - val_loss: 5.4509 - val_mse: 5.4509\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 91911.4062 - mse: 91911.4062 - val_loss: 5.2369 - val_mse: 5.2369\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1262970.1250 - mse: 1262970.1250 - val_loss: 5.4836 - val_mse: 5.4836\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1123620.2500 - mse: 1123620.2500 - val_loss: 6.1871 - val_mse: 6.1871\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 894830.2500 - mse: 894830.2500 - val_loss: 7.2560 - val_mse: 7.2560\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 633596.5625 - mse: 633596.5625 - val_loss: 8.4733 - val_mse: 8.4733\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 387602.7500 - mse: 387602.7500 - val_loss: 9.7780 - val_mse: 9.7780\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 191126.8594 - mse: 191126.8594 - val_loss: 11.1450 - val_mse: 11.1450\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 62654.6836 - mse: 62654.6836 - val_loss: 12.5217 - val_mse: 12.5217\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5127.4019 - mse: 5127.4019 - val_loss: 13.8254 - val_mse: 13.8254\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 8541.1016 - mse: 8541.1016 - val_loss: 14.9602 - val_mse: 14.9602\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 54476.7773 - mse: 54476.7773 - val_loss: 15.8352 - val_mse: 15.8352\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 121165.7500 - mse: 121165.7500 - val_loss: 16.3814 - val_mse: 16.3814\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 188221.4219 - mse: 188221.4219 - val_loss: 16.5597 - val_mse: 16.5597\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 239946.9219 - mse: 239946.9219 - val_loss: 16.3630 - val_mse: 16.3630\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 267036.2500 - mse: 267036.2500 - val_loss: 15.8162 - val_mse: 15.8162\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 266765.8438 - mse: 266765.8438 - val_loss: 14.9681 - val_mse: 14.9681\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 242025.5469 - mse: 242025.5469 - val_loss: 13.8878 - val_mse: 13.8878\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 199562.8594 - mse: 199562.8594 - val_loss: 12.6574 - val_mse: 12.6574\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 148190.1094 - mse: 148190.1094 - val_loss: 11.3652 - val_mse: 11.3652\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 96724.1172 - mse: 96724.1172 - val_loss: 10.0905 - val_mse: 10.0905\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 52645.0586 - mse: 52645.0586 - val_loss: 8.8793 - val_mse: 8.8793\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 20964.4082 - mse: 20964.4082 - val_loss: 7.7402 - val_mse: 7.7402\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3777.1492 - mse: 3777.1492 - val_loss: 6.6602 - val_mse: 6.6602\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 363.2826 - mse: 363.2826 - val_loss: 5.6213 - val_mse: 5.6213\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7748.1133 - mse: 7748.1133 - val_loss: 4.6023 - val_mse: 4.6023\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 21606.0996 - mse: 21606.0996 - val_loss: 3.5767 - val_mse: 3.5767\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 37256.9570 - mse: 37256.9570 - val_loss: 2.5245 - val_mse: 2.5245\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 50606.3398 - mse: 50606.3398 - val_loss: 1.5800 - val_mse: 1.5800\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 59252.6133 - mse: 59252.6133 - val_loss: 2.3367 - val_mse: 2.3367\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 60623.8789 - mse: 60623.8789 - val_loss: 3.4524 - val_mse: 3.4524\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 56268.1836 - mse: 56268.1836 - val_loss: 4.7050 - val_mse: 4.7050\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 47166.7773 - mse: 47166.7773 - val_loss: 5.9491 - val_mse: 5.9491\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 35440.8086 - mse: 35440.8086 - val_loss: 7.1081 - val_mse: 7.1081\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 23341.2793 - mse: 23341.2793 - val_loss: 8.1681 - val_mse: 8.1681\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12819.3975 - mse: 12819.3975 - val_loss: 9.1273 - val_mse: 9.1273\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5189.4722 - mse: 5189.4722 - val_loss: 9.9873 - val_mse: 9.9873\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1008.5083 - mse: 1008.5083 - val_loss: 10.7459 - val_mse: 10.7459\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 109.2016 - mse: 109.2016 - val_loss: 11.3977 - val_mse: 11.3977\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1776.3573 - mse: 1776.3573 - val_loss: 11.9362 - val_mse: 11.9362\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4976.5376 - mse: 4976.5376 - val_loss: 12.3573 - val_mse: 12.3573\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8606.1885 - mse: 8606.1885 - val_loss: 12.6597 - val_mse: 12.6597\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 11708.1484 - mse: 11708.1484 - val_loss: 12.8468 - val_mse: 12.8468\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 13633.6045 - mse: 13633.6045 - val_loss: 12.9258 - val_mse: 12.9258\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 14075.5156 - mse: 14075.5156 - val_loss: 12.9072 - val_mse: 12.9072\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 13090.5205 - mse: 13090.5205 - val_loss: 12.8051 - val_mse: 12.8051\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10991.5596 - mse: 10991.5596 - val_loss: 12.6350 - val_mse: 12.6350\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 8271.9004 - mse: 8271.9004 - val_loss: 12.4132 - val_mse: 12.4132\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5457.9888 - mse: 5457.9888 - val_loss: 12.1560 - val_mse: 12.1560\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2999.6897 - mse: 2999.6897 - val_loss: 11.8784 - val_mse: 11.8784\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1219.2651 - mse: 1219.2651 - val_loss: 11.5943 - val_mse: 11.5943\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 253.7131 - mse: 253.7131 - val_loss: 11.3154 - val_mse: 11.3154\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 69.4679 - mse: 69.4679 - val_loss: 11.0508 - val_mse: 11.0508\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 494.1632 - mse: 494.1632 - val_loss: 10.8075 - val_mse: 10.8075\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1276.1786 - mse: 1276.1786 - val_loss: 10.5898 - val_mse: 10.5898\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2147.3337 - mse: 2147.3337 - val_loss: 10.4004 - val_mse: 10.4004\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2870.8997 - mse: 2870.8997 - val_loss: 10.2394 - val_mse: 10.2394\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3291.1458 - mse: 3291.1458 - val_loss: 10.1053 - val_mse: 10.1053\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3347.2927 - mse: 3347.2927 - val_loss: 9.9961 - val_mse: 9.9961\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3055.5022 - mse: 3055.5022 - val_loss: 9.9074 - val_mse: 9.9074\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2511.5862 - mse: 2511.5862 - val_loss: 9.8351 - val_mse: 9.8351\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1838.1851 - mse: 1838.1851 - val_loss: 9.7740 - val_mse: 9.7740\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1169.0509 - mse: 1169.0509 - val_loss: 9.7193 - val_mse: 9.7193\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 610.7310 - mse: 610.7310 - val_loss: 9.6655 - val_mse: 9.6655\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 233.9233 - mse: 233.9233 - val_loss: 9.6081 - val_mse: 9.6081\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 60.9030 - mse: 60.9030 - val_loss: 9.5428 - val_mse: 9.5428\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 70.2967 - mse: 70.2967 - val_loss: 9.4667 - val_mse: 9.4667\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 211.4402 - mse: 211.4402 - val_loss: 9.3776 - val_mse: 9.3776\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 418.2402 - mse: 418.2402 - val_loss: 9.2733 - val_mse: 9.2733\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 623.7666 - mse: 623.7666 - val_loss: 9.1536 - val_mse: 9.1536\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 776.5496 - mse: 776.5496 - val_loss: 9.0192 - val_mse: 9.0192\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 845.8659 - mse: 845.8659 - val_loss: 8.8705 - val_mse: 8.8705\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 822.4595 - mse: 822.4595 - val_loss: 8.7099 - val_mse: 8.7099\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 721.9045 - mse: 721.9045 - val_loss: 8.5396 - val_mse: 8.5396\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 570.3466 - mse: 570.3466 - val_loss: 8.3624 - val_mse: 8.3624\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 400.6053 - mse: 400.6053 - val_loss: 8.1810 - val_mse: 8.1810\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 246.1804 - mse: 246.1804 - val_loss: 7.9984 - val_mse: 7.9984\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 127.6699 - mse: 127.6699 - val_loss: 7.8170 - val_mse: 7.8170\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 59.2541 - mse: 59.2541 - val_loss: 7.6391 - val_mse: 7.6391\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 40.0540 - mse: 40.0540 - val_loss: 7.4668 - val_mse: 7.4668\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 61.1422 - mse: 61.1422 - val_loss: 7.3009 - val_mse: 7.3009\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 107.3155 - mse: 107.3155 - val_loss: 7.1431 - val_mse: 7.1431\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 160.5981 - mse: 160.5981 - val_loss: 6.9930 - val_mse: 6.9930\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 205.6533 - mse: 205.6533 - val_loss: 6.8510 - val_mse: 6.8510\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 232.2552 - mse: 232.2552 - val_loss: 6.7162 - val_mse: 6.7162\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 235.6686 - mse: 235.6686 - val_loss: 6.5882 - val_mse: 6.5882\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 216.8252 - mse: 216.8252 - val_loss: 6.4658 - val_mse: 6.4658\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 182.0814 - mse: 182.0814 - val_loss: 6.3478 - val_mse: 6.3478\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 139.8890 - mse: 139.8890 - val_loss: 6.2329 - val_mse: 6.2329\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 97.9283 - mse: 97.9283 - val_loss: 6.1200 - val_mse: 6.1200\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 64.5503 - mse: 64.5503 - val_loss: 6.0076 - val_mse: 6.0076\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 43.0944 - mse: 43.0944 - val_loss: 5.8946 - val_mse: 5.8946\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 34.4978 - mse: 34.4978 - val_loss: 5.7805 - val_mse: 5.7805\n",
      "Epoch 277/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step - loss: 37.1556 - mse: 37.1556 - val_loss: 5.6643 - val_mse: 5.6643\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 47.3072 - mse: 47.3072 - val_loss: 5.5451 - val_mse: 5.5451\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 60.4024 - mse: 60.4024 - val_loss: 5.4231 - val_mse: 5.4231\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 72.2404 - mse: 72.2404 - val_loss: 5.2980 - val_mse: 5.2980\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 79.7698 - mse: 79.7698 - val_loss: 5.1699 - val_mse: 5.1699\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 81.0615 - mse: 81.0615 - val_loss: 5.0389 - val_mse: 5.0389\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 76.7902 - mse: 76.7902 - val_loss: 4.9054 - val_mse: 4.9054\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 67.7515 - mse: 67.7515 - val_loss: 4.7699 - val_mse: 4.7699\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 56.6514 - mse: 56.6514 - val_loss: 4.6326 - val_mse: 4.6326\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 45.1117 - mse: 45.1117 - val_loss: 4.4939 - val_mse: 4.4939\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 35.3830 - mse: 35.3830 - val_loss: 4.3541 - val_mse: 4.3541\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 28.5612 - mse: 28.5612 - val_loss: 4.2135 - val_mse: 4.2135\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 25.0697 - mse: 25.0697 - val_loss: 4.0723 - val_mse: 4.0723\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 24.4726 - mse: 24.4726 - val_loss: 3.9304 - val_mse: 3.9304\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 25.7759 - mse: 25.7759 - val_loss: 3.7878 - val_mse: 3.7878\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 27.7961 - mse: 27.7961 - val_loss: 3.6443 - val_mse: 3.6443\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 29.2852 - mse: 29.2852 - val_loss: 3.4997 - val_mse: 3.4997\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 29.3046 - mse: 29.3046 - val_loss: 3.3540 - val_mse: 3.3540\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 27.5155 - mse: 27.5155 - val_loss: 3.2074 - val_mse: 3.2074\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 24.1302 - mse: 24.1302 - val_loss: 3.0608 - val_mse: 3.0608\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 19.4486 - mse: 19.4486 - val_loss: 2.9164 - val_mse: 2.9164\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 14.7432 - mse: 14.7432 - val_loss: 2.7804 - val_mse: 2.7804\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 11.5354 - mse: 11.5354 - val_loss: 2.6650 - val_mse: 2.6650\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 11.7575 - mse: 11.7575 - val_loss: 2.5926 - val_mse: 2.5926\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 13.3473 - mse: 13.3473 - val_loss: 2.5809 - val_mse: 2.5809\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.8933 - mse: 10.8933 - val_loss: 2.6106 - val_mse: 2.6106\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.9440 - mse: 7.9440 - val_loss: 2.6518 - val_mse: 2.6518\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.7410 - mse: 7.7410 - val_loss: 2.6874 - val_mse: 2.6874\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.1150 - mse: 9.1150 - val_loss: 2.7095 - val_mse: 2.7095\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.7872 - mse: 10.7872 - val_loss: 2.7155 - val_mse: 2.7155\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 11.9679 - mse: 11.9679 - val_loss: 2.7052 - val_mse: 2.7052\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12.4465 - mse: 12.4465 - val_loss: 2.6799 - val_mse: 2.6799\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 12.1669 - mse: 12.1669 - val_loss: 2.6415 - val_mse: 2.6415\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 11.2437 - mse: 11.2437 - val_loss: 2.5922 - val_mse: 2.5922\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.9794 - mse: 9.9794 - val_loss: 2.5342 - val_mse: 2.5342\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8.5252 - mse: 8.5252 - val_loss: 2.4705 - val_mse: 2.4705\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7.2974 - mse: 7.2974 - val_loss: 2.4044 - val_mse: 2.4044\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 6.4494 - mse: 6.4494 - val_loss: 2.3403 - val_mse: 2.3403\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.2433 - mse: 6.2433 - val_loss: 2.2842 - val_mse: 2.2842\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 6.6925 - mse: 6.6925 - val_loss: 2.2419 - val_mse: 2.2419\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.3792 - mse: 7.3792 - val_loss: 2.2178 - val_mse: 2.2178\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7.7766 - mse: 7.7766 - val_loss: 2.2111 - val_mse: 2.2111\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.5944 - mse: 7.5944 - val_loss: 2.2161 - val_mse: 2.2161\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7.1805 - mse: 7.1805 - val_loss: 2.2252 - val_mse: 2.2252\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.9108 - mse: 6.9108 - val_loss: 2.2320 - val_mse: 2.2320\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 6.7834 - mse: 6.7834 - val_loss: 2.2329 - val_mse: 2.2329\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.7392 - mse: 6.7392 - val_loss: 2.2268 - val_mse: 2.2268\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.6500 - mse: 6.6500 - val_loss: 2.2132 - val_mse: 2.2132\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.5063 - mse: 6.5063 - val_loss: 2.1932 - val_mse: 2.1932\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.3575 - mse: 6.3575 - val_loss: 2.1676 - val_mse: 2.1676\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.1707 - mse: 6.1707 - val_loss: 2.1379 - val_mse: 2.1379\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.0033 - mse: 6.0033 - val_loss: 2.1055 - val_mse: 2.1055\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.8873 - mse: 5.8873 - val_loss: 2.0727 - val_mse: 2.0727\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.8443 - mse: 5.8443 - val_loss: 2.0418 - val_mse: 2.0418\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.8757 - mse: 5.8757 - val_loss: 2.0147 - val_mse: 2.0147\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.9608 - mse: 5.9608 - val_loss: 1.9930 - val_mse: 1.9930\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.0225 - mse: 6.0225 - val_loss: 1.9767 - val_mse: 1.9767\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.0094 - mse: 6.0094 - val_loss: 1.9647 - val_mse: 1.9647\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.8958 - mse: 5.8958 - val_loss: 1.9557 - val_mse: 1.9557\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.7461 - mse: 5.7461 - val_loss: 1.9482 - val_mse: 1.9482\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.6044 - mse: 5.6044 - val_loss: 1.9408 - val_mse: 1.9408\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.5248 - mse: 5.5248 - val_loss: 1.9324 - val_mse: 1.9324\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.4831 - mse: 5.4831 - val_loss: 1.9224 - val_mse: 1.9224\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.4759 - mse: 5.4759 - val_loss: 1.9104 - val_mse: 1.9104\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.4757 - mse: 5.4757 - val_loss: 1.8962 - val_mse: 1.8962\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.4639 - mse: 5.4639 - val_loss: 1.8799 - val_mse: 1.8799\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.4417 - mse: 5.4417 - val_loss: 1.8619 - val_mse: 1.8619\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.4069 - mse: 5.4069 - val_loss: 1.8425 - val_mse: 1.8425\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.3552 - mse: 5.3552 - val_loss: 1.8220 - val_mse: 1.8220\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.3135 - mse: 5.3135 - val_loss: 1.8012 - val_mse: 1.8012\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.2904 - mse: 5.2904 - val_loss: 1.7807 - val_mse: 1.7807\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.2758 - mse: 5.2758 - val_loss: 1.7616 - val_mse: 1.7616\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.2438 - mse: 5.2438 - val_loss: 1.7447 - val_mse: 1.7447\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.2115 - mse: 5.2115 - val_loss: 1.7305 - val_mse: 1.7305\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.1606 - mse: 5.1606 - val_loss: 1.7187 - val_mse: 1.7187\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.1154 - mse: 5.1154 - val_loss: 1.7088 - val_mse: 1.7088\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.0782 - mse: 5.0782 - val_loss: 1.6998 - val_mse: 1.6998\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.0499 - mse: 5.0499 - val_loss: 1.6909 - val_mse: 1.6909\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.0305 - mse: 5.0305 - val_loss: 1.6812 - val_mse: 1.6812\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.0155 - mse: 5.0155 - val_loss: 1.6699 - val_mse: 1.6699\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.9994 - mse: 4.9994 - val_loss: 1.6566 - val_mse: 1.6566\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.9767 - mse: 4.9767 - val_loss: 1.6414 - val_mse: 1.6414\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.9501 - mse: 4.9501 - val_loss: 1.6250 - val_mse: 1.6250\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.9198 - mse: 4.9198 - val_loss: 1.6078 - val_mse: 1.6078\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.8893 - mse: 4.8893 - val_loss: 1.5907 - val_mse: 1.5907\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.8610 - mse: 4.8610 - val_loss: 1.5742 - val_mse: 1.5742\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.8323 - mse: 4.8323 - val_loss: 1.5588 - val_mse: 1.5588\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.8092 - mse: 4.8092 - val_loss: 1.5446 - val_mse: 1.5446\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.7841 - mse: 4.7841 - val_loss: 1.5314 - val_mse: 1.5314\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.7639 - mse: 4.7639 - val_loss: 1.5190 - val_mse: 1.5190\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.7354 - mse: 4.7354 - val_loss: 1.5070 - val_mse: 1.5070\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.7108 - mse: 4.7108 - val_loss: 1.4952 - val_mse: 1.4952\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.6825 - mse: 4.6825 - val_loss: 1.4832 - val_mse: 1.4832\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.6642 - mse: 4.6642 - val_loss: 1.4711 - val_mse: 1.4711\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.6402 - mse: 4.6402 - val_loss: 1.4589 - val_mse: 1.4589\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.6179 - mse: 4.6179 - val_loss: 1.4465 - val_mse: 1.4465\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.5961 - mse: 4.5961 - val_loss: 1.4341 - val_mse: 1.4341\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.5735 - mse: 4.5735 - val_loss: 1.4214 - val_mse: 1.4214\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.5515 - mse: 4.5515 - val_loss: 1.4086 - val_mse: 1.4086\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.5217 - mse: 4.5217 - val_loss: 1.3953 - val_mse: 1.3953\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.5001 - mse: 4.5001 - val_loss: 1.3818 - val_mse: 1.3818\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.4787 - mse: 4.4787 - val_loss: 1.3682 - val_mse: 1.3682\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.4537 - mse: 4.4537 - val_loss: 1.3545 - val_mse: 1.3545\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.4340 - mse: 4.4340 - val_loss: 1.3411 - val_mse: 1.3411\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.4137 - mse: 4.4137 - val_loss: 1.3282 - val_mse: 1.3282\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.3915 - mse: 4.3915 - val_loss: 1.3159 - val_mse: 1.3159\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.3693 - mse: 4.3693 - val_loss: 1.3042 - val_mse: 1.3042\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.3474 - mse: 4.3474 - val_loss: 1.2929 - val_mse: 1.2929\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.3267 - mse: 4.3267 - val_loss: 1.2819 - val_mse: 1.2819\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.3051 - mse: 4.3051 - val_loss: 1.2710 - val_mse: 1.2710\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.2838 - mse: 4.2838 - val_loss: 1.2599 - val_mse: 1.2599\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.2622 - mse: 4.2622 - val_loss: 1.2485 - val_mse: 1.2485\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.2407 - mse: 4.2407 - val_loss: 1.2369 - val_mse: 1.2369\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.2201 - mse: 4.2201 - val_loss: 1.2252 - val_mse: 1.2252\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.1988 - mse: 4.1988 - val_loss: 1.2134 - val_mse: 1.2134\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.1762 - mse: 4.1762 - val_loss: 1.2016 - val_mse: 1.2016\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.1547 - mse: 4.1547 - val_loss: 1.1901 - val_mse: 1.1901\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.1360 - mse: 4.1360 - val_loss: 1.1785 - val_mse: 1.1785\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.1148 - mse: 4.1148 - val_loss: 1.1671 - val_mse: 1.1671\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.0942 - mse: 4.0942 - val_loss: 1.1559 - val_mse: 1.1559\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.0735 - mse: 4.0735 - val_loss: 1.1447 - val_mse: 1.1447\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.0521 - mse: 4.0521 - val_loss: 1.1337 - val_mse: 1.1337\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.0321 - mse: 4.0321 - val_loss: 1.1228 - val_mse: 1.1228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.0119 - mse: 4.0119 - val_loss: 1.1122 - val_mse: 1.1122\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.9911 - mse: 3.9911 - val_loss: 1.1016 - val_mse: 1.1016\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.9703 - mse: 3.9703 - val_loss: 1.0914 - val_mse: 1.0914\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.9504 - mse: 3.9504 - val_loss: 1.0813 - val_mse: 1.0813\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.9306 - mse: 3.9306 - val_loss: 1.0712 - val_mse: 1.0712\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.9110 - mse: 3.9110 - val_loss: 1.0612 - val_mse: 1.0612\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8914 - mse: 3.8914 - val_loss: 1.0510 - val_mse: 1.0510\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8720 - mse: 3.8720 - val_loss: 1.0408 - val_mse: 1.0408\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8524 - mse: 3.8524 - val_loss: 1.0306 - val_mse: 1.0306\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8331 - mse: 3.8331 - val_loss: 1.0204 - val_mse: 1.0204\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8141 - mse: 3.8141 - val_loss: 1.0103 - val_mse: 1.0103\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.7950 - mse: 3.7950 - val_loss: 1.0004 - val_mse: 1.0004\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.7759 - mse: 3.7759 - val_loss: 0.9906 - val_mse: 0.9906\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.7571 - mse: 3.7571 - val_loss: 0.9809 - val_mse: 0.9809\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.7381 - mse: 3.7381 - val_loss: 0.9713 - val_mse: 0.9713\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.7194 - mse: 3.7194 - val_loss: 0.9618 - val_mse: 0.9618\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.7011 - mse: 3.7011 - val_loss: 0.9523 - val_mse: 0.9523\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.6826 - mse: 3.6826 - val_loss: 0.9430 - val_mse: 0.9430\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.6640 - mse: 3.6640 - val_loss: 0.9337 - val_mse: 0.9337\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.6455 - mse: 3.6455 - val_loss: 0.9245 - val_mse: 0.9245\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.6273 - mse: 3.6273 - val_loss: 0.9155 - val_mse: 0.9155\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.6092 - mse: 3.6092 - val_loss: 0.9065 - val_mse: 0.9065\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.5914 - mse: 3.5914 - val_loss: 0.8975 - val_mse: 0.8975\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.5739 - mse: 3.5739 - val_loss: 0.8886 - val_mse: 0.8886\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.5561 - mse: 3.5561 - val_loss: 0.8797 - val_mse: 0.8797\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.5385 - mse: 3.5385 - val_loss: 0.8707 - val_mse: 0.8707\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.5206 - mse: 3.5206 - val_loss: 0.8619 - val_mse: 0.8619\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.5031 - mse: 3.5031 - val_loss: 0.8531 - val_mse: 0.8531\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.4863 - mse: 3.4863 - val_loss: 0.8444 - val_mse: 0.8444\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.4690 - mse: 3.4690 - val_loss: 0.8358 - val_mse: 0.8358\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.4513 - mse: 3.4513 - val_loss: 0.8273 - val_mse: 0.8273\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.4342 - mse: 3.4342 - val_loss: 0.8189 - val_mse: 0.8189\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.4168 - mse: 3.4168 - val_loss: 0.8105 - val_mse: 0.8105\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.3999 - mse: 3.3999 - val_loss: 0.8022 - val_mse: 0.8022\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.3831 - mse: 3.3831 - val_loss: 0.7939 - val_mse: 0.7939\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.3662 - mse: 3.3662 - val_loss: 0.7858 - val_mse: 0.7858\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.3491 - mse: 3.3491 - val_loss: 0.7777 - val_mse: 0.7777\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.3324 - mse: 3.3324 - val_loss: 0.7697 - val_mse: 0.7697\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.3161 - mse: 3.3161 - val_loss: 0.7618 - val_mse: 0.7618\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.2992 - mse: 3.2992 - val_loss: 0.7539 - val_mse: 0.7539\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.2828 - mse: 3.2828 - val_loss: 0.7461 - val_mse: 0.7461\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.2665 - mse: 3.2665 - val_loss: 0.7382 - val_mse: 0.7382\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.2501 - mse: 3.2501 - val_loss: 0.7304 - val_mse: 0.7304\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.2338 - mse: 3.2338 - val_loss: 0.7227 - val_mse: 0.7227\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.2176 - mse: 3.2176 - val_loss: 0.7150 - val_mse: 0.7150\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.2013 - mse: 3.2013 - val_loss: 0.7075 - val_mse: 0.7075\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.1853 - mse: 3.1853 - val_loss: 0.6999 - val_mse: 0.6999\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.1695 - mse: 3.1695 - val_loss: 0.6925 - val_mse: 0.6925\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.1536 - mse: 3.1536 - val_loss: 0.6851 - val_mse: 0.6851\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.1379 - mse: 3.1379 - val_loss: 0.6777 - val_mse: 0.6777\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.1220 - mse: 3.1220 - val_loss: 0.6705 - val_mse: 0.6705\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.1065 - mse: 3.1065 - val_loss: 0.6632 - val_mse: 0.6632\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.0909 - mse: 3.0909 - val_loss: 0.6561 - val_mse: 0.6561\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.0754 - mse: 3.0754 - val_loss: 0.6490 - val_mse: 0.6490\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.0601 - mse: 3.0601 - val_loss: 0.6420 - val_mse: 0.6420\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.0450 - mse: 3.0450 - val_loss: 0.6350 - val_mse: 0.6350\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.0299 - mse: 3.0299 - val_loss: 0.6281 - val_mse: 0.6281\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.0148 - mse: 3.0148 - val_loss: 0.6212 - val_mse: 0.6212\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9994 - mse: 2.9994 - val_loss: 0.6143 - val_mse: 0.6143\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9844 - mse: 2.9844 - val_loss: 0.6076 - val_mse: 0.6076\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9695 - mse: 2.9695 - val_loss: 0.6009 - val_mse: 0.6009\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9548 - mse: 2.9548 - val_loss: 0.5942 - val_mse: 0.5942\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9399 - mse: 2.9399 - val_loss: 0.5876 - val_mse: 0.5876\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9257 - mse: 2.9257 - val_loss: 0.5811 - val_mse: 0.5811\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9110 - mse: 2.9110 - val_loss: 0.5746 - val_mse: 0.5746\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.8966 - mse: 2.8966 - val_loss: 0.5681 - val_mse: 0.5681\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.8817 - mse: 2.8817 - val_loss: 0.5618 - val_mse: 0.5618\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.8675 - mse: 2.8675 - val_loss: 0.5555 - val_mse: 0.5555\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.8532 - mse: 2.8532 - val_loss: 0.5492 - val_mse: 0.5492\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.8387 - mse: 2.8387 - val_loss: 0.5430 - val_mse: 0.5430\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.8248 - mse: 2.8248 - val_loss: 0.5369 - val_mse: 0.5369\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.8106 - mse: 2.8106 - val_loss: 0.5307 - val_mse: 0.5307\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.7968 - mse: 2.7968 - val_loss: 0.5247 - val_mse: 0.5247\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.7830 - mse: 2.7830 - val_loss: 0.5186 - val_mse: 0.5186\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.7690 - mse: 2.7690 - val_loss: 0.5127 - val_mse: 0.5127\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7549 - mse: 2.7549 - val_loss: 0.5068 - val_mse: 0.5068\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7415 - mse: 2.7415 - val_loss: 0.5009 - val_mse: 0.5009\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.7280 - mse: 2.7280 - val_loss: 0.4951 - val_mse: 0.4951\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7142 - mse: 2.7142 - val_loss: 0.4894 - val_mse: 0.4894\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.7010 - mse: 2.7010 - val_loss: 0.4836 - val_mse: 0.4836\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6871 - mse: 2.6871 - val_loss: 0.4780 - val_mse: 0.4780\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6739 - mse: 2.6739 - val_loss: 0.4724 - val_mse: 0.4724\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6605 - mse: 2.6605 - val_loss: 0.4669 - val_mse: 0.4669\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6473 - mse: 2.6473 - val_loss: 0.4613 - val_mse: 0.4613\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6343 - mse: 2.6343 - val_loss: 0.4559 - val_mse: 0.4559\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6211 - mse: 2.6211 - val_loss: 0.4504 - val_mse: 0.4504\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6084 - mse: 2.6084 - val_loss: 0.4451 - val_mse: 0.4451\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.5955 - mse: 2.5955 - val_loss: 0.4398 - val_mse: 0.4398\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.5826 - mse: 2.5826 - val_loss: 0.4345 - val_mse: 0.4345\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.5698 - mse: 2.5698 - val_loss: 0.4293 - val_mse: 0.4293\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.5568 - mse: 2.5568 - val_loss: 0.4241 - val_mse: 0.4241\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.5441 - mse: 2.5441 - val_loss: 0.4190 - val_mse: 0.4190\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.5313 - mse: 2.5313 - val_loss: 0.4139 - val_mse: 0.4139\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.5189 - mse: 2.5189 - val_loss: 0.4089 - val_mse: 0.4089\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.5063 - mse: 2.5063 - val_loss: 0.4038 - val_mse: 0.4038\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4942 - mse: 2.4942 - val_loss: 0.3989 - val_mse: 0.3989\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4816 - mse: 2.4816 - val_loss: 0.3940 - val_mse: 0.3940\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4693 - mse: 2.4693 - val_loss: 0.3892 - val_mse: 0.3892\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4573 - mse: 2.4573 - val_loss: 0.3843 - val_mse: 0.3843\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4453 - mse: 2.4453 - val_loss: 0.3795 - val_mse: 0.3795\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4329 - mse: 2.4329 - val_loss: 0.3748 - val_mse: 0.3748\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4209 - mse: 2.4209 - val_loss: 0.3701 - val_mse: 0.3701\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4088 - mse: 2.4088 - val_loss: 0.3655 - val_mse: 0.3655\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3970 - mse: 2.3970 - val_loss: 0.3609 - val_mse: 0.3609\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.3850 - mse: 2.3850 - val_loss: 0.3563 - val_mse: 0.3563\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3733 - mse: 2.3733 - val_loss: 0.3518 - val_mse: 0.3518\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3616 - mse: 2.3616 - val_loss: 0.3473 - val_mse: 0.3473\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3500 - mse: 2.3500 - val_loss: 0.3429 - val_mse: 0.3429\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3383 - mse: 2.3383 - val_loss: 0.3385 - val_mse: 0.3385\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3268 - mse: 2.3268 - val_loss: 0.3342 - val_mse: 0.3342\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3154 - mse: 2.3154 - val_loss: 0.3298 - val_mse: 0.3298\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3038 - mse: 2.3038 - val_loss: 0.3256 - val_mse: 0.3256\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2927 - mse: 2.2927 - val_loss: 0.3213 - val_mse: 0.3213\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2817 - mse: 2.2817 - val_loss: 0.3172 - val_mse: 0.3172\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2706 - mse: 2.2706 - val_loss: 0.3130 - val_mse: 0.3130\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2591 - mse: 2.2591 - val_loss: 0.3089 - val_mse: 0.3089\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2478 - mse: 2.2478 - val_loss: 0.3048 - val_mse: 0.3048\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2368 - mse: 2.2368 - val_loss: 0.3008 - val_mse: 0.3008\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2257 - mse: 2.2257 - val_loss: 0.2968 - val_mse: 0.2968\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2148 - mse: 2.2148 - val_loss: 0.2928 - val_mse: 0.2928\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2040 - mse: 2.2040 - val_loss: 0.2889 - val_mse: 0.2889\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1932 - mse: 2.1932 - val_loss: 0.2850 - val_mse: 0.2850\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1823 - mse: 2.1823 - val_loss: 0.2812 - val_mse: 0.2812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1719 - mse: 2.1719 - val_loss: 0.2774 - val_mse: 0.2774\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1612 - mse: 2.1612 - val_loss: 0.2736 - val_mse: 0.2736\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1507 - mse: 2.1507 - val_loss: 0.2699 - val_mse: 0.2699\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.1401 - mse: 2.1401 - val_loss: 0.2662 - val_mse: 0.2662\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1297 - mse: 2.1297 - val_loss: 0.2625 - val_mse: 0.2625\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1194 - mse: 2.1194 - val_loss: 0.2589 - val_mse: 0.2589\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.1090 - mse: 2.1090 - val_loss: 0.2553 - val_mse: 0.2553\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.0985 - mse: 2.0985 - val_loss: 0.2518 - val_mse: 0.2518\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.0881 - mse: 2.0881 - val_loss: 0.2482 - val_mse: 0.2482\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.0781 - mse: 2.0781 - val_loss: 0.2448 - val_mse: 0.2448\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.0681 - mse: 2.0681 - val_loss: 0.2413 - val_mse: 0.2413\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.0579 - mse: 2.0579 - val_loss: 0.2379 - val_mse: 0.2379\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.0478 - mse: 2.0478 - val_loss: 0.2346 - val_mse: 0.2346\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.0379 - mse: 2.0379 - val_loss: 0.2312 - val_mse: 0.2312\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.0288 - mse: 2.0288 - val_loss: 0.2279 - val_mse: 0.2279\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.0190 - mse: 2.0190 - val_loss: 0.2246 - val_mse: 0.2246\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.0091 - mse: 2.0091 - val_loss: 0.2214 - val_mse: 0.2214\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9993 - mse: 1.9993 - val_loss: 0.2182 - val_mse: 0.2182\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9897 - mse: 1.9897 - val_loss: 0.2150 - val_mse: 0.2150\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9799 - mse: 1.9799 - val_loss: 0.2119 - val_mse: 0.2119\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9704 - mse: 1.9704 - val_loss: 0.2088 - val_mse: 0.2088\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9610 - mse: 1.9610 - val_loss: 0.2057 - val_mse: 0.2057\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9514 - mse: 1.9514 - val_loss: 0.2026 - val_mse: 0.2026\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9419 - mse: 1.9419 - val_loss: 0.1997 - val_mse: 0.1997\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9324 - mse: 1.9324 - val_loss: 0.1967 - val_mse: 0.1967\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9234 - mse: 1.9234 - val_loss: 0.1937 - val_mse: 0.1937\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9141 - mse: 1.9141 - val_loss: 0.1908 - val_mse: 0.1908\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9045 - mse: 1.9045 - val_loss: 0.1879 - val_mse: 0.1879\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8955 - mse: 1.8955 - val_loss: 0.1851 - val_mse: 0.1851\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8862 - mse: 1.8862 - val_loss: 0.1823 - val_mse: 0.1823\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8773 - mse: 1.8773 - val_loss: 0.1795 - val_mse: 0.1795\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8682 - mse: 1.8682 - val_loss: 0.1767 - val_mse: 0.1767\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8592 - mse: 1.8592 - val_loss: 0.1740 - val_mse: 0.1740\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8504 - mse: 1.8504 - val_loss: 0.1713 - val_mse: 0.1713\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8416 - mse: 1.8416 - val_loss: 0.1686 - val_mse: 0.1686\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8326 - mse: 1.8326 - val_loss: 0.1660 - val_mse: 0.1660\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8238 - mse: 1.8238 - val_loss: 0.1633 - val_mse: 0.1633\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8151 - mse: 1.8151 - val_loss: 0.1608 - val_mse: 0.1608\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8064 - mse: 1.8064 - val_loss: 0.1582 - val_mse: 0.1582\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7977 - mse: 1.7977 - val_loss: 0.1557 - val_mse: 0.1557\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7889 - mse: 1.7889 - val_loss: 0.1532 - val_mse: 0.1532\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7804 - mse: 1.7804 - val_loss: 0.1507 - val_mse: 0.1507\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7718 - mse: 1.7718 - val_loss: 0.1483 - val_mse: 0.1483\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7637 - mse: 1.7637 - val_loss: 0.1459 - val_mse: 0.1459\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.7550 - mse: 1.7550 - val_loss: 0.1435 - val_mse: 0.1435\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7467 - mse: 1.7467 - val_loss: 0.1411 - val_mse: 0.1411\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7384 - mse: 1.7384 - val_loss: 0.1388 - val_mse: 0.1388\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7301 - mse: 1.7301 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7218 - mse: 1.7218 - val_loss: 0.1342 - val_mse: 0.1342\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7138 - mse: 1.7138 - val_loss: 0.1320 - val_mse: 0.1320\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7055 - mse: 1.7055 - val_loss: 0.1298 - val_mse: 0.1298\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6974 - mse: 1.6974 - val_loss: 0.1276 - val_mse: 0.1276\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6893 - mse: 1.6893 - val_loss: 0.1254 - val_mse: 0.1254\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6813 - mse: 1.6813 - val_loss: 0.1233 - val_mse: 0.1233\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6732 - mse: 1.6732 - val_loss: 0.1211 - val_mse: 0.1211\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6653 - mse: 1.6653 - val_loss: 0.1190 - val_mse: 0.1190\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6575 - mse: 1.6575 - val_loss: 0.1170 - val_mse: 0.1170\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6496 - mse: 1.6496 - val_loss: 0.1149 - val_mse: 0.1149\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6419 - mse: 1.6419 - val_loss: 0.1129 - val_mse: 0.1129\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6341 - mse: 1.6341 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6264 - mse: 1.6264 - val_loss: 0.1090 - val_mse: 0.1090\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6185 - mse: 1.6185 - val_loss: 0.1070 - val_mse: 0.1070\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6110 - mse: 1.6110 - val_loss: 0.1051 - val_mse: 0.1051\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6035 - mse: 1.6035 - val_loss: 0.1032 - val_mse: 0.1032\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5959 - mse: 1.5959 - val_loss: 0.1013 - val_mse: 0.1013\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5884 - mse: 1.5884 - val_loss: 0.0995 - val_mse: 0.0995\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5812 - mse: 1.5812 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5735 - mse: 1.5735 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5661 - mse: 1.5661 - val_loss: 0.0941 - val_mse: 0.0941\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5590 - mse: 1.5590 - val_loss: 0.0923 - val_mse: 0.0923\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5516 - mse: 1.5516 - val_loss: 0.0906 - val_mse: 0.0906\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5443 - mse: 1.5443 - val_loss: 0.0889 - val_mse: 0.0889\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5371 - mse: 1.5371 - val_loss: 0.0872 - val_mse: 0.0872\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5300 - mse: 1.5300 - val_loss: 0.0856 - val_mse: 0.0856\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5228 - mse: 1.5228 - val_loss: 0.0839 - val_mse: 0.0839\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5157 - mse: 1.5157 - val_loss: 0.0823 - val_mse: 0.0823\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5085 - mse: 1.5085 - val_loss: 0.0807 - val_mse: 0.0807\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5015 - mse: 1.5015 - val_loss: 0.0791 - val_mse: 0.0791\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4946 - mse: 1.4946 - val_loss: 0.0776 - val_mse: 0.0776\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4876 - mse: 1.4876 - val_loss: 0.0761 - val_mse: 0.0761\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4807 - mse: 1.4807 - val_loss: 0.0745 - val_mse: 0.0745\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4740 - mse: 1.4740 - val_loss: 0.0731 - val_mse: 0.0731\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4671 - mse: 1.4671 - val_loss: 0.0716 - val_mse: 0.0716\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4601 - mse: 1.4601 - val_loss: 0.0701 - val_mse: 0.0701\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4533 - mse: 1.4533 - val_loss: 0.0687 - val_mse: 0.0687\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4468 - mse: 1.4468 - val_loss: 0.0673 - val_mse: 0.0673\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4400 - mse: 1.4400 - val_loss: 0.0659 - val_mse: 0.0659\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4327 - mse: 1.4327 - val_loss: 0.0645 - val_mse: 0.0645\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4259 - mse: 1.4259 - val_loss: 0.0632 - val_mse: 0.0632\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4193 - mse: 1.4193 - val_loss: 0.0619 - val_mse: 0.0619\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4128 - mse: 1.4128 - val_loss: 0.0606 - val_mse: 0.0606\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4064 - mse: 1.4064 - val_loss: 0.0593 - val_mse: 0.0593\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4000 - mse: 1.4000 - val_loss: 0.0580 - val_mse: 0.0580\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3935 - mse: 1.3935 - val_loss: 0.0568 - val_mse: 0.0568\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3871 - mse: 1.3871 - val_loss: 0.0555 - val_mse: 0.0555\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3809 - mse: 1.3809 - val_loss: 0.0543 - val_mse: 0.0543\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3744 - mse: 1.3744 - val_loss: 0.0531 - val_mse: 0.0531\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3682 - mse: 1.3682 - val_loss: 0.0520 - val_mse: 0.0520\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3619 - mse: 1.3619 - val_loss: 0.0508 - val_mse: 0.0508\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3557 - mse: 1.3557 - val_loss: 0.0497 - val_mse: 0.0497\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3497 - mse: 1.3497 - val_loss: 0.0486 - val_mse: 0.0486\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3434 - mse: 1.3434 - val_loss: 0.0475 - val_mse: 0.0475\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3373 - mse: 1.3373 - val_loss: 0.0464 - val_mse: 0.0464\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3312 - mse: 1.3312 - val_loss: 0.0453 - val_mse: 0.0453\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3252 - mse: 1.3252 - val_loss: 0.0443 - val_mse: 0.0443\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3191 - mse: 1.3191 - val_loss: 0.0432 - val_mse: 0.0432\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3133 - mse: 1.3133 - val_loss: 0.0422 - val_mse: 0.0422\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3073 - mse: 1.3073 - val_loss: 0.0412 - val_mse: 0.0412\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3012 - mse: 1.3012 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2955 - mse: 1.2955 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2896 - mse: 1.2896 - val_loss: 0.0383 - val_mse: 0.0383\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2838 - mse: 1.2838 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2781 - mse: 1.2781 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2725 - mse: 1.2725 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2667 - mse: 1.2667 - val_loss: 0.0347 - val_mse: 0.0347\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2611 - mse: 1.2611 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2553 - mse: 1.2553 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2496 - mse: 1.2496 - val_loss: 0.0322 - val_mse: 0.0322\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2442 - mse: 1.2442 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2386 - mse: 1.2386 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2330 - mse: 1.2330 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2277 - mse: 1.2277 - val_loss: 0.0290 - val_mse: 0.0290\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2222 - mse: 1.2222 - val_loss: 0.0283 - val_mse: 0.0283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2167 - mse: 1.2167 - val_loss: 0.0275 - val_mse: 0.0275\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2114 - mse: 1.2114 - val_loss: 0.0268 - val_mse: 0.0268\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2061 - mse: 1.2061 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2005 - mse: 1.2005 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1954 - mse: 1.1954 - val_loss: 0.0247 - val_mse: 0.0247\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1902 - mse: 1.1902 - val_loss: 0.0241 - val_mse: 0.0241\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1848 - mse: 1.1848 - val_loss: 0.0234 - val_mse: 0.0234\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1795 - mse: 1.1795 - val_loss: 0.0228 - val_mse: 0.0228\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1744 - mse: 1.1744 - val_loss: 0.0221 - val_mse: 0.0221\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1694 - mse: 1.1694 - val_loss: 0.0215 - val_mse: 0.0215\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1641 - mse: 1.1641 - val_loss: 0.0209 - val_mse: 0.0209\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1591 - mse: 1.1591 - val_loss: 0.0204 - val_mse: 0.0204\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1539 - mse: 1.1539 - val_loss: 0.0198 - val_mse: 0.0198\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1491 - mse: 1.1491 - val_loss: 0.0192 - val_mse: 0.0192\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1440 - mse: 1.1440 - val_loss: 0.0187 - val_mse: 0.0187\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1390 - mse: 1.1390 - val_loss: 0.0182 - val_mse: 0.0182\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1340 - mse: 1.1340 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1292 - mse: 1.1292 - val_loss: 0.0172 - val_mse: 0.0172\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1243 - mse: 1.1243 - val_loss: 0.0167 - val_mse: 0.0167\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1193 - mse: 1.1193 - val_loss: 0.0162 - val_mse: 0.0162\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1146 - mse: 1.1146 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1097 - mse: 1.1097 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1051 - mse: 1.1051 - val_loss: 0.0149 - val_mse: 0.0149\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1001 - mse: 1.1001 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0954 - mse: 1.0954 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0908 - mse: 1.0908 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0861 - mse: 1.0861 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0814 - mse: 1.0814 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0767 - mse: 1.0767 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0722 - mse: 1.0722 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0676 - mse: 1.0676 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0630 - mse: 1.0630 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0584 - mse: 1.0584 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0540 - mse: 1.0540 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0495 - mse: 1.0495 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0452 - mse: 1.0452 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0407 - mse: 1.0407 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0363 - mse: 1.0363 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0320 - mse: 1.0320 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0276 - mse: 1.0276 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0233 - mse: 1.0233 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0189 - mse: 1.0189 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0147 - mse: 1.0147 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0106 - mse: 1.0106 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0063 - mse: 1.0063 - val_loss: 0.0082 - val_mse: 0.0082\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0022 - mse: 1.0022 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9979 - mse: 0.9979 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9938 - mse: 0.9938 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9897 - mse: 0.9897 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9854 - mse: 0.9854 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9814 - mse: 0.9814 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9773 - mse: 0.9773 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9732 - mse: 0.9732 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9695 - mse: 0.9695 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9653 - mse: 0.9653 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9615 - mse: 0.9615 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9575 - mse: 0.9575 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9536 - mse: 0.9536 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9497 - mse: 0.9497 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9457 - mse: 0.9457 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9418 - mse: 0.9418 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.9381 - mse: 0.9381 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9344 - mse: 0.9344 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9305 - mse: 0.9305 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9266 - mse: 0.9266 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9230 - mse: 0.9230 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9193 - mse: 0.9193 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9156 - mse: 0.9156 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9119 - mse: 0.9119 - val_loss: 0.0066 - val_mse: 0.0066\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9083 - mse: 0.9083 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9047 - mse: 0.9047 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9011 - mse: 0.9011 - val_loss: 0.0068 - val_mse: 0.0068\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8974 - mse: 0.8974 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8938 - mse: 0.8938 - val_loss: 0.0069 - val_mse: 0.0069\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8903 - mse: 0.8903 - val_loss: 0.0070 - val_mse: 0.0070\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8868 - mse: 0.8868 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8833 - mse: 0.8833 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8796 - mse: 0.8796 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8763 - mse: 0.8763 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8727 - mse: 0.8727 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8694 - mse: 0.8694 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8659 - mse: 0.8659 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8625 - mse: 0.8625 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8591 - mse: 0.8591 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8556 - mse: 0.8556 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8523 - mse: 0.8523 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8491 - mse: 0.8491 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8458 - mse: 0.8458 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8424 - mse: 0.8424 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8392 - mse: 0.8392 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8363 - mse: 0.8363 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8331 - mse: 0.8331 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8297 - mse: 0.8297 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8257 - mse: 0.8257 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8225 - mse: 0.8225 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8193 - mse: 0.8193 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8163 - mse: 0.8163 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8131 - mse: 0.8131 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8101 - mse: 0.8101 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8069 - mse: 0.8069 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8039 - mse: 0.8039 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8009 - mse: 0.8009 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7979 - mse: 0.7979 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7949 - mse: 0.7949 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7918 - mse: 0.7918 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7889 - mse: 0.7889 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7859 - mse: 0.7859 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7829 - mse: 0.7829 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7800 - mse: 0.7800 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7771 - mse: 0.7771 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7741 - mse: 0.7741 - val_loss: 0.0149 - val_mse: 0.0149\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7714 - mse: 0.7714 - val_loss: 0.0152 - val_mse: 0.0152\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7685 - mse: 0.7685 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7656 - mse: 0.7656 - val_loss: 0.0159 - val_mse: 0.0159\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7629 - mse: 0.7629 - val_loss: 0.0163 - val_mse: 0.0163\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7599 - mse: 0.7599 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7574 - mse: 0.7574 - val_loss: 0.0170 - val_mse: 0.0170\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7546 - mse: 0.7546 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7518 - mse: 0.7518 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7490 - mse: 0.7490 - val_loss: 0.0181 - val_mse: 0.0181\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7462 - mse: 0.7462 - val_loss: 0.0185 - val_mse: 0.0185\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7435 - mse: 0.7435 - val_loss: 0.0189 - val_mse: 0.0189\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7409 - mse: 0.7409 - val_loss: 0.0193 - val_mse: 0.0193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.0197 - val_mse: 0.0197\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7355 - mse: 0.7355 - val_loss: 0.0201 - val_mse: 0.0201\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7329 - mse: 0.7329 - val_loss: 0.0206 - val_mse: 0.0206\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7302 - mse: 0.7302 - val_loss: 0.0210 - val_mse: 0.0210\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7276 - mse: 0.7276 - val_loss: 0.0214 - val_mse: 0.0214\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7252 - mse: 0.7252 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7225 - mse: 0.7225 - val_loss: 0.0223 - val_mse: 0.0223\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7199 - mse: 0.7199 - val_loss: 0.0227 - val_mse: 0.0227\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7174 - mse: 0.7174 - val_loss: 0.0232 - val_mse: 0.0232\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7148 - mse: 0.7148 - val_loss: 0.0236 - val_mse: 0.0236\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7122 - mse: 0.7122 - val_loss: 0.0241 - val_mse: 0.0241\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7097 - mse: 0.7097 - val_loss: 0.0245 - val_mse: 0.0245\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7072 - mse: 0.7072 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7047 - mse: 0.7047 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7024 - mse: 0.7024 - val_loss: 0.0259 - val_mse: 0.0259\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7000 - mse: 0.7000 - val_loss: 0.0264 - val_mse: 0.0264\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.0269 - val_mse: 0.0269\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6952 - mse: 0.6952 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6927 - mse: 0.6927 - val_loss: 0.0278 - val_mse: 0.0278\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6902 - mse: 0.6902 - val_loss: 0.0283 - val_mse: 0.0283\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6879 - mse: 0.6879 - val_loss: 0.0288 - val_mse: 0.0288\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6855 - mse: 0.6855 - val_loss: 0.0293 - val_mse: 0.0293\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6831 - mse: 0.6831 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6808 - mse: 0.6808 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6784 - mse: 0.6784 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6760 - mse: 0.6760 - val_loss: 0.0314 - val_mse: 0.0314\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6738 - mse: 0.6738 - val_loss: 0.0319 - val_mse: 0.0319\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6714 - mse: 0.6714 - val_loss: 0.0325 - val_mse: 0.0325\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6691 - mse: 0.6691 - val_loss: 0.0330 - val_mse: 0.0330\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6669 - mse: 0.6669 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6647 - mse: 0.6647 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6624 - mse: 0.6624 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6602 - mse: 0.6602 - val_loss: 0.0352 - val_mse: 0.0352\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6580 - mse: 0.6580 - val_loss: 0.0357 - val_mse: 0.0357\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6558 - mse: 0.6558 - val_loss: 0.0363 - val_mse: 0.0363\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6536 - mse: 0.6536 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6514 - mse: 0.6514 - val_loss: 0.0374 - val_mse: 0.0374\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6494 - mse: 0.6494 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6472 - mse: 0.6472 - val_loss: 0.0385 - val_mse: 0.0385\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6450 - mse: 0.6450 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6429 - mse: 0.6429 - val_loss: 0.0397 - val_mse: 0.0397\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6408 - mse: 0.6408 - val_loss: 0.0403 - val_mse: 0.0403\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6387 - mse: 0.6387 - val_loss: 0.0409 - val_mse: 0.0409\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6365 - mse: 0.6365 - val_loss: 0.0415 - val_mse: 0.0415\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6345 - mse: 0.6345 - val_loss: 0.0420 - val_mse: 0.0420\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6323 - mse: 0.6323 - val_loss: 0.0426 - val_mse: 0.0426\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6303 - mse: 0.6303 - val_loss: 0.0432 - val_mse: 0.0432\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6282 - mse: 0.6282 - val_loss: 0.0438 - val_mse: 0.0438\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6262 - mse: 0.6262 - val_loss: 0.0444 - val_mse: 0.0444\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6242 - mse: 0.6242 - val_loss: 0.0451 - val_mse: 0.0451\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6222 - mse: 0.6222 - val_loss: 0.0457 - val_mse: 0.0457\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6203 - mse: 0.6203 - val_loss: 0.0463 - val_mse: 0.0463\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6184 - mse: 0.6184 - val_loss: 0.0469 - val_mse: 0.0469\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6164 - mse: 0.6164 - val_loss: 0.0475 - val_mse: 0.0475\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6145 - mse: 0.6145 - val_loss: 0.0482 - val_mse: 0.0482\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6125 - mse: 0.6125 - val_loss: 0.0488 - val_mse: 0.0488\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6107 - mse: 0.6107 - val_loss: 0.0494 - val_mse: 0.0494\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6087 - mse: 0.6087 - val_loss: 0.0501 - val_mse: 0.0501\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6068 - mse: 0.6068 - val_loss: 0.0507 - val_mse: 0.0507\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6047 - mse: 0.6047 - val_loss: 0.0513 - val_mse: 0.0513\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6028 - mse: 0.6028 - val_loss: 0.0520 - val_mse: 0.0520\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6010 - mse: 0.6010 - val_loss: 0.0526 - val_mse: 0.0526\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5991 - mse: 0.5991 - val_loss: 0.0533 - val_mse: 0.0533\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5981 - mse: 0.5981 - val_loss: 0.0539 - val_mse: 0.0539\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5963 - mse: 0.5963 - val_loss: 0.0546 - val_mse: 0.0546\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5944 - mse: 0.5944 - val_loss: 0.0553 - val_mse: 0.0553\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5926 - mse: 0.5926 - val_loss: 0.0559 - val_mse: 0.0559\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5908 - mse: 0.5908 - val_loss: 0.0566 - val_mse: 0.0566\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5891 - mse: 0.5891 - val_loss: 0.0573 - val_mse: 0.0573\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5864 - mse: 0.5864 - val_loss: 0.0579 - val_mse: 0.0579\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5845 - mse: 0.5845 - val_loss: 0.0586 - val_mse: 0.0586\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5827 - mse: 0.5827 - val_loss: 0.0593 - val_mse: 0.0593\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5810 - mse: 0.5810 - val_loss: 0.0600 - val_mse: 0.0600\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5802 - mse: 0.5802 - val_loss: 0.0607 - val_mse: 0.0607\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5784 - mse: 0.5784 - val_loss: 0.0614 - val_mse: 0.0614\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5767 - mse: 0.5767 - val_loss: 0.0620 - val_mse: 0.0620\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5751 - mse: 0.5751 - val_loss: 0.0627 - val_mse: 0.0627\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5733 - mse: 0.5733 - val_loss: 0.0634 - val_mse: 0.0634\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5714 - mse: 0.5714 - val_loss: 0.0641 - val_mse: 0.0641\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5698 - mse: 0.5698 - val_loss: 0.0648 - val_mse: 0.0648\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5681 - mse: 0.5681 - val_loss: 0.0655 - val_mse: 0.0655\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5666 - mse: 0.5666 - val_loss: 0.0662 - val_mse: 0.0662\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5649 - mse: 0.5649 - val_loss: 0.0670 - val_mse: 0.0670\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5632 - mse: 0.5632 - val_loss: 0.0677 - val_mse: 0.0677\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5616 - mse: 0.5616 - val_loss: 0.0684 - val_mse: 0.0684\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5601 - mse: 0.5601 - val_loss: 0.0691 - val_mse: 0.0691\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5584 - mse: 0.5584 - val_loss: 0.0698 - val_mse: 0.0698\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5567 - mse: 0.5567 - val_loss: 0.0705 - val_mse: 0.0705\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5551 - mse: 0.5551 - val_loss: 0.0712 - val_mse: 0.0712\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5535 - mse: 0.5535 - val_loss: 0.0720 - val_mse: 0.0720\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5518 - mse: 0.5518 - val_loss: 0.0727 - val_mse: 0.0727\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5504 - mse: 0.5504 - val_loss: 0.0734 - val_mse: 0.0734\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5487 - mse: 0.5487 - val_loss: 0.0742 - val_mse: 0.0742\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5474 - mse: 0.5474 - val_loss: 0.0749 - val_mse: 0.0749\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5459 - mse: 0.5459 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5443 - mse: 0.5443 - val_loss: 0.0764 - val_mse: 0.0764\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5427 - mse: 0.5427 - val_loss: 0.0771 - val_mse: 0.0771\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5411 - mse: 0.5411 - val_loss: 0.0778 - val_mse: 0.0778\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5396 - mse: 0.5396 - val_loss: 0.0786 - val_mse: 0.0786\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5380 - mse: 0.5380 - val_loss: 0.0793 - val_mse: 0.0793\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5367 - mse: 0.5367 - val_loss: 0.0801 - val_mse: 0.0801\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5351 - mse: 0.5351 - val_loss: 0.0808 - val_mse: 0.0808\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5336 - mse: 0.5336 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5322 - mse: 0.5322 - val_loss: 0.0823 - val_mse: 0.0823\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5307 - mse: 0.5307 - val_loss: 0.0831 - val_mse: 0.0831\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5292 - mse: 0.5292 - val_loss: 0.0839 - val_mse: 0.0839\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5277 - mse: 0.5277 - val_loss: 0.0846 - val_mse: 0.0846\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5262 - mse: 0.5262 - val_loss: 0.0854 - val_mse: 0.0854\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5247 - mse: 0.5247 - val_loss: 0.0861 - val_mse: 0.0861\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5233 - mse: 0.5233 - val_loss: 0.0869 - val_mse: 0.0869\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5221 - mse: 0.5221 - val_loss: 0.0877 - val_mse: 0.0877\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5206 - mse: 0.5206 - val_loss: 0.0885 - val_mse: 0.0885\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5194 - mse: 0.5194 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5179 - mse: 0.5179 - val_loss: 0.0900 - val_mse: 0.0900\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.0908 - val_mse: 0.0908\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5151 - mse: 0.5151 - val_loss: 0.0916 - val_mse: 0.0916\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5138 - mse: 0.5138 - val_loss: 0.0923 - val_mse: 0.0923\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5125 - mse: 0.5125 - val_loss: 0.0931 - val_mse: 0.0931\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5111 - mse: 0.5111 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5099 - mse: 0.5099 - val_loss: 0.0947 - val_mse: 0.0947\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5085 - mse: 0.5085 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5071 - mse: 0.5071 - val_loss: 0.0963 - val_mse: 0.0963\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5058 - mse: 0.5058 - val_loss: 0.0971 - val_mse: 0.0971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5045 - mse: 0.5045 - val_loss: 0.0979 - val_mse: 0.0979\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5033 - mse: 0.5033 - val_loss: 0.0986 - val_mse: 0.0986\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5020 - mse: 0.5020 - val_loss: 0.0995 - val_mse: 0.0995\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5007 - mse: 0.5007 - val_loss: 0.1002 - val_mse: 0.1002\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4994 - mse: 0.4994 - val_loss: 0.1010 - val_mse: 0.1010\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4981 - mse: 0.4981 - val_loss: 0.1018 - val_mse: 0.1018\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4969 - mse: 0.4969 - val_loss: 0.1026 - val_mse: 0.1026\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4956 - mse: 0.4956 - val_loss: 0.1034 - val_mse: 0.1034\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4943 - mse: 0.4943 - val_loss: 0.1043 - val_mse: 0.1043\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4930 - mse: 0.4930 - val_loss: 0.1050 - val_mse: 0.1050\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4917 - mse: 0.4917 - val_loss: 0.1059 - val_mse: 0.1059\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4904 - mse: 0.4904 - val_loss: 0.1067 - val_mse: 0.1067\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4894 - mse: 0.4894 - val_loss: 0.1075 - val_mse: 0.1075\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4883 - mse: 0.4883 - val_loss: 0.1083 - val_mse: 0.1083\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4871 - mse: 0.4871 - val_loss: 0.1091 - val_mse: 0.1091\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4859 - mse: 0.4859 - val_loss: 0.1099 - val_mse: 0.1099\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4846 - mse: 0.4846 - val_loss: 0.1107 - val_mse: 0.1107\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4835 - mse: 0.4835 - val_loss: 0.1116 - val_mse: 0.1116\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.1124 - val_mse: 0.1124\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4811 - mse: 0.4811 - val_loss: 0.1132 - val_mse: 0.1132\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4800 - mse: 0.4800 - val_loss: 0.1140 - val_mse: 0.1140\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4788 - mse: 0.4788 - val_loss: 0.1148 - val_mse: 0.1148\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4775 - mse: 0.4775 - val_loss: 0.1157 - val_mse: 0.1157\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4764 - mse: 0.4764 - val_loss: 0.1165 - val_mse: 0.1165\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4754 - mse: 0.4754 - val_loss: 0.1173 - val_mse: 0.1173\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4742 - mse: 0.4742 - val_loss: 0.1181 - val_mse: 0.1181\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4731 - mse: 0.4731 - val_loss: 0.1190 - val_mse: 0.1190\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4721 - mse: 0.4721 - val_loss: 0.1198 - val_mse: 0.1198\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4710 - mse: 0.4710 - val_loss: 0.1206 - val_mse: 0.1206\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4698 - mse: 0.4698 - val_loss: 0.1215 - val_mse: 0.1215\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4689 - mse: 0.4689 - val_loss: 0.1223 - val_mse: 0.1223\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4677 - mse: 0.4677 - val_loss: 0.1231 - val_mse: 0.1231\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4666 - mse: 0.4666 - val_loss: 0.1240 - val_mse: 0.1240\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4656 - mse: 0.4656 - val_loss: 0.1248 - val_mse: 0.1248\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4646 - mse: 0.4646 - val_loss: 0.1257 - val_mse: 0.1257\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4635 - mse: 0.4635 - val_loss: 0.1265 - val_mse: 0.1265\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4625 - mse: 0.4625 - val_loss: 0.1273 - val_mse: 0.1273\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4614 - mse: 0.4614 - val_loss: 0.1282 - val_mse: 0.1282\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4603 - mse: 0.4603 - val_loss: 0.1290 - val_mse: 0.1290\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4591 - mse: 0.4591 - val_loss: 0.1299 - val_mse: 0.1299\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4581 - mse: 0.4581 - val_loss: 0.1307 - val_mse: 0.1307\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4572 - mse: 0.4572 - val_loss: 0.1316 - val_mse: 0.1316\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4561 - mse: 0.4561 - val_loss: 0.1324 - val_mse: 0.1324\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4553 - mse: 0.4553 - val_loss: 0.1332 - val_mse: 0.1332\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4543 - mse: 0.4543 - val_loss: 0.1341 - val_mse: 0.1341\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4533 - mse: 0.4533 - val_loss: 0.1349 - val_mse: 0.1349\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4523 - mse: 0.4523 - val_loss: 0.1358 - val_mse: 0.1358\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4513 - mse: 0.4513 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4502 - mse: 0.4502 - val_loss: 0.1375 - val_mse: 0.1375\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4493 - mse: 0.4493 - val_loss: 0.1383 - val_mse: 0.1383\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4482 - mse: 0.4482 - val_loss: 0.1392 - val_mse: 0.1392\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4474 - mse: 0.4474 - val_loss: 0.1401 - val_mse: 0.1401\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4464 - mse: 0.4464 - val_loss: 0.1409 - val_mse: 0.1409\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4455 - mse: 0.4455 - val_loss: 0.1418 - val_mse: 0.1418\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4445 - mse: 0.4445 - val_loss: 0.1426 - val_mse: 0.1426\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4434 - mse: 0.4434 - val_loss: 0.1435 - val_mse: 0.1435\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4425 - mse: 0.4425 - val_loss: 0.1443 - val_mse: 0.1443\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4415 - mse: 0.4415 - val_loss: 0.1452 - val_mse: 0.1452\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4407 - mse: 0.4407 - val_loss: 0.1461 - val_mse: 0.1461\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4397 - mse: 0.4397 - val_loss: 0.1469 - val_mse: 0.1469\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4389 - mse: 0.4389 - val_loss: 0.1478 - val_mse: 0.1478\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4379 - mse: 0.4379 - val_loss: 0.1486 - val_mse: 0.1486\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4370 - mse: 0.4370 - val_loss: 0.1495 - val_mse: 0.1495\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4360 - mse: 0.4360 - val_loss: 0.1504 - val_mse: 0.1504\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4350 - mse: 0.4350 - val_loss: 0.1512 - val_mse: 0.1512\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4341 - mse: 0.4341 - val_loss: 0.1521 - val_mse: 0.1521\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4334 - mse: 0.4334 - val_loss: 0.1530 - val_mse: 0.1530\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4326 - mse: 0.4326 - val_loss: 0.1538 - val_mse: 0.1538\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4318 - mse: 0.4318 - val_loss: 0.1547 - val_mse: 0.1547\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4309 - mse: 0.4309 - val_loss: 0.1556 - val_mse: 0.1556\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4300 - mse: 0.4300 - val_loss: 0.1564 - val_mse: 0.1564\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4291 - mse: 0.4291 - val_loss: 0.1573 - val_mse: 0.1573\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4282 - mse: 0.4282 - val_loss: 0.1582 - val_mse: 0.1582\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4274 - mse: 0.4274 - val_loss: 0.1590 - val_mse: 0.1590\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4265 - mse: 0.4265 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4258 - mse: 0.4258 - val_loss: 0.1608 - val_mse: 0.1608\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4248 - mse: 0.4248 - val_loss: 0.1617 - val_mse: 0.1617\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4239 - mse: 0.4239 - val_loss: 0.1625 - val_mse: 0.1625\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4231 - mse: 0.4231 - val_loss: 0.1634 - val_mse: 0.1634\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4226 - mse: 0.4226 - val_loss: 0.1643 - val_mse: 0.1643\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4217 - mse: 0.4217 - val_loss: 0.1651 - val_mse: 0.1651\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4210 - mse: 0.4210 - val_loss: 0.1660 - val_mse: 0.1660\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4201 - mse: 0.4201 - val_loss: 0.1669 - val_mse: 0.1669\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4192 - mse: 0.4192 - val_loss: 0.1677 - val_mse: 0.1677\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4184 - mse: 0.4184 - val_loss: 0.1686 - val_mse: 0.1686\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4176 - mse: 0.4176 - val_loss: 0.1695 - val_mse: 0.1695\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4169 - mse: 0.4169 - val_loss: 0.1704 - val_mse: 0.1704\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4161 - mse: 0.4161 - val_loss: 0.1713 - val_mse: 0.1713\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4154 - mse: 0.4154 - val_loss: 0.1721 - val_mse: 0.1721\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4145 - mse: 0.4145 - val_loss: 0.1730 - val_mse: 0.1730\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4138 - mse: 0.4138 - val_loss: 0.1739 - val_mse: 0.1739\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4129 - mse: 0.4129 - val_loss: 0.1747 - val_mse: 0.1747\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4121 - mse: 0.4121 - val_loss: 0.1757 - val_mse: 0.1757\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4114 - mse: 0.4114 - val_loss: 0.1765 - val_mse: 0.1765\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4105 - mse: 0.4105 - val_loss: 0.1774 - val_mse: 0.1774\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4099 - mse: 0.4099 - val_loss: 0.1783 - val_mse: 0.1783\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4092 - mse: 0.4092 - val_loss: 0.1791 - val_mse: 0.1791\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4094 - mse: 0.4094 - val_loss: 0.1800 - val_mse: 0.1800\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4086 - mse: 0.4086 - val_loss: 0.1809 - val_mse: 0.1809\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4080 - mse: 0.4080 - val_loss: 0.1818 - val_mse: 0.1818\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4072 - mse: 0.4072 - val_loss: 0.1827 - val_mse: 0.1827\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4055 - mse: 0.4055 - val_loss: 0.1835 - val_mse: 0.1835\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4048 - mse: 0.4048 - val_loss: 0.1844 - val_mse: 0.1844\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4040 - mse: 0.4040 - val_loss: 0.1853 - val_mse: 0.1853\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4035 - mse: 0.4035 - val_loss: 0.1861 - val_mse: 0.1861\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4037 - mse: 0.4037 - val_loss: 0.1871 - val_mse: 0.1871\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4031 - mse: 0.4031 - val_loss: 0.1879 - val_mse: 0.1879\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4024 - mse: 0.4024 - val_loss: 0.1888 - val_mse: 0.1888\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4018 - mse: 0.4018 - val_loss: 0.1897 - val_mse: 0.1897\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "bihistory = bimodel.fit(X_train,y_train,epochs=1000, validation_split=0.2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [3]]]\n",
      "Prediction: [2.3694847 3.3205621 5.013885 ]\n",
      "Actual: [2 3 5]\n"
     ]
    }
   ],
   "source": [
    "#try on prediction\n",
    "print(X_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(bimodel.predict(X_test))))\n",
    "print(\"Actual: \" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacked LSTM graph\n",
    "smodel = Sequential()\n",
    "smodel.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(2, 1)))\n",
    "smodel.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "smodel.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "smodel.add(LSTM(25, activation='relu'))\n",
    "smodel.add(Dense(20, activation='relu'))\n",
    "smodel.add(Dense(10, activation='relu'))\n",
    "smodel.add(Dense(1))\n",
    "smodel.compile(optimizer='adam', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 46683762688.0000 - mse: 46683762688.0000 - val_loss: 108641.1875 - val_mse: 108641.1875\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46720413696.0000 - mse: 46720413696.0000 - val_loss: 108606.0234 - val_mse: 108606.0234\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 46700982272.0000 - mse: 46700982272.0000 - val_loss: 108594.5703 - val_mse: 108594.5703\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46712860672.0000 - mse: 46712860672.0000 - val_loss: 108587.4766 - val_mse: 108587.4766\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 46711730176.0000 - mse: 46711730176.0000 - val_loss: 108584.7500 - val_mse: 108584.7500\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46740537344.0000 - mse: 46740537344.0000 - val_loss: 108580.1484 - val_mse: 108580.1484\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46733492224.0000 - mse: 46733492224.0000 - val_loss: 108577.3125 - val_mse: 108577.3125\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46693412864.0000 - mse: 46693412864.0000 - val_loss: 108576.2266 - val_mse: 108576.2266\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46689595392.0000 - mse: 46689595392.0000 - val_loss: 108573.8516 - val_mse: 108573.8516\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46684942336.0000 - mse: 46684942336.0000 - val_loss: 108571.2500 - val_mse: 108571.2500\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46683033600.0000 - mse: 46683033600.0000 - val_loss: 108568.2500 - val_mse: 108568.2500\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46682001408.0000 - mse: 46682001408.0000 - val_loss: 108564.5000 - val_mse: 108564.5000\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46680571904.0000 - mse: 46680571904.0000 - val_loss: 108560.4609 - val_mse: 108560.4609\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 46678142976.0000 - mse: 46678142976.0000 - val_loss: 108556.1953 - val_mse: 108556.1953\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 46678528000.0000 - mse: 46678528000.0000 - val_loss: 108552.2500 - val_mse: 108552.2500\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46677889024.0000 - mse: 46677889024.0000 - val_loss: 108548.5859 - val_mse: 108548.5859\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46682689536.0000 - mse: 46682689536.0000 - val_loss: 108544.5938 - val_mse: 108544.5938\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46678781952.0000 - mse: 46678781952.0000 - val_loss: 108539.7812 - val_mse: 108539.7812\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46677635072.0000 - mse: 46677635072.0000 - val_loss: 108532.8359 - val_mse: 108532.8359\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46676459520.0000 - mse: 46676459520.0000 - val_loss: 108522.3203 - val_mse: 108522.3203\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46675554304.0000 - mse: 46675554304.0000 - val_loss: 108507.6172 - val_mse: 108507.6172\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46674407424.0000 - mse: 46674407424.0000 - val_loss: 108481.4141 - val_mse: 108481.4141\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46677241856.0000 - mse: 46677241856.0000 - val_loss: 108441.1641 - val_mse: 108441.1641\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46675177472.0000 - mse: 46675177472.0000 - val_loss: 108385.7734 - val_mse: 108385.7734\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46638354432.0000 - mse: 46638354432.0000 - val_loss: 108297.4297 - val_mse: 108297.4297\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46610624512.0000 - mse: 46610624512.0000 - val_loss: 108197.7891 - val_mse: 108197.7891\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46588411904.0000 - mse: 46588411904.0000 - val_loss: 108059.6562 - val_mse: 108059.6562\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46579535872.0000 - mse: 46579535872.0000 - val_loss: 107828.3203 - val_mse: 107828.3203\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46568013824.0000 - mse: 46568013824.0000 - val_loss: 107444.0625 - val_mse: 107444.0625\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 46546358272.0000 - mse: 46546358272.0000 - val_loss: 106803.6641 - val_mse: 106803.6641\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 46576480256.0000 - mse: 46576480256.0000 - val_loss: 106172.1484 - val_mse: 106172.1484\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46483337216.0000 - mse: 46483337216.0000 - val_loss: 105398.6016 - val_mse: 105398.6016\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46230777856.0000 - mse: 46230777856.0000 - val_loss: 104468.6875 - val_mse: 104468.6875\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 46076923904.0000 - mse: 46076923904.0000 - val_loss: 103502.2812 - val_mse: 103502.2812\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 46173356032.0000 - mse: 46173356032.0000 - val_loss: 102258.6797 - val_mse: 102258.6797\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 45907656704.0000 - mse: 45907656704.0000 - val_loss: 100727.9375 - val_mse: 100727.9375\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 45517332480.0000 - mse: 45517332480.0000 - val_loss: 98431.9688 - val_mse: 98431.9688\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 45408292864.0000 - mse: 45408292864.0000 - val_loss: 95931.4453 - val_mse: 95931.4453\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 44823580672.0000 - mse: 44823580672.0000 - val_loss: 93244.5391 - val_mse: 93244.5391\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 44653752320.0000 - mse: 44653752320.0000 - val_loss: 90491.5234 - val_mse: 90491.5234\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 43232587776.0000 - mse: 43232587776.0000 - val_loss: 86946.5234 - val_mse: 86946.5234\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 42673991680.0000 - mse: 42673991680.0000 - val_loss: 83037.6328 - val_mse: 83037.6328\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 40242044928.0000 - mse: 40242044928.0000 - val_loss: 78176.6953 - val_mse: 78176.6953\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 39200112640.0000 - mse: 39200112640.0000 - val_loss: 72858.4766 - val_mse: 72858.4766\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 37347684352.0000 - mse: 37347684352.0000 - val_loss: 66957.9141 - val_mse: 66957.9141\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 33612142592.0000 - mse: 33612142592.0000 - val_loss: 59893.3906 - val_mse: 59893.3906\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 31218204672.0000 - mse: 31218204672.0000 - val_loss: 52223.7656 - val_mse: 52223.7656\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 26947721216.0000 - mse: 26947721216.0000 - val_loss: 43828.2461 - val_mse: 43828.2461\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 24855873536.0000 - mse: 24855873536.0000 - val_loss: 34379.7734 - val_mse: 34379.7734\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 21319268352.0000 - mse: 21319268352.0000 - val_loss: 24699.1660 - val_mse: 24699.1660\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 17262346240.0000 - mse: 17262346240.0000 - val_loss: 15872.5430 - val_mse: 15872.5430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10112766976.0000 - mse: 10112766976.0000 - val_loss: 8035.5786 - val_mse: 8035.5786\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4433086976.0000 - mse: 4433086976.0000 - val_loss: 2578.9995 - val_mse: 2578.9995\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1313720704.0000 - mse: 1313720704.0000 - val_loss: 881.5610 - val_mse: 881.5610\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 133192680.0000 - mse: 133192680.0000 - val_loss: 3601.3757 - val_mse: 3601.3757\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 941138176.0000 - mse: 941138176.0000 - val_loss: 8365.2295 - val_mse: 8365.2295\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1895509504.0000 - mse: 1895509504.0000 - val_loss: 12317.7568 - val_mse: 12317.7568\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2030249472.0000 - mse: 2030249472.0000 - val_loss: 14252.7744 - val_mse: 14252.7744\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2388355328.0000 - mse: 2388355328.0000 - val_loss: 13737.4326 - val_mse: 13737.4326\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2207664128.0000 - mse: 2207664128.0000 - val_loss: 11446.7256 - val_mse: 11446.7256\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1579628416.0000 - mse: 1579628416.0000 - val_loss: 8356.2969 - val_mse: 8356.2969\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 929030464.0000 - mse: 929030464.0000 - val_loss: 5440.8989 - val_mse: 5440.8989\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 402552192.0000 - mse: 402552192.0000 - val_loss: 3150.1511 - val_mse: 3150.1511\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 87765104.0000 - mse: 87765104.0000 - val_loss: 1649.5288 - val_mse: 1649.5288\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 54203.3867 - mse: 54203.3867 - val_loss: 814.4651 - val_mse: 814.4651\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 70625608.0000 - mse: 70625608.0000 - val_loss: 428.8591 - val_mse: 428.8591\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 224988272.0000 - mse: 224988272.0000 - val_loss: 307.0417 - val_mse: 307.0417\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 393437408.0000 - mse: 393437408.0000 - val_loss: 297.7799 - val_mse: 297.7799\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 523577696.0000 - mse: 523577696.0000 - val_loss: 321.8536 - val_mse: 321.8536\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 613696960.0000 - mse: 613696960.0000 - val_loss: 307.7198 - val_mse: 307.7198\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 621313600.0000 - mse: 621313600.0000 - val_loss: 255.3858 - val_mse: 255.3858\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 507599904.0000 - mse: 507599904.0000 - val_loss: 225.6572 - val_mse: 225.6572\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 388151552.0000 - mse: 388151552.0000 - val_loss: 255.2702 - val_mse: 255.2702\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 249312976.0000 - mse: 249312976.0000 - val_loss: 398.4780 - val_mse: 398.4780\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 125208872.0000 - mse: 125208872.0000 - val_loss: 680.7358 - val_mse: 680.7358\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 38461380.0000 - mse: 38461380.0000 - val_loss: 1087.2058 - val_mse: 1087.2058\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1354760.6250 - mse: 1354760.6250 - val_loss: 1416.6227 - val_mse: 1416.6227\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 12727281.0000 - mse: 12727281.0000 - val_loss: 1430.7222 - val_mse: 1430.7222\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5159025.5000 - mse: 5159025.5000 - val_loss: 1768.5770 - val_mse: 1768.5770\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 32782720.0000 - mse: 32782720.0000 - val_loss: 2005.0677 - val_mse: 2005.0677\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 59774144.0000 - mse: 59774144.0000 - val_loss: 2033.6188 - val_mse: 2033.6188\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 41245616.0000 - mse: 41245616.0000 - val_loss: 1836.9922 - val_mse: 1836.9922\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 40313724.0000 - mse: 40313724.0000 - val_loss: 1724.2338 - val_mse: 1724.2338\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 43791396.0000 - mse: 43791396.0000 - val_loss: 1609.8744 - val_mse: 1609.8744\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 37649616.0000 - mse: 37649616.0000 - val_loss: 1452.8925 - val_mse: 1452.8925\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 32001610.0000 - mse: 32001610.0000 - val_loss: 1259.5177 - val_mse: 1259.5177\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 17899158.0000 - mse: 17899158.0000 - val_loss: 1054.2571 - val_mse: 1054.2571\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6423332.0000 - mse: 6423332.0000 - val_loss: 859.2141 - val_mse: 859.2141\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1848044.1250 - mse: 1848044.1250 - val_loss: 682.7784 - val_mse: 682.7784\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 38333.2070 - mse: 38333.2070 - val_loss: 502.5642 - val_mse: 502.5642\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7628837.5000 - mse: 7628837.5000 - val_loss: 375.9353 - val_mse: 375.9353\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12241816.0000 - mse: 12241816.0000 - val_loss: 288.5622 - val_mse: 288.5622\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 15212932.0000 - mse: 15212932.0000 - val_loss: 167.2039 - val_mse: 167.2039\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 33716400.0000 - mse: 33716400.0000 - val_loss: 86.0579 - val_mse: 86.0579\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 29542456.0000 - mse: 29542456.0000 - val_loss: 143.6466 - val_mse: 143.6466\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 21990002.0000 - mse: 21990002.0000 - val_loss: 176.8175 - val_mse: 176.8175\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 15164403.0000 - mse: 15164403.0000 - val_loss: 211.6259 - val_mse: 211.6259\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7102518.0000 - mse: 7102518.0000 - val_loss: 251.3194 - val_mse: 251.3194\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1799407.1250 - mse: 1799407.1250 - val_loss: 289.1768 - val_mse: 289.1768\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1248.5853 - mse: 1248.5853 - val_loss: 313.9093 - val_mse: 313.9093\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1346847.6250 - mse: 1346847.6250 - val_loss: 312.8945 - val_mse: 312.8945\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1845230.3750 - mse: 1845230.3750 - val_loss: 289.2365 - val_mse: 289.2365\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3049954.2500 - mse: 3049954.2500 - val_loss: 243.6830 - val_mse: 243.6830\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5351230.5000 - mse: 5351230.5000 - val_loss: 136.5434 - val_mse: 136.5434\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3417196.7500 - mse: 3417196.7500 - val_loss: 106.2710 - val_mse: 106.2710\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 78.8670 - mse: 78.8670 - val_loss: 38.1915 - val_mse: 38.1915\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 57686028.0000 - mse: 57686028.0000 - val_loss: 71.0581 - val_mse: 71.0581\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 848324.1875 - mse: 848324.1875 - val_loss: 78.9544 - val_mse: 78.9544\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1024232.0000 - mse: 1024232.0000 - val_loss: 75.5460 - val_mse: 75.5460\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 294019.8438 - mse: 294019.8438 - val_loss: 67.1998 - val_mse: 67.1998\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 304985.1562 - mse: 304985.1562 - val_loss: 55.5858 - val_mse: 55.5858\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 266804.6562 - mse: 266804.6562 - val_loss: 35.7174 - val_mse: 35.7174\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 53908372.0000 - mse: 53908372.0000 - val_loss: 25.1740 - val_mse: 25.1740\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 18145330.0000 - mse: 18145330.0000 - val_loss: 17.5497 - val_mse: 17.5497\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10103453.0000 - mse: 10103453.0000 - val_loss: 14.7786 - val_mse: 14.7786\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1715213.0000 - mse: 1715213.0000 - val_loss: 16.3828 - val_mse: 16.3828\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 683.4601 - mse: 683.4601 - val_loss: 20.3807 - val_mse: 20.3807\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1334290.6250 - mse: 1334290.6250 - val_loss: 18.2917 - val_mse: 18.2917\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3835368.2500 - mse: 3835368.2500 - val_loss: 4.3302 - val_mse: 4.3302\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 814446.2500 - mse: 814446.2500 - val_loss: 4.3057 - val_mse: 4.3057\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2182369.0000 - mse: 2182369.0000 - val_loss: 3.3852 - val_mse: 3.3852\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1619545.5000 - mse: 1619545.5000 - val_loss: 2.9898 - val_mse: 2.9898\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 251185.0000 - mse: 251185.0000 - val_loss: 3.6187 - val_mse: 3.6187\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 19256250.0000 - mse: 19256250.0000 - val_loss: 2.5924 - val_mse: 2.5924\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5535500.0000 - mse: 5535500.0000 - val_loss: 4.1929 - val_mse: 4.1929\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2665625.7500 - mse: 2665625.7500 - val_loss: 5.6673 - val_mse: 5.6673\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4524524.0000 - mse: 4524524.0000 - val_loss: 5.0922 - val_mse: 5.0922\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5465751.0000 - mse: 5465751.0000 - val_loss: 3.0018 - val_mse: 3.0018\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 422.1410 - mse: 422.1410 - val_loss: 1.6799 - val_mse: 1.6799\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12702872.0000 - mse: 12702872.0000 - val_loss: 1.2358 - val_mse: 1.2358\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 63076.3398 - mse: 63076.3398 - val_loss: 1.1451 - val_mse: 1.1451\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 268727.7812 - mse: 268727.7812 - val_loss: 1.1280 - val_mse: 1.1280\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 495693.5938 - mse: 495693.5938 - val_loss: 1.4154 - val_mse: 1.4154\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 16939038.0000 - mse: 16939038.0000 - val_loss: 1.8786 - val_mse: 1.8786\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1064112.8750 - mse: 1064112.8750 - val_loss: 4.3388 - val_mse: 4.3388\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1924569.3750 - mse: 1924569.3750 - val_loss: 7.1121 - val_mse: 7.1121\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2425564.2500 - mse: 2425564.2500 - val_loss: 8.7757 - val_mse: 8.7757\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2373729.2500 - mse: 2373729.2500 - val_loss: 8.6639 - val_mse: 8.6639\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1842705.6250 - mse: 1842705.6250 - val_loss: 7.1283 - val_mse: 7.1283\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1092172.8750 - mse: 1092172.8750 - val_loss: 5.1042 - val_mse: 5.1042\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 424195.5938 - mse: 424195.5938 - val_loss: 3.5297 - val_mse: 3.5297\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 51794.3242 - mse: 51794.324 - 0s 16ms/step - loss: 51794.3242 - mse: 51794.3242 - val_loss: 2.8932 - val_mse: 2.8932\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 31759.1621 - mse: 31759.1621 - val_loss: 4.1124 - val_mse: 4.1124\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 273909.5938 - mse: 273909.5938 - val_loss: 4.5268 - val_mse: 4.5268\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 605621.8125 - mse: 605621.8125 - val_loss: 3.8975 - val_mse: 3.8975\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 855887.5625 - mse: 855887.5625 - val_loss: 3.1009 - val_mse: 3.1009\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 920396.8125 - mse: 920396.8125 - val_loss: 2.3726 - val_mse: 2.3726\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 788798.5000 - mse: 788798.5000 - val_loss: 1.8153 - val_mse: 1.8153\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 531294.5625 - mse: 531294.5625 - val_loss: 1.4966 - val_mse: 1.4966\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 256630.0781 - mse: 256630.0781 - val_loss: 1.4013 - val_mse: 1.4013\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 62883.4648 - mse: 62883.4648 - val_loss: 1.0545 - val_mse: 1.0545\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 99.8582 - mse: 99.8582 - val_loss: 1.0324 - val_mse: 1.0324\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 58506.3633 - mse: 58506.3633 - val_loss: 1.9799 - val_mse: 1.9799\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 181536.5625 - mse: 181536.5625 - val_loss: 3.4689 - val_mse: 3.4689\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 297632.6562 - mse: 297632.6562 - val_loss: 4.4942 - val_mse: 4.4942\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 351054.2812 - mse: 351054.2812 - val_loss: 4.7904 - val_mse: 4.7904\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 323086.7500 - mse: 323086.7500 - val_loss: 4.5308 - val_mse: 4.5308\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 233130.0156 - mse: 233130.0156 - val_loss: 3.9965 - val_mse: 3.9965\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 123457.0000 - mse: 123457.0000 - val_loss: 3.3828 - val_mse: 3.3828\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 37388.1445 - mse: 37388.1445 - val_loss: 2.7790 - val_mse: 2.7790\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 927.5458 - mse: 927.5458 - val_loss: 2.3665 - val_mse: 2.3665\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 15092.3037 - mse: 15092.3037 - val_loss: 2.2352 - val_mse: 2.2352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 60210.2617 - mse: 60210.2617 - val_loss: 1.9322 - val_mse: 1.9322\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 107763.6328 - mse: 107763.6328 - val_loss: 3.6892 - val_mse: 3.6892\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 133920.0781 - mse: 133920.0781 - val_loss: 5.0406 - val_mse: 5.0406\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 128727.3047 - mse: 128727.3047 - val_loss: 3.9634 - val_mse: 3.9634\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 96760.2891 - mse: 96760.2891 - val_loss: 3.1317 - val_mse: 3.1317\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 54177.7812 - mse: 54177.7812 - val_loss: 2.5199 - val_mse: 2.5199\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 18328.9277 - mse: 18328.9277 - val_loss: 2.1817 - val_mse: 2.1817\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1070.9639 - mse: 1070.9639 - val_loss: 2.1448 - val_mse: 2.1448\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4422.0508 - mse: 4422.0508 - val_loss: 2.3212 - val_mse: 2.3212\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 21436.3262 - mse: 21436.3262 - val_loss: 2.5454 - val_mse: 2.5454\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 40696.8125 - mse: 40696.8125 - val_loss: 2.6893 - val_mse: 2.6893\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 52035.8711 - mse: 52035.8711 - val_loss: 2.7199 - val_mse: 2.7199\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 50661.7500 - mse: 50661.7500 - val_loss: 2.7862 - val_mse: 2.7862\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 19666.7070 - mse: 19666.7070 - val_loss: 2.5992 - val_mse: 2.5992\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 569247.9375 - mse: 569247.9375 - val_loss: 2.4345 - val_mse: 2.4345\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 304010.1562 - mse: 304010.1562 - val_loss: 2.5041 - val_mse: 2.5041\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 71339.7891 - mse: 71339.7891 - val_loss: 3.0208 - val_mse: 3.0208\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1704.3716 - mse: 1704.3716 - val_loss: 3.6881 - val_mse: 3.6881\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 75791.6484 - mse: 75791.6484 - val_loss: 4.1128 - val_mse: 4.1128\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 183473.2500 - mse: 183473.2500 - val_loss: 4.1104 - val_mse: 4.1104\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 231484.2969 - mse: 231484.2969 - val_loss: 3.9378 - val_mse: 3.9378\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 201827.2031 - mse: 201827.2031 - val_loss: 3.7917 - val_mse: 3.7917\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 133782.8281 - mse: 133782.8281 - val_loss: 3.7352 - val_mse: 3.7352\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 60452.6758 - mse: 60452.6758 - val_loss: 3.7438 - val_mse: 3.7438\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 11650.8047 - mse: 11650.8047 - val_loss: 3.7472 - val_mse: 3.7472\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 975.7632 - mse: 975.7632 - val_loss: 3.6839 - val_mse: 3.6839\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 22720.2598 - mse: 22720.2598 - val_loss: 3.5539 - val_mse: 3.5539\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 58003.2617 - mse: 58003.2617 - val_loss: 3.4269 - val_mse: 3.4269\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 85665.0156 - mse: 85665.0156 - val_loss: 3.3664 - val_mse: 3.3664\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 92147.3125 - mse: 92147.3125 - val_loss: 3.3323 - val_mse: 3.3323\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 76175.6875 - mse: 76175.6875 - val_loss: 3.2745 - val_mse: 3.2745\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 47122.5508 - mse: 47122.5508 - val_loss: 3.2478 - val_mse: 3.2478\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 18768.9199 - mse: 18768.9199 - val_loss: 3.3062 - val_mse: 3.3062\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2302.9578 - mse: 2302.9578 - val_loss: 3.4332 - val_mse: 3.4332\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1609.8428 - mse: 1609.8428 - val_loss: 3.5855 - val_mse: 3.5855\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12763.3467 - mse: 12763.3467 - val_loss: 3.7401 - val_mse: 3.7401\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 27191.5723 - mse: 27191.5723 - val_loss: 3.9143 - val_mse: 3.9143\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 36475.3086 - mse: 36475.3086 - val_loss: 4.1862 - val_mse: 4.1862\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 36220.7305 - mse: 36220.7305 - val_loss: 4.7488 - val_mse: 4.7488\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 27406.9922 - mse: 27406.9922 - val_loss: 6.0183 - val_mse: 6.0183\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14912.7197 - mse: 14912.7197 - val_loss: 8.3353 - val_mse: 8.3353\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4547.2539 - mse: 4547.2539 - val_loss: 11.7876 - val_mse: 11.7876\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 21453.8359 - mse: 21453.8359 - val_loss: 9.8145 - val_mse: 9.8145\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1964.9923 - mse: 1964.9923 - val_loss: 8.6492 - val_mse: 8.6492\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7468.0786 - mse: 7468.0786 - val_loss: 8.5483 - val_mse: 8.5483\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12858.5117 - mse: 12858.5117 - val_loss: 10.7709 - val_mse: 10.7709\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2560351.2500 - mse: 2560351.2500 - val_loss: 8.4501 - val_mse: 8.4501\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1971898.0000 - mse: 1971898.0000 - val_loss: 6.5964 - val_mse: 6.5964\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 218959.3750 - mse: 218959.3750 - val_loss: 5.3584 - val_mse: 5.3584\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 14316.6914 - mse: 14316.6914 - val_loss: 3.6502 - val_mse: 3.6502\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 43901.5156 - mse: 43901.5156 - val_loss: 2.7913 - val_mse: 2.7913\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 232085.7656 - mse: 232085.7656 - val_loss: 2.4920 - val_mse: 2.4920\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 448950.5000 - mse: 448950.5000 - val_loss: 2.4740 - val_mse: 2.4740\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 577148.6875 - mse: 577148.6875 - val_loss: 2.5925 - val_mse: 2.5925\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 561320.4375 - mse: 561320.4375 - val_loss: 2.8137 - val_mse: 2.8137\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 421737.6250 - mse: 421737.6250 - val_loss: 3.1400 - val_mse: 3.1400\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 231330.6875 - mse: 231330.6875 - val_loss: 3.5391 - val_mse: 3.5391\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 73410.0703 - mse: 73410.0703 - val_loss: 3.9154 - val_mse: 3.9154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2402.3364 - mse: 2402.3364 - val_loss: 4.1393 - val_mse: 4.1393\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 24812.2402 - mse: 24812.2402 - val_loss: 4.1299 - val_mse: 4.1299\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 105146.3125 - mse: 105146.3125 - val_loss: 3.9234 - val_mse: 3.9234\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 189495.6406 - mse: 189495.6406 - val_loss: 3.6568 - val_mse: 3.6568\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 232851.0000 - mse: 232851.0000 - val_loss: 3.4321 - val_mse: 3.4321\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 217463.4531 - mse: 217463.4531 - val_loss: 3.2532 - val_mse: 3.2532\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 155756.5625 - mse: 155756.5625 - val_loss: 3.1190 - val_mse: 3.1190\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 79314.6016 - mse: 79314.6016 - val_loss: 3.0725 - val_mse: 3.0725\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 21019.7637 - mse: 21019.7637 - val_loss: 3.1356 - val_mse: 3.1356\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 35.6981 - mse: 35.6981 - val_loss: 3.2735 - val_mse: 3.2735\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 15479.2432 - mse: 15479.2432 - val_loss: 3.4187 - val_mse: 3.4187\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 50384.3008 - mse: 50384.3008 - val_loss: 3.5203 - val_mse: 3.5203\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 82222.4453 - mse: 82222.4453 - val_loss: 3.5555 - val_mse: 3.5555\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 94321.6797 - mse: 94321.6797 - val_loss: 3.5278 - val_mse: 3.5278\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 82351.1484 - mse: 82351.1484 - val_loss: 3.4742 - val_mse: 3.4742\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 54198.0000 - mse: 54198.0000 - val_loss: 3.4811 - val_mse: 3.4811\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 23912.4355 - mse: 23912.4355 - val_loss: 3.5521 - val_mse: 3.5521\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4188.2603 - mse: 4188.2603 - val_loss: 3.6857 - val_mse: 3.6857\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 631.0046 - mse: 631.0046 - val_loss: 3.8565 - val_mse: 3.8565\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10459.1885 - mse: 10459.1885 - val_loss: 4.0645 - val_mse: 4.0645\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 25324.1953 - mse: 25324.1953 - val_loss: 4.2886 - val_mse: 4.2886\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 36171.4883 - mse: 36171.4883 - val_loss: 4.5004 - val_mse: 4.5004\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 37654.4023 - mse: 37654.4023 - val_loss: 4.6530 - val_mse: 4.6530\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 29848.5137 - mse: 29848.5137 - val_loss: 4.7020 - val_mse: 4.7020\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 17284.2168 - mse: 17284.2168 - val_loss: 4.6260 - val_mse: 4.6260\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5992.8462 - mse: 5992.8462 - val_loss: 4.4411 - val_mse: 4.4411\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 356.7367 - mse: 356.7367 - val_loss: 4.1961 - val_mse: 4.1961\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1368.3198 - mse: 1368.3198 - val_loss: 3.9544 - val_mse: 3.9544\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6746.6138 - mse: 6746.6138 - val_loss: 3.7813 - val_mse: 3.7813\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12604.7314 - mse: 12604.7314 - val_loss: 3.7645 - val_mse: 3.7645\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15071.5576 - mse: 15071.5576 - val_loss: 4.2639 - val_mse: 4.2639\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 254474.1250 - mse: 254474.1250 - val_loss: 4.0521 - val_mse: 4.0521\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 160131.0469 - mse: 160131.0469 - val_loss: 3.5500 - val_mse: 3.5500\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 55449.4844 - mse: 55449.4844 - val_loss: 3.0656 - val_mse: 3.0656\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1974.5542 - mse: 1974.5542 - val_loss: 2.8043 - val_mse: 2.8043\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 18706.7852 - mse: 18706.7852 - val_loss: 2.8503 - val_mse: 2.8503\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 74301.0234 - mse: 74301.0234 - val_loss: 3.1817 - val_mse: 3.1817\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 116027.4297 - mse: 116027.4297 - val_loss: 3.6659 - val_mse: 3.6659\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 113703.9609 - mse: 113703.9609 - val_loss: 4.0946 - val_mse: 4.0946\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 74427.4453 - mse: 74427.4453 - val_loss: 4.2770 - val_mse: 4.2770\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 27668.8828 - mse: 27668.8828 - val_loss: 4.3405 - val_mse: 4.3405\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1679.0317 - mse: 1679.0317 - val_loss: 4.5632 - val_mse: 4.5632\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6359.6333 - mse: 6359.6333 - val_loss: 4.5856 - val_mse: 4.5856\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 30244.4160 - mse: 30244.4160 - val_loss: 4.4085 - val_mse: 4.4085\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 51655.2852 - mse: 51655.2852 - val_loss: 4.2477 - val_mse: 4.2477\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 55004.0156 - mse: 55004.0156 - val_loss: 4.2760 - val_mse: 4.2760\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 39755.8320 - mse: 39755.8320 - val_loss: 4.5029 - val_mse: 4.5029\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 17547.4941 - mse: 17547.4941 - val_loss: 4.8008 - val_mse: 4.8008\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2379.8042 - mse: 2379.8042 - val_loss: 5.1188 - val_mse: 5.1188\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1230.9987 - mse: 1230.9987 - val_loss: 5.4867 - val_mse: 5.4867\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2773.4932 - mse: 2773.4932 - val_loss: 5.7426 - val_mse: 5.7426\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1105.9852 - mse: 1105.9852 - val_loss: 6.0486 - val_mse: 6.0486\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2239.5627 - mse: 2239.5627 - val_loss: 5.2640 - val_mse: 5.2640\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 18.3519 - mse: 18.3519 - val_loss: 4.5533 - val_mse: 4.5533\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1855.8612 - mse: 1855.8612 - val_loss: 4.5390 - val_mse: 4.5390\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 311.3689 - mse: 311.3689 - val_loss: 5.0449 - val_mse: 5.0449\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1595.2085 - mse: 1595.2085 - val_loss: 5.2955 - val_mse: 5.2955\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 181.8619 - mse: 181.8619 - val_loss: 5.4568 - val_mse: 5.4568\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1153.1866 - mse: 1153.1866 - val_loss: 5.8477 - val_mse: 5.8477\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 473.0012 - mse: 473.0012 - val_loss: 6.1517 - val_mse: 6.1517\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 677.8953 - mse: 677.8953 - val_loss: 5.7104 - val_mse: 5.7104\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 522.0292 - mse: 522.0292 - val_loss: 4.8356 - val_mse: 4.8356\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 389.0833 - mse: 389.0833 - val_loss: 4.4658 - val_mse: 4.4658\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 640.4409 - mse: 640.4409 - val_loss: 4.8715 - val_mse: 4.8715\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 120.1568 - mse: 120.1568 - val_loss: 5.4308 - val_mse: 5.4308\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 659.1663 - mse: 659.1663 - val_loss: 5.6303 - val_mse: 5.6303\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 40.2344 - mse: 40.2344 - val_loss: 5.6993 - val_mse: 5.6993\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 586.5977 - mse: 586.5977 - val_loss: 5.8380 - val_mse: 5.8380\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15.0365 - mse: 15.0365 - val_loss: 5.8413 - val_mse: 5.8413\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 490.9019 - mse: 490.9019 - val_loss: 5.5501 - val_mse: 5.5501\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 39.9351 - mse: 39.9351 - val_loss: 5.3452 - val_mse: 5.3452\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 354.5101 - mse: 354.5101 - val_loss: 5.5569 - val_mse: 5.5569\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 100.5571 - mse: 100.5571 - val_loss: 6.0220 - val_mse: 6.0220\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 227.3607 - mse: 227.3607 - val_loss: 6.3122 - val_mse: 6.3122\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 139.5204 - mse: 139.5204 - val_loss: 6.3651 - val_mse: 6.3651\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 131.0466 - mse: 131.0466 - val_loss: 6.4352 - val_mse: 6.4352\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 171.9644 - mse: 171.9644 - val_loss: 6.5552 - val_mse: 6.5552\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 57.9708 - mse: 57.9708 - val_loss: 6.5328 - val_mse: 6.5328\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 178.2169 - mse: 178.2169 - val_loss: 6.3994 - val_mse: 6.3994\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 24.6003 - mse: 24.6003 - val_loss: 6.4613 - val_mse: 6.4613\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 164.5029 - mse: 164.5029 - val_loss: 6.8194 - val_mse: 6.8194\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.9227 - mse: 10.9227 - val_loss: 7.2150 - val_mse: 7.2150\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 140.5577 - mse: 140.5577 - val_loss: 7.3880 - val_mse: 7.3880\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.4699 - mse: 13.4699 - val_loss: 7.4348 - val_mse: 7.4348\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 109.2451 - mse: 109.2451 - val_loss: 7.5390 - val_mse: 7.5390\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 23.8581 - mse: 23.8581 - val_loss: 7.6617 - val_mse: 7.6617\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 78.5965 - mse: 78.5965 - val_loss: 7.7217 - val_mse: 7.7217\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 89.4655 - mse: 89.4655 - val_loss: 7.6870 - val_mse: 7.6870\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 195.2708 - mse: 195.2708 - val_loss: 7.9837 - val_mse: 7.9837\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 64.0886 - mse: 64.0886 - val_loss: 8.4019 - val_mse: 8.4019\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 124.9096 - mse: 124.9096 - val_loss: 8.5796 - val_mse: 8.5796\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 85.1144 - mse: 85.1144 - val_loss: 8.5411 - val_mse: 8.5411\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 72.4173 - mse: 72.4173 - val_loss: 8.5981 - val_mse: 8.5981\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 100.3097 - mse: 100.3097 - val_loss: 8.8227 - val_mse: 8.8227\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 33.9121 - mse: 33.9121 - val_loss: 9.0171 - val_mse: 9.0171\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 103.2890 - mse: 103.2890 - val_loss: 9.1214 - val_mse: 9.1214\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 15.8637 - mse: 15.8637 - val_loss: 9.3271 - val_mse: 9.3271\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 94.7873 - mse: 94.7873 - val_loss: 9.6948 - val_mse: 9.6948\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.1909 - mse: 9.1909 - val_loss: 10.0126 - val_mse: 10.0126\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 79.9827 - mse: 79.9827 - val_loss: 10.1301 - val_mse: 10.1301\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.0225 - mse: 11.0225 - val_loss: 10.2242 - val_mse: 10.2242\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 61.0336 - mse: 61.0336 - val_loss: 10.5070 - val_mse: 10.5070\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 17.1042 - mse: 17.1042 - val_loss: 10.9329 - val_mse: 10.9329\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 43.0628 - mse: 43.0628 - val_loss: 11.3376 - val_mse: 11.3376\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 22.3913 - mse: 22.3913 - val_loss: 11.7409 - val_mse: 11.7409\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 27.6378 - mse: 27.6378 - val_loss: 12.2746 - val_mse: 12.2746\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 25.4099 - mse: 25.4099 - val_loss: 12.8944 - val_mse: 12.8944\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.0294 - mse: 14.0294 - val_loss: 13.4278 - val_mse: 13.4278\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 22.1909 - mse: 22.1909 - val_loss: 13.8123 - val_mse: 13.8123\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1078029.6250 - mse: 1078029.6250 - val_loss: 8.6842 - val_mse: 8.6842\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 117996.6875 - mse: 117996.6875 - val_loss: 6.9544 - val_mse: 6.9544\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 282304.8750 - mse: 282304.8750 - val_loss: 7.6694 - val_mse: 7.6694\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 300649.2812 - mse: 300649.2812 - val_loss: 9.7461 - val_mse: 9.7461\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 176126.1719 - mse: 176126.1719 - val_loss: 11.8336 - val_mse: 11.8336\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 37082.3242 - mse: 37082.3242 - val_loss: 12.6011 - val_mse: 12.6011\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4771.8491 - mse: 4771.8491 - val_loss: 11.2484 - val_mse: 11.2484\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 83659.4297 - mse: 83659.4297 - val_loss: 8.5131 - val_mse: 8.5131\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 160996.4219 - mse: 160996.4219 - val_loss: 6.9598 - val_mse: 6.9598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 145451.9219 - mse: 145451.9219 - val_loss: 7.4175 - val_mse: 7.4175\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 65251.5625 - mse: 65251.5625 - val_loss: 8.9649 - val_mse: 8.9649\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5781.2324 - mse: 5781.2324 - val_loss: 10.6747 - val_mse: 10.6747\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10130.9971 - mse: 10130.9971 - val_loss: 11.8158 - val_mse: 11.8158\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 52982.6836 - mse: 52982.6836 - val_loss: 12.1655 - val_mse: 12.1655\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 83122.6484 - mse: 83122.6484 - val_loss: 11.8651 - val_mse: 11.8651\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 73534.2344 - mse: 73534.234 - 0s 15ms/step - loss: 73534.2344 - mse: 73534.2344 - val_loss: 11.1974 - val_mse: 11.1974\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 36698.0508 - mse: 36698.0508 - val_loss: 10.5362 - val_mse: 10.5362\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5513.8613 - mse: 5513.8613 - val_loss: 10.2992 - val_mse: 10.2992\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2493.8445 - mse: 2493.8445 - val_loss: 10.7547 - val_mse: 10.7547\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 22487.4141 - mse: 22487.4141 - val_loss: 11.8255 - val_mse: 11.8255\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 41284.6836 - mse: 41284.6836 - val_loss: 13.0485 - val_mse: 13.0485\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 40343.4062 - mse: 40343.4062 - val_loss: 13.8270 - val_mse: 13.8270\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 22420.2969 - mse: 22420.2969 - val_loss: 13.8549 - val_mse: 13.8549\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4659.1387 - mse: 4659.1387 - val_loss: 13.2802 - val_mse: 13.2802\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 422.8035 - mse: 422.8035 - val_loss: 12.5251 - val_mse: 12.5251\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 8952.9297 - mse: 8952.9297 - val_loss: 12.0158 - val_mse: 12.0158\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 19333.8340 - mse: 19333.8340 - val_loss: 12.0038 - val_mse: 12.0038\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 21639.5449 - mse: 21639.5449 - val_loss: 12.5119 - val_mse: 12.5119\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 14617.2139 - mse: 14617.2139 - val_loss: 13.3471 - val_mse: 13.3471\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4863.7100 - mse: 4863.7100 - val_loss: 14.1683 - val_mse: 14.1683\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 63.2230 - mse: 63.2230 - val_loss: 14.6384 - val_mse: 14.6384\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2533.7480 - mse: 2533.7480 - val_loss: 14.6093 - val_mse: 14.6093\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 8253.7139 - mse: 8253.7139 - val_loss: 14.2179 - val_mse: 14.2179\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 11288.0068 - mse: 11288.0068 - val_loss: 13.8011 - val_mse: 13.8011\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9090.2969 - mse: 9090.2969 - val_loss: 13.6775 - val_mse: 13.6775\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4032.4695 - mse: 4032.4695 - val_loss: 13.9591 - val_mse: 13.9591\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 413.5109 - mse: 413.5109 - val_loss: 14.5242 - val_mse: 14.5242\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 540.7946 - mse: 540.7946 - val_loss: 15.1191 - val_mse: 15.1191\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3232.0781 - mse: 3232.0781 - val_loss: 15.5100 - val_mse: 15.5100\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5506.4751 - mse: 5506.4751 - val_loss: 15.6032 - val_mse: 15.6032\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5321.0034 - mse: 5321.0034 - val_loss: 15.4745 - val_mse: 15.4745\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3058.2422 - mse: 3058.2422 - val_loss: 15.3261 - val_mse: 15.3261\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 719.5368 - mse: 719.5368 - val_loss: 15.3807 - val_mse: 15.3807\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 30.0515 - mse: 30.0515 - val_loss: 15.7503 - val_mse: 15.7503\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1068.2111 - mse: 1068.2111 - val_loss: 16.3726 - val_mse: 16.3726\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2502.2966 - mse: 2502.2966 - val_loss: 17.0345 - val_mse: 17.0345\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2926.7703 - mse: 2926.7703 - val_loss: 17.5086 - val_mse: 17.5086\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2049.1516 - mse: 2049.1516 - val_loss: 17.6910 - val_mse: 17.6910\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 723.2080 - mse: 723.2080 - val_loss: 17.6599 - val_mse: 17.6599\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 19.9006 - mse: 19.9006 - val_loss: 17.6061 - val_mse: 17.6061\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 302.6215 - mse: 302.6215 - val_loss: 17.7118 - val_mse: 17.7118\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1061.5463 - mse: 1061.5463 - val_loss: 18.0494 - val_mse: 18.0494\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1510.4390 - mse: 1510.4390 - val_loss: 18.5553 - val_mse: 18.5553\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1266.4147 - mse: 1266.4147 - val_loss: 19.0558 - val_mse: 19.0558\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 597.5555 - mse: 597.5555 - val_loss: 19.3894 - val_mse: 19.3894\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 78.0923 - mse: 78.0923 - val_loss: 19.4781 - val_mse: 19.4781\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 57.3868 - mse: 57.3868 - val_loss: 19.3706 - val_mse: 19.3706\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 414.2495 - mse: 414.2495 - val_loss: 19.1977 - val_mse: 19.1977\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 738.5349 - mse: 738.5349 - val_loss: 19.0758 - val_mse: 19.0758\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 730.1171 - mse: 730.1171 - val_loss: 19.0364 - val_mse: 19.0364\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 423.5217 - mse: 423.5217 - val_loss: 18.9983 - val_mse: 18.9983\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 100.6589 - mse: 100.6589 - val_loss: 18.8072 - val_mse: 18.8072\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.4712 - mse: 5.4712 - val_loss: 18.3044 - val_mse: 18.3044\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 150.0436 - mse: 150.0436 - val_loss: 17.3395 - val_mse: 17.3395\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 346.2971 - mse: 346.2971 - val_loss: 15.6509 - val_mse: 15.6509\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 399.8642 - mse: 399.8642 - val_loss: 12.3995 - val_mse: 12.3995\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 275.5702 - mse: 275.5702 - val_loss: 6.6746 - val_mse: 6.6746\n",
      "Epoch 397/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: 93.7625 - mse: 93.7625 - val_loss: 13.8359 - val_mse: 13.8359\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.8326 - mse: 2.8326 - val_loss: 19.7507 - val_mse: 19.7507\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 49.1765 - mse: 49.1765 - val_loss: 22.4404 - val_mse: 22.4404\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 155.5222 - mse: 155.5222 - val_loss: 21.6256 - val_mse: 21.6256\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 210.9382 - mse: 210.9382 - val_loss: 18.3627 - val_mse: 18.3627\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 168.3503 - mse: 168.3503 - val_loss: 14.5340 - val_mse: 14.5340\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 72.8237 - mse: 72.8237 - val_loss: 11.6428 - val_mse: 11.6428\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7.6192 - mse: 7.6192 - val_loss: 9.9077 - val_mse: 9.9077\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 14.8369 - mse: 14.8369 - val_loss: 8.6499 - val_mse: 8.6499\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 68.7399 - mse: 68.7399 - val_loss: 6.7388 - val_mse: 6.7388\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 108.1906 - mse: 108.1906 - val_loss: 3.1498 - val_mse: 3.1498\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 97.6459 - mse: 97.6459 - val_loss: 0.3784 - val_mse: 0.3784\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 51.4114 - mse: 51.4114 - val_loss: 9.5457 - val_mse: 9.5457\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.4590 - mse: 9.4590 - val_loss: 23.5899 - val_mse: 23.5899\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.0956 - mse: 5.0956 - val_loss: 33.1471 - val_mse: 33.1471\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 30.6343 - mse: 30.6343 - val_loss: 37.6114 - val_mse: 37.6114\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 56.1822 - mse: 56.1822 - val_loss: 38.0685 - val_mse: 38.0685\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 56.1009 - mse: 56.1009 - val_loss: 30.5289 - val_mse: 30.5289\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 33.6606 - mse: 33.6606 - val_loss: 37.9021 - val_mse: 37.9021\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.1804 - mse: 9.1804 - val_loss: 41.5094 - val_mse: 41.5094\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.4998 - mse: 2.4998 - val_loss: 42.9371 - val_mse: 42.9371\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 13.7510 - mse: 13.7510 - val_loss: 43.0568 - val_mse: 43.0568\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 28.0075 - mse: 28.0075 - val_loss: 41.6823 - val_mse: 41.6823\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 30.6728 - mse: 30.6728 - val_loss: 36.6481 - val_mse: 36.6481\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 20.8299 - mse: 20.8299 - val_loss: 38.7748 - val_mse: 38.7748\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7.4351 - mse: 7.4351 - val_loss: 42.9363 - val_mse: 42.9363\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.8617 - mse: 1.8617 - val_loss: 43.1765 - val_mse: 43.1765\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.3461 - mse: 6.3461 - val_loss: 36.0384 - val_mse: 36.0384\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 13.9847 - mse: 13.9847 - val_loss: 19.1158 - val_mse: 19.1158\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 16.9291 - mse: 16.9291 - val_loss: 1.0094 - val_mse: 1.0094\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 22796262.0000 - mse: 22796262.0000 - val_loss: 70.8553 - val_mse: 70.8553\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 897456.6875 - mse: 897456.6875 - val_loss: 97.6571 - val_mse: 97.6571\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2582133.7500 - mse: 2582133.7500 - val_loss: 99.8090 - val_mse: 99.8090\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3435758.2500 - mse: 3435758.2500 - val_loss: 82.8671 - val_mse: 82.8671\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2811281.0000 - mse: 2811281.0000 - val_loss: 57.2847 - val_mse: 57.2847\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1361808.8750 - mse: 1361808.8750 - val_loss: 34.0931 - val_mse: 34.0931\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 222961.4531 - mse: 222961.4531 - val_loss: 20.1314 - val_mse: 20.1314\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 59212.2148 - mse: 59212.2148 - val_loss: 15.1739 - val_mse: 15.1739\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 693877.6875 - mse: 693877.6875 - val_loss: 13.6254 - val_mse: 13.6254\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1424603.3750 - mse: 1424603.3750 - val_loss: 11.4732 - val_mse: 11.4732\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1635859.0000 - mse: 1635859.0000 - val_loss: 9.5735 - val_mse: 9.5735\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1215207.1250 - mse: 1215207.1250 - val_loss: 10.5623 - val_mse: 10.5623\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 527315.7500 - mse: 527315.7500 - val_loss: 15.5857 - val_mse: 15.5857\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 60023.7812 - mse: 60023.7812 - val_loss: 23.3318 - val_mse: 23.3318\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 55725.2539 - mse: 55725.2539 - val_loss: 30.9714 - val_mse: 30.9714\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 387683.0938 - mse: 387683.0938 - val_loss: 35.7073 - val_mse: 35.7073\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 719795.0000 - mse: 719795.0000 - val_loss: 36.0355 - val_mse: 36.0355\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 784060.5000 - mse: 784060.5000 - val_loss: 32.2114 - val_mse: 32.2114\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 556359.5625 - mse: 556359.5625 - val_loss: 25.8563 - val_mse: 25.8563\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 225430.5156 - mse: 225430.5156 - val_loss: 19.0854 - val_mse: 19.0854\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 19152.9688 - mse: 19152.9688 - val_loss: 13.5925 - val_mse: 13.5925\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 36602.7266 - mse: 36602.7266 - val_loss: 10.1378 - val_mse: 10.1378\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 202954.5781 - mse: 202954.5781 - val_loss: 8.5925 - val_mse: 8.5925\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 354676.9062 - mse: 354676.9062 - val_loss: 8.3871 - val_mse: 8.3871\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 371300.2812 - mse: 371300.2812 - val_loss: 9.0298 - val_mse: 9.0298\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 252353.8594 - mse: 252353.8594 - val_loss: 10.3291 - val_mse: 10.3291\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 94638.0312 - mse: 94638.0312 - val_loss: 12.2287 - val_mse: 12.2287\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5203.2251 - mse: 5203.2251 - val_loss: 14.5149 - val_mse: 14.5149\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 23755.3066 - mse: 23755.3066 - val_loss: 16.7101 - val_mse: 16.7101\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 107350.3984 - mse: 107350.3984 - val_loss: 18.2452 - val_mse: 18.2452\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 176298.7031 - mse: 176298.7031 - val_loss: 18.7323 - val_mse: 18.7323\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 177085.4375 - mse: 177085.4375 - val_loss: 18.1417 - val_mse: 18.1417\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 115139.3047 - mse: 115139.3047 - val_loss: 16.7799 - val_mse: 16.7799\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 39761.5781 - mse: 39761.5781 - val_loss: 15.1177 - val_mse: 15.1177\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1172.0817 - mse: 1172.0817 - val_loss: 13.6008 - val_mse: 13.6008\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 14775.5029 - mse: 14775.5029 - val_loss: 12.5323 - val_mse: 12.5323\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 56561.5742 - mse: 56561.5742 - val_loss: 12.0550 - val_mse: 12.0550\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 87423.4688 - mse: 87423.4688 - val_loss: 12.1918 - val_mse: 12.1918\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 83690.9219 - mse: 83690.9219 - val_loss: 12.8785 - val_mse: 12.8785\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 51211.1289 - mse: 51211.1289 - val_loss: 13.9732 - val_mse: 13.9732\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 15562.7383 - mse: 15562.7383 - val_loss: 15.2533 - val_mse: 15.2533\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 82.8604 - mse: 82.8604 - val_loss: 16.4460 - val_mse: 16.4460\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9508.3320 - mse: 9508.3320 - val_loss: 17.2881 - val_mse: 17.2881\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 30220.0703 - mse: 30220.0703 - val_loss: 17.6089 - val_mse: 17.6089\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 43303.9570 - mse: 43303.9570 - val_loss: 17.3826 - val_mse: 17.3826\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 39038.7148 - mse: 39038.7148 - val_loss: 16.7285 - val_mse: 16.7285\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 22137.9473 - mse: 22137.9473 - val_loss: 15.8532 - val_mse: 15.8532\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5655.1875 - mse: 5655.1875 - val_loss: 14.9821 - val_mse: 14.9821\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 45.0477 - mse: 45.0477 - val_loss: 14.2991 - val_mse: 14.2991\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6146.0806 - mse: 6146.0806 - val_loss: 13.9214 - val_mse: 13.9214\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 16295.0391 - mse: 16295.0391 - val_loss: 13.9038 - val_mse: 13.9038\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 21449.9629 - mse: 21449.9629 - val_loss: 14.2482 - val_mse: 14.2482\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 17944.9395 - mse: 17944.9395 - val_loss: 14.9064 - val_mse: 14.9064\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9169.2158 - mse: 9169.2158 - val_loss: 15.7756 - val_mse: 15.7756\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1773.1914 - mse: 1773.1914 - val_loss: 16.7078 - val_mse: 16.7078\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 225.0394 - mse: 225.0394 - val_loss: 17.5342 - val_mse: 17.5342\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3981.5261 - mse: 3981.5261 - val_loss: 18.1087 - val_mse: 18.1087\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 8797.2373 - mse: 8797.2373 - val_loss: 18.3480 - val_mse: 18.3480\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10511.6807 - mse: 10511.6807 - val_loss: 18.2511 - val_mse: 18.2511\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 8036.7036 - mse: 8036.7036 - val_loss: 17.8952 - val_mse: 17.8952\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3584.4426 - mse: 3584.4426 - val_loss: 17.4053 - val_mse: 17.4053\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 440.4323 - mse: 440.4323 - val_loss: 16.9148 - val_mse: 16.9148\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 348.7736 - mse: 348.7736 - val_loss: 16.5367 - val_mse: 16.5367\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2527.3528 - mse: 2527.3528 - val_loss: 16.3439 - val_mse: 16.3439\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4705.5571 - mse: 4705.5571 - val_loss: 16.3651 - val_mse: 16.3651\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5061.0996 - mse: 5061.0996 - val_loss: 16.5901 - val_mse: 16.5901\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3469.7776 - mse: 3469.7776 - val_loss: 16.9756 - val_mse: 16.9756\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1281.6848 - mse: 1281.6848 - val_loss: 17.4559 - val_mse: 17.4559\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 63.2898 - mse: 63.2898 - val_loss: 17.9514 - val_mse: 17.9514\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 377.4219 - mse: 377.4219 - val_loss: 18.3875 - val_mse: 18.3875\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1571.2247 - mse: 1571.2247 - val_loss: 18.7051 - val_mse: 18.7051\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2478.6377 - mse: 2478.6377 - val_loss: 18.8792 - val_mse: 18.8792\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2370.0105 - mse: 2370.0105 - val_loss: 18.9180 - val_mse: 18.9180\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1419.4502 - mse: 1419.4502 - val_loss: 18.8589 - val_mse: 18.8589\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 401.1815 - mse: 401.1815 - val_loss: 18.7582 - val_mse: 18.7582\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.9761 - mse: 4.9761 - val_loss: 18.6732 - val_mse: 18.6732\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 335.3965 - mse: 335.3965 - val_loss: 18.6504 - val_mse: 18.6504\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 944.6519 - mse: 944.6519 - val_loss: 18.7185 - val_mse: 18.7185\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1271.1556 - mse: 1271.1556 - val_loss: 18.8831 - val_mse: 18.8831\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1067.4569 - mse: 1067.4569 - val_loss: 19.1302 - val_mse: 19.1302\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 539.4857 - mse: 539.4857 - val_loss: 19.4279 - val_mse: 19.4279\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 100.2628 - mse: 100.2628 - val_loss: 19.7377 - val_mse: 19.7377\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 23.3004 - mse: 23.3004 - val_loss: 20.0207 - val_mse: 20.0207\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 262.6288 - mse: 262.6288 - val_loss: 20.2480 - val_mse: 20.2480\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 547.8674 - mse: 547.8674 - val_loss: 20.4057 - val_mse: 20.4057\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 628.6033 - mse: 628.6033 - val_loss: 20.4979 - val_mse: 20.4979\n",
      "Epoch 513/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: 455.3613 - mse: 455.3613 - val_loss: 20.5439 - val_mse: 20.5439\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 183.9927 - mse: 183.9927 - val_loss: 20.5721 - val_mse: 20.5721\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 16.7869 - mse: 16.7869 - val_loss: 20.6137 - val_mse: 20.6137\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 41.7488 - mse: 41.7488 - val_loss: 20.6961 - val_mse: 20.6961\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 186.0334 - mse: 186.0334 - val_loss: 20.8357 - val_mse: 20.8357\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 303.8231 - mse: 303.8231 - val_loss: 21.0366 - val_mse: 21.0366\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 297.2303 - mse: 297.2303 - val_loss: 21.2917 - val_mse: 21.2917\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 181.2988 - mse: 181.2988 - val_loss: 21.5815 - val_mse: 21.5815\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 53.9908 - mse: 53.9908 - val_loss: 21.8829 - val_mse: 21.8829\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.1964 - mse: 4.1964 - val_loss: 22.1704 - val_mse: 22.1704\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 45.9996 - mse: 45.9996 - val_loss: 22.4247 - val_mse: 22.4247\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 122.1100 - mse: 122.1100 - val_loss: 22.6338 - val_mse: 22.6338\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 160.9730 - mse: 160.9730 - val_loss: 22.7993 - val_mse: 22.7993\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 132.7470 - mse: 132.7470 - val_loss: 22.9304 - val_mse: 22.9304\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 66.5062 - mse: 66.5062 - val_loss: 23.0444 - val_mse: 23.0444\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 13.3532 - mse: 13.3532 - val_loss: 23.1624 - val_mse: 23.1624\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7.6961 - mse: 7.6961 - val_loss: 23.3021 - val_mse: 23.3021\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 40.4506 - mse: 40.4506 - val_loss: 23.4788 - val_mse: 23.4788\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 74.8975 - mse: 74.8975 - val_loss: 23.6988 - val_mse: 23.6988\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 80.7856 - mse: 80.7856 - val_loss: 23.9603 - val_mse: 23.9603\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 55.5017 - mse: 55.5017 - val_loss: 24.2545 - val_mse: 24.2545\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 21.4846 - mse: 21.4846 - val_loss: 24.5692 - val_mse: 24.5692\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.0534 - mse: 4.0534 - val_loss: 24.8890 - val_mse: 24.8890\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.8885 - mse: 10.8885 - val_loss: 25.2020 - val_mse: 25.2020\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 30.2040 - mse: 30.2040 - val_loss: 25.4986 - val_mse: 25.4986\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 42.4930 - mse: 42.4930 - val_loss: 25.7764 - val_mse: 25.7764\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 38.1495 - mse: 38.1495 - val_loss: 26.0380 - val_mse: 26.0380\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 21.6826 - mse: 21.6826 - val_loss: 26.2915 - val_mse: 26.2915\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.9583 - mse: 6.9583 - val_loss: 26.5472 - val_mse: 26.5472\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.6740 - mse: 3.6740 - val_loss: 26.8150 - val_mse: 26.8150\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 11.0912 - mse: 11.0912 - val_loss: 27.1046 - val_mse: 27.1046\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 20.2770 - mse: 20.2770 - val_loss: 27.4205 - val_mse: 27.4205\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 22.9201 - mse: 22.9201 - val_loss: 27.7635 - val_mse: 27.7635\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 17.0529 - mse: 17.0529 - val_loss: 28.1290 - val_mse: 28.1290\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 8.2898 - mse: 8.2898 - val_loss: 28.5118 - val_mse: 28.5118\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.1047 - mse: 3.1047 - val_loss: 28.9022 - val_mse: 28.9022\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.3122 - mse: 4.3122 - val_loss: 29.2925 - val_mse: 29.2925\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.1996 - mse: 9.1996 - val_loss: 29.6762 - val_mse: 29.6762\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 12.6092 - mse: 12.6092 - val_loss: 30.0489 - val_mse: 30.0489\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 11.8113 - mse: 11.8113 - val_loss: 30.4091 - val_mse: 30.4091\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 7.4761 - mse: 7.4761 - val_loss: 30.7572 - val_mse: 30.7572\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.5315 - mse: 3.5315 - val_loss: 31.0956 - val_mse: 31.0956\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.4681 - mse: 2.4681 - val_loss: 31.4238 - val_mse: 31.4238\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.2997 - mse: 4.2997 - val_loss: 31.7420 - val_mse: 31.7420\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.7199 - mse: 6.7199 - val_loss: 32.0471 - val_mse: 32.0471\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7.3080 - mse: 7.3080 - val_loss: 32.3333 - val_mse: 32.3333\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.9036 - mse: 5.9036 - val_loss: 32.5911 - val_mse: 32.5911\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.5672 - mse: 3.5672 - val_loss: 32.8099 - val_mse: 32.8099\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.1915 - mse: 2.1915 - val_loss: 32.9768 - val_mse: 32.9768\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.5206 - mse: 2.5206 - val_loss: 33.0797 - val_mse: 33.0797\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.7871 - mse: 3.7871 - val_loss: 33.1084 - val_mse: 33.1084\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4.7471 - mse: 4.7471 - val_loss: 33.0552 - val_mse: 33.0552\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.4688 - mse: 4.4688 - val_loss: 32.9175 - val_mse: 32.9175\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.3955 - mse: 3.3955 - val_loss: 32.6982 - val_mse: 32.6982\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3251 - mse: 2.3251 - val_loss: 32.4073 - val_mse: 32.4073\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.0604 - mse: 2.0604 - val_loss: 32.0603 - val_mse: 32.0603\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.5854 - mse: 2.5854 - val_loss: 31.6744 - val_mse: 31.6744\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.2011 - mse: 3.2011 - val_loss: 31.2653 - val_mse: 31.2653\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.2153 - mse: 3.2153 - val_loss: 30.8476 - val_mse: 30.8476\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7176 - mse: 2.7176 - val_loss: 30.4323 - val_mse: 30.4323\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.0730 - mse: 2.0730 - val_loss: 30.0280 - val_mse: 30.0280\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.7567 - mse: 1.7567 - val_loss: 29.6406 - val_mse: 29.6406\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.8137 - mse: 1.8137 - val_loss: 29.2733 - val_mse: 29.2733\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1369 - mse: 2.1369 - val_loss: 28.9267 - val_mse: 28.9267\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2876 - mse: 2.2876 - val_loss: 28.6041 - val_mse: 28.6041\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.1804 - mse: 2.1804 - val_loss: 28.3046 - val_mse: 28.3046\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.8632 - mse: 1.8632 - val_loss: 28.0273 - val_mse: 28.0273\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.5738 - mse: 1.5738 - val_loss: 27.7721 - val_mse: 27.7721\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.4788 - mse: 1.4788 - val_loss: 27.5358 - val_mse: 27.5358\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.5782 - mse: 1.5782 - val_loss: 27.3151 - val_mse: 27.3151\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6753 - mse: 1.6753 - val_loss: 27.1039 - val_mse: 27.1039\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6341 - mse: 1.6341 - val_loss: 26.8940 - val_mse: 26.8940\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.5212 - mse: 1.5212 - val_loss: 26.6766 - val_mse: 26.6766\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.3279 - mse: 1.3279 - val_loss: 26.4426 - val_mse: 26.4426\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2295 - mse: 1.2295 - val_loss: 26.1850 - val_mse: 26.1850\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2180 - mse: 1.2180 - val_loss: 25.9030 - val_mse: 25.9030\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2694 - mse: 1.2694 - val_loss: 25.6018 - val_mse: 25.6018\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2825 - mse: 1.2825 - val_loss: 25.2911 - val_mse: 25.2911\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2179 - mse: 1.2179 - val_loss: 24.9828 - val_mse: 24.9828\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1250 - mse: 1.1250 - val_loss: 24.6872 - val_mse: 24.6872\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0306 - mse: 1.0306 - val_loss: 24.4121 - val_mse: 24.4121\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0041 - mse: 1.0041 - val_loss: 24.1617 - val_mse: 24.1617\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9956 - mse: 0.9956 - val_loss: 23.9369 - val_mse: 23.9369\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0145 - mse: 1.0145 - val_loss: 23.7340 - val_mse: 23.7340\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9897 - mse: 0.9897 - val_loss: 23.5474 - val_mse: 23.5474\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9572 - mse: 0.9572 - val_loss: 23.3696 - val_mse: 23.3696\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9075 - mse: 0.9075 - val_loss: 23.1930 - val_mse: 23.1930\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8721 - mse: 0.8721 - val_loss: 23.0102 - val_mse: 23.0102\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8503 - mse: 0.8503 - val_loss: 22.8160 - val_mse: 22.8160\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8440 - mse: 0.8440 - val_loss: 22.6097 - val_mse: 22.6097\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8329 - mse: 0.8329 - val_loss: 22.3954 - val_mse: 22.3954\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8178 - mse: 0.8178 - val_loss: 22.1809 - val_mse: 22.1809\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8019 - mse: 0.8019 - val_loss: 21.9731 - val_mse: 21.9731\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7820 - mse: 0.7820 - val_loss: 21.7789 - val_mse: 21.7789\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7700 - mse: 0.7700 - val_loss: 21.6019 - val_mse: 21.6019\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7442 - mse: 0.7442 - val_loss: 21.4439 - val_mse: 21.4439\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7518 - mse: 0.7518 - val_loss: 21.3023 - val_mse: 21.3023\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7368 - mse: 0.7368 - val_loss: 21.1730 - val_mse: 21.1730\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7295 - mse: 0.7295 - val_loss: 21.0501 - val_mse: 21.0501\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7217 - mse: 0.7217 - val_loss: 20.9268 - val_mse: 20.9268\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7137 - mse: 0.7137 - val_loss: 20.7999 - val_mse: 20.7999\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7133 - mse: 0.7133 - val_loss: 20.6662 - val_mse: 20.6662\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6967 - mse: 0.6967 - val_loss: 20.5266 - val_mse: 20.5266\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6845 - mse: 0.6845 - val_loss: 20.3848 - val_mse: 20.3848\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6934 - mse: 0.6934 - val_loss: 20.2457 - val_mse: 20.2457\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6860 - mse: 0.6860 - val_loss: 20.1131 - val_mse: 20.1131\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6871 - mse: 0.6871 - val_loss: 19.9903 - val_mse: 19.9903\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6840 - mse: 0.6840 - val_loss: 19.8788 - val_mse: 19.8788\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6727 - mse: 0.6727 - val_loss: 19.7777 - val_mse: 19.7777\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6689 - mse: 0.6689 - val_loss: 19.6852 - val_mse: 19.6852\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6604 - mse: 0.6604 - val_loss: 19.5977 - val_mse: 19.5977\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6591 - mse: 0.6591 - val_loss: 19.5118 - val_mse: 19.5118\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6598 - mse: 0.6598 - val_loss: 19.4257 - val_mse: 19.4257\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6562 - mse: 0.6562 - val_loss: 19.3377 - val_mse: 19.3377\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6514 - mse: 0.6514 - val_loss: 19.2496 - val_mse: 19.2496\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6489 - mse: 0.6489 - val_loss: 19.1625 - val_mse: 19.1625\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6485 - mse: 0.6485 - val_loss: 19.0783 - val_mse: 19.0783\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6473 - mse: 0.6473 - val_loss: 18.9993 - val_mse: 18.9993\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6467 - mse: 0.6467 - val_loss: 18.9258 - val_mse: 18.9258\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6559 - mse: 0.6559 - val_loss: 18.8579 - val_mse: 18.8579\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6481 - mse: 0.6481 - val_loss: 18.7944 - val_mse: 18.7944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6401 - mse: 0.6401 - val_loss: 18.7336 - val_mse: 18.7336\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6393 - mse: 0.6393 - val_loss: 18.6745 - val_mse: 18.6745\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6465 - mse: 0.6465 - val_loss: 18.6153 - val_mse: 18.6153\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6425 - mse: 0.6425 - val_loss: 18.5566 - val_mse: 18.5566\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6446 - mse: 0.6446 - val_loss: 18.4990 - val_mse: 18.4990\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6429 - mse: 0.6429 - val_loss: 18.4429 - val_mse: 18.4429\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6423 - mse: 0.6423 - val_loss: 18.3896 - val_mse: 18.3896\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6422 - mse: 0.6422 - val_loss: 18.3409 - val_mse: 18.3409\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6419 - mse: 0.6419 - val_loss: 18.2952 - val_mse: 18.2952\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6417 - mse: 0.6417 - val_loss: 18.2536 - val_mse: 18.2536\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6415 - mse: 0.6415 - val_loss: 18.2144 - val_mse: 18.2144\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6410 - mse: 0.6410 - val_loss: 18.1767 - val_mse: 18.1767\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6401 - mse: 0.6401 - val_loss: 18.1391 - val_mse: 18.1391\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6423 - mse: 0.6423 - val_loss: 18.1017 - val_mse: 18.1017\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6366 - mse: 0.6366 - val_loss: 18.0637 - val_mse: 18.0637\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6365 - mse: 0.6365 - val_loss: 18.0264 - val_mse: 18.0264\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6368 - mse: 0.6368 - val_loss: 17.9896 - val_mse: 17.9896\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6377 - mse: 0.6377 - val_loss: 17.9551 - val_mse: 17.9551\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6394 - mse: 0.6394 - val_loss: 17.9230 - val_mse: 17.9230\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6389 - mse: 0.6389 - val_loss: 17.8931 - val_mse: 17.8931\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6278 - mse: 0.6278 - val_loss: 17.8654 - val_mse: 17.8654\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6276 - mse: 0.6276 - val_loss: 17.8401 - val_mse: 17.8401\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6388 - mse: 0.6388 - val_loss: 17.8155 - val_mse: 17.8155\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6389 - mse: 0.6389 - val_loss: 17.7912 - val_mse: 17.7912\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6359 - mse: 0.6359 - val_loss: 17.7673 - val_mse: 17.7673\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6371 - mse: 0.6371 - val_loss: 17.7430 - val_mse: 17.7430\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6370 - mse: 0.6370 - val_loss: 17.7198 - val_mse: 17.7198\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6368 - mse: 0.6368 - val_loss: 17.6969 - val_mse: 17.6969\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6368 - mse: 0.6368 - val_loss: 17.6749 - val_mse: 17.6749\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6367 - mse: 0.6367 - val_loss: 17.6549 - val_mse: 17.6549\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6366 - mse: 0.6366 - val_loss: 17.6358 - val_mse: 17.6358\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6365 - mse: 0.6365 - val_loss: 17.6178 - val_mse: 17.6178\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6352 - mse: 0.6352 - val_loss: 17.6002 - val_mse: 17.6002\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6351 - mse: 0.6351 - val_loss: 17.5837 - val_mse: 17.5837\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6351 - mse: 0.6351 - val_loss: 17.5675 - val_mse: 17.5675\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6351 - mse: 0.6351 - val_loss: 17.5515 - val_mse: 17.5515\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6350 - mse: 0.6350 - val_loss: 17.5358 - val_mse: 17.5358\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6350 - mse: 0.6350 - val_loss: 17.5202 - val_mse: 17.5202\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6349 - mse: 0.6349 - val_loss: 17.5060 - val_mse: 17.5060\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6350 - mse: 0.6350 - val_loss: 17.4916 - val_mse: 17.4916\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6349 - mse: 0.6349 - val_loss: 17.4785 - val_mse: 17.4785\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6349 - mse: 0.6349 - val_loss: 17.4660 - val_mse: 17.4660\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6349 - mse: 0.6349 - val_loss: 17.4545 - val_mse: 17.4545\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6347 - mse: 0.6347 - val_loss: 17.4431 - val_mse: 17.4431\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6347 - mse: 0.6347 - val_loss: 17.4315 - val_mse: 17.4315\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6347 - mse: 0.6347 - val_loss: 17.4205 - val_mse: 17.4205\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6347 - mse: 0.6347 - val_loss: 17.4090 - val_mse: 17.4090\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6346 - mse: 0.6346 - val_loss: 17.3979 - val_mse: 17.3979\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6347 - mse: 0.6347 - val_loss: 17.3869 - val_mse: 17.3869\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6345 - mse: 0.6345 - val_loss: 17.3759 - val_mse: 17.3759\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6346 - mse: 0.6346 - val_loss: 17.3657 - val_mse: 17.3657\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6346 - mse: 0.6346 - val_loss: 17.3558 - val_mse: 17.3558\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6346 - mse: 0.6346 - val_loss: 17.3461 - val_mse: 17.3461\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6345 - mse: 0.6345 - val_loss: 17.3365 - val_mse: 17.3365\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6346 - mse: 0.6346 - val_loss: 17.3274 - val_mse: 17.3274\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6343 - mse: 0.6343 - val_loss: 17.3180 - val_mse: 17.3180\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6343 - mse: 0.6343 - val_loss: 17.3092 - val_mse: 17.3092\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6342 - mse: 0.6342 - val_loss: 17.2999 - val_mse: 17.2999\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6342 - mse: 0.6342 - val_loss: 17.2908 - val_mse: 17.2908\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6344 - mse: 0.6344 - val_loss: 17.2817 - val_mse: 17.2817\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6343 - mse: 0.6343 - val_loss: 17.2728 - val_mse: 17.2728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6342 - mse: 0.6342 - val_loss: 17.2642 - val_mse: 17.2642\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6342 - mse: 0.6342 - val_loss: 17.2552 - val_mse: 17.2552\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6342 - mse: 0.6342 - val_loss: 17.2469 - val_mse: 17.2469\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6341 - mse: 0.6341 - val_loss: 17.2384 - val_mse: 17.2384\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6341 - mse: 0.6341 - val_loss: 17.2298 - val_mse: 17.2298\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6341 - mse: 0.6341 - val_loss: 17.2216 - val_mse: 17.2216\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6341 - mse: 0.6341 - val_loss: 17.2124 - val_mse: 17.2124\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6340 - mse: 0.6340 - val_loss: 17.2039 - val_mse: 17.2039\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6340 - mse: 0.6340 - val_loss: 17.1953 - val_mse: 17.1953\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6339 - mse: 0.6339 - val_loss: 17.1866 - val_mse: 17.1866\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6339 - mse: 0.6339 - val_loss: 17.1785 - val_mse: 17.1785\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6339 - mse: 0.6339 - val_loss: 17.1705 - val_mse: 17.1705\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6340 - mse: 0.6340 - val_loss: 17.1621 - val_mse: 17.1621\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6336 - mse: 0.6336 - val_loss: 17.1542 - val_mse: 17.1542\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6335 - mse: 0.6335 - val_loss: 17.1462 - val_mse: 17.1462\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6335 - mse: 0.6335 - val_loss: 17.1384 - val_mse: 17.1384\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6335 - mse: 0.6335 - val_loss: 17.1302 - val_mse: 17.1302\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6335 - mse: 0.6335 - val_loss: 17.1221 - val_mse: 17.1221\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6335 - mse: 0.6335 - val_loss: 17.1138 - val_mse: 17.1138\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6333 - mse: 0.6333 - val_loss: 17.1058 - val_mse: 17.1058\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6333 - mse: 0.6333 - val_loss: 17.0977 - val_mse: 17.0977\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6334 - mse: 0.6334 - val_loss: 17.0894 - val_mse: 17.0894\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6332 - mse: 0.6332 - val_loss: 17.0817 - val_mse: 17.0817\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6332 - mse: 0.6332 - val_loss: 17.0736 - val_mse: 17.0736\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6331 - mse: 0.6331 - val_loss: 17.0657 - val_mse: 17.0657\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6331 - mse: 0.6331 - val_loss: 17.0580 - val_mse: 17.0580\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6331 - mse: 0.6331 - val_loss: 17.0501 - val_mse: 17.0501\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6330 - mse: 0.6330 - val_loss: 17.0422 - val_mse: 17.0422\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6331 - mse: 0.6331 - val_loss: 17.0344 - val_mse: 17.0344\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6331 - mse: 0.6331 - val_loss: 17.0270 - val_mse: 17.0270\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6330 - mse: 0.6330 - val_loss: 17.0189 - val_mse: 17.0189\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6330 - mse: 0.6330 - val_loss: 17.0112 - val_mse: 17.0112\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6329 - mse: 0.6329 - val_loss: 17.0036 - val_mse: 17.0036\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6330 - mse: 0.6330 - val_loss: 16.9962 - val_mse: 16.9962\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6330 - mse: 0.6330 - val_loss: 16.9888 - val_mse: 16.9888\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6328 - mse: 0.6328 - val_loss: 16.9809 - val_mse: 16.9809\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6329 - mse: 0.6329 - val_loss: 16.9732 - val_mse: 16.9732\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6329 - mse: 0.6329 - val_loss: 16.9654 - val_mse: 16.9654\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6328 - mse: 0.6328 - val_loss: 16.9578 - val_mse: 16.9578\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6328 - mse: 0.6328 - val_loss: 16.9502 - val_mse: 16.9502\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6328 - mse: 0.6328 - val_loss: 16.9424 - val_mse: 16.9424\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6327 - mse: 0.6327 - val_loss: 16.9350 - val_mse: 16.9350\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6327 - mse: 0.6327 - val_loss: 16.9271 - val_mse: 16.9271\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6327 - mse: 0.6327 - val_loss: 16.9200 - val_mse: 16.9200\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6326 - mse: 0.6326 - val_loss: 16.9123 - val_mse: 16.9123\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6326 - mse: 0.6326 - val_loss: 16.9048 - val_mse: 16.9048\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6326 - mse: 0.6326 - val_loss: 16.8973 - val_mse: 16.8973\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6327 - mse: 0.6327 - val_loss: 16.8897 - val_mse: 16.8897\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6327 - mse: 0.6327 - val_loss: 16.8820 - val_mse: 16.8820\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6326 - mse: 0.6326 - val_loss: 16.8747 - val_mse: 16.8747\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6326 - mse: 0.6326 - val_loss: 16.8670 - val_mse: 16.8670\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6325 - mse: 0.6325 - val_loss: 16.8595 - val_mse: 16.8595\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6325 - mse: 0.6325 - val_loss: 16.8521 - val_mse: 16.8521\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6325 - mse: 0.6325 - val_loss: 16.8449 - val_mse: 16.8449\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6325 - mse: 0.6325 - val_loss: 16.8372 - val_mse: 16.8372\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6325 - mse: 0.6325 - val_loss: 16.8298 - val_mse: 16.8298\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6324 - mse: 0.6324 - val_loss: 16.8225 - val_mse: 16.8225\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6326 - mse: 0.6326 - val_loss: 16.8151 - val_mse: 16.8151\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6326 - mse: 0.6326 - val_loss: 16.8076 - val_mse: 16.8076\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6325 - mse: 0.6325 - val_loss: 16.8003 - val_mse: 16.8003\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6325 - mse: 0.6325 - val_loss: 16.7932 - val_mse: 16.7932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6324 - mse: 0.6324 - val_loss: 16.7856 - val_mse: 16.7856\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6324 - mse: 0.6324 - val_loss: 16.7782 - val_mse: 16.7782\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6324 - mse: 0.6324 - val_loss: 16.7709 - val_mse: 16.7709\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6324 - mse: 0.6324 - val_loss: 16.7637 - val_mse: 16.7637\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6322 - mse: 0.6322 - val_loss: 16.7565 - val_mse: 16.7565\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6322 - mse: 0.6322 - val_loss: 16.7493 - val_mse: 16.7493\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6322 - mse: 0.6322 - val_loss: 16.7421 - val_mse: 16.7421\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6323 - mse: 0.6323 - val_loss: 16.7348 - val_mse: 16.7348\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6321 - mse: 0.6321 - val_loss: 16.7277 - val_mse: 16.7277\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6321 - mse: 0.6321 - val_loss: 16.7206 - val_mse: 16.7206\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6321 - mse: 0.6321 - val_loss: 16.7135 - val_mse: 16.7135\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6322 - mse: 0.6322 - val_loss: 16.7063 - val_mse: 16.7063\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6322 - mse: 0.6322 - val_loss: 16.6993 - val_mse: 16.6993\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6321 - mse: 0.6321 - val_loss: 16.6924 - val_mse: 16.6924\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6320 - mse: 0.6320 - val_loss: 16.6854 - val_mse: 16.6854\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6320 - mse: 0.6320 - val_loss: 16.6778 - val_mse: 16.6778\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6319 - mse: 0.6319 - val_loss: 16.6707 - val_mse: 16.6707\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6320 - mse: 0.6320 - val_loss: 16.6633 - val_mse: 16.6633\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6319 - mse: 0.6319 - val_loss: 16.6563 - val_mse: 16.6563\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6319 - mse: 0.6319 - val_loss: 16.6490 - val_mse: 16.6490\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6319 - mse: 0.6319 - val_loss: 16.6418 - val_mse: 16.6418\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6319 - mse: 0.6319 - val_loss: 16.6347 - val_mse: 16.6347\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6319 - mse: 0.6319 - val_loss: 16.6274 - val_mse: 16.6274\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6319 - mse: 0.6319 - val_loss: 16.6195 - val_mse: 16.6195\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6319 - mse: 0.6319 - val_loss: 16.6126 - val_mse: 16.6126\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6319 - mse: 0.6319 - val_loss: 16.6052 - val_mse: 16.6052\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5977 - val_mse: 16.5977\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5904 - val_mse: 16.5904\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5830 - val_mse: 16.5830\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5757 - val_mse: 16.5757\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5687 - val_mse: 16.5687\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5613 - val_mse: 16.5613\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6317 - mse: 0.6317 - val_loss: 16.5542 - val_mse: 16.5542\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5470 - val_mse: 16.5470\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5402 - val_mse: 16.5402\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5328 - val_mse: 16.5328\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5256 - val_mse: 16.5256\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6317 - mse: 0.6317 - val_loss: 16.5187 - val_mse: 16.5187\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - mse: 0.6318 - val_loss: 16.5119 - val_mse: 16.5119\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6317 - mse: 0.6317 - val_loss: 16.5052 - val_mse: 16.5052\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6317 - mse: 0.6317 - val_loss: 16.4980 - val_mse: 16.4980\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6317 - mse: 0.6317 - val_loss: 16.4913 - val_mse: 16.4913\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6316 - mse: 0.6316 - val_loss: 16.4845 - val_mse: 16.4845\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6316 - mse: 0.6316 - val_loss: 16.4774 - val_mse: 16.4774\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6316 - mse: 0.6316 - val_loss: 16.4706 - val_mse: 16.4706\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6316 - mse: 0.6316 - val_loss: 16.4634 - val_mse: 16.4634\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6317 - mse: 0.6317 - val_loss: 16.4562 - val_mse: 16.4562\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6316 - mse: 0.6316 - val_loss: 16.4492 - val_mse: 16.4492\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6315 - mse: 0.6315 - val_loss: 16.4420 - val_mse: 16.4420\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6315 - mse: 0.6315 - val_loss: 16.4351 - val_mse: 16.4351\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6315 - mse: 0.6315 - val_loss: 16.4283 - val_mse: 16.4283\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6315 - mse: 0.6315 - val_loss: 16.4215 - val_mse: 16.4215\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6314 - mse: 0.6314 - val_loss: 16.4149 - val_mse: 16.4149\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6314 - mse: 0.6314 - val_loss: 16.4081 - val_mse: 16.4081\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6314 - mse: 0.6314 - val_loss: 16.4015 - val_mse: 16.4015\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6314 - mse: 0.6314 - val_loss: 16.3948 - val_mse: 16.3948\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6313 - mse: 0.6313 - val_loss: 16.3882 - val_mse: 16.3882\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6313 - mse: 0.6313 - val_loss: 16.3814 - val_mse: 16.3814\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6313 - mse: 0.6313 - val_loss: 16.3745 - val_mse: 16.3745\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6313 - mse: 0.6313 - val_loss: 16.3677 - val_mse: 16.3677\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6313 - mse: 0.6313 - val_loss: 16.3608 - val_mse: 16.3608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6312 - mse: 0.6312 - val_loss: 16.3543 - val_mse: 16.3543\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6312 - mse: 0.6312 - val_loss: 16.3478 - val_mse: 16.3478\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6312 - mse: 0.6312 - val_loss: 16.3416 - val_mse: 16.3416\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6311 - mse: 0.6311 - val_loss: 16.3356 - val_mse: 16.3356\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6312 - mse: 0.6312 - val_loss: 16.3295 - val_mse: 16.3295\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6311 - mse: 0.6311 - val_loss: 16.3237 - val_mse: 16.3237\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6311 - mse: 0.6311 - val_loss: 16.3177 - val_mse: 16.3177\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6311 - mse: 0.6311 - val_loss: 16.3119 - val_mse: 16.3119\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6311 - mse: 0.6311 - val_loss: 16.3057 - val_mse: 16.3057\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2998 - val_mse: 16.2998\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2935 - val_mse: 16.2935\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2876 - val_mse: 16.2876\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2814 - val_mse: 16.2814\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6311 - mse: 0.6311 - val_loss: 16.2751 - val_mse: 16.2751\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2689 - val_mse: 16.2689\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6311 - mse: 0.6311 - val_loss: 16.2629 - val_mse: 16.2629\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2571 - val_mse: 16.2571\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2514 - val_mse: 16.2514\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2455 - val_mse: 16.2455\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2399 - val_mse: 16.2399\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2336 - val_mse: 16.2336\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 16.2278 - val_mse: 16.2278\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6308 - mse: 0.6308 - val_loss: 16.2217 - val_mse: 16.2217\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6308 - mse: 0.6308 - val_loss: 16.2156 - val_mse: 16.2156\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6308 - mse: 0.6308 - val_loss: 16.2092 - val_mse: 16.2092\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 16.2032 - val_mse: 16.2032\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 16.1973 - val_mse: 16.1973\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 16.1906 - val_mse: 16.1906\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 16.1847 - val_mse: 16.1847\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6308 - mse: 0.6308 - val_loss: 16.1781 - val_mse: 16.1781\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6308 - mse: 0.6308 - val_loss: 16.1716 - val_mse: 16.1716\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6308 - mse: 0.6308 - val_loss: 16.1655 - val_mse: 16.1655\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 16.1592 - val_mse: 16.1592\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 16.1531 - val_mse: 16.1531\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 16.1466 - val_mse: 16.1466\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 16.1405 - val_mse: 16.1405\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 16.1342 - val_mse: 16.1342\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6304 - mse: 0.6304 - val_loss: 16.1282 - val_mse: 16.1282\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6304 - mse: 0.6304 - val_loss: 16.1218 - val_mse: 16.1218\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6304 - mse: 0.6304 - val_loss: 16.1158 - val_mse: 16.1158\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6303 - mse: 0.6303 - val_loss: 16.1094 - val_mse: 16.1094\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.1033 - val_mse: 16.1033\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0970 - val_mse: 16.0970\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0913 - val_mse: 16.0913\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0853 - val_mse: 16.0853\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0796 - val_mse: 16.0796\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6303 - mse: 0.6303 - val_loss: 16.0738 - val_mse: 16.0738\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0682 - val_mse: 16.0682\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0621 - val_mse: 16.0621\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0564 - val_mse: 16.0564\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0506 - val_mse: 16.0506\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0448 - val_mse: 16.0448\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0396 - val_mse: 16.0396\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 16.0337 - val_mse: 16.0337\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0283 - val_mse: 16.0283\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0228 - val_mse: 16.0228\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 16.0172 - val_mse: 16.0172\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 16.0117 - val_mse: 16.0117\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 16.0061 - val_mse: 16.0061\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 16.0006 - val_mse: 16.0006\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 15.9951 - val_mse: 15.9951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6300 - mse: 0.6300 - val_loss: 15.9897 - val_mse: 15.9897\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 15.9841 - val_mse: 15.9841\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 15.9787 - val_mse: 15.9787\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 15.9734 - val_mse: 15.9734\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 15.9680 - val_mse: 15.9680\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6304 - mse: 0.6304 - val_loss: 15.9627 - val_mse: 15.9627\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6303 - mse: 0.6303 - val_loss: 15.9570 - val_mse: 15.9570\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6303 - mse: 0.6303 - val_loss: 15.9517 - val_mse: 15.9517\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6303 - mse: 0.6303 - val_loss: 15.9461 - val_mse: 15.9461\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6303 - mse: 0.6303 - val_loss: 15.9409 - val_mse: 15.9409\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6303 - mse: 0.6303 - val_loss: 15.9353 - val_mse: 15.9353\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6303 - mse: 0.6303 - val_loss: 15.9299 - val_mse: 15.9299\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 15.9244 - val_mse: 15.9244\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 15.9188 - val_mse: 15.9188\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6303 - mse: 0.6303 - val_loss: 15.9134 - val_mse: 15.9134\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 15.9076 - val_mse: 15.9076\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 15.9021 - val_mse: 15.9021\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 15.8963 - val_mse: 15.8963\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 15.8911 - val_mse: 15.8911\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 15.8853 - val_mse: 15.8853\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 15.8793 - val_mse: 15.8793\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 15.8740 - val_mse: 15.8740\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6302 - mse: 0.6302 - val_loss: 15.8684 - val_mse: 15.8684\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 15.8631 - val_mse: 15.8631\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 15.8582 - val_mse: 15.8582\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 15.8531 - val_mse: 15.8531\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 15.8481 - val_mse: 15.8481\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.8432 - val_mse: 15.8432\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.8383 - val_mse: 15.8383\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.8332 - val_mse: 15.8332\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6301 - mse: 0.6301 - val_loss: 15.8284 - val_mse: 15.8284\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.8233 - val_mse: 15.8233\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.8179 - val_mse: 15.8179\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.8130 - val_mse: 15.8130\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6300 - mse: 0.6300 - val_loss: 15.8080 - val_mse: 15.8080\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6297 - mse: 0.6297 - val_loss: 15.8030 - val_mse: 15.8030\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6300 - mse: 0.6300 - val_loss: 15.7980 - val_mse: 15.7980\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6297 - mse: 0.6297 - val_loss: 15.7936 - val_mse: 15.7936\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.7887 - val_mse: 15.7887\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.7842 - val_mse: 15.7842\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.7794 - val_mse: 15.7794\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6297 - mse: 0.6297 - val_loss: 15.7744 - val_mse: 15.7744\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6297 - mse: 0.6297 - val_loss: 15.7696 - val_mse: 15.7696\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.7647 - val_mse: 15.7647\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6297 - mse: 0.6297 - val_loss: 15.7594 - val_mse: 15.7594\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.7544 - val_mse: 15.7544\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6299 - mse: 0.6299 - val_loss: 15.7490 - val_mse: 15.7490\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.7436 - val_mse: 15.7436\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.7386 - val_mse: 15.7386\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.7334 - val_mse: 15.7334\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6298 - mse: 0.6298 - val_loss: 15.7281 - val_mse: 15.7281\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.7233 - val_mse: 15.7233\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.7184 - val_mse: 15.7184\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.7136 - val_mse: 15.7136\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.7090 - val_mse: 15.7090\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.7042 - val_mse: 15.7042\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.6993 - val_mse: 15.6993\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.6946 - val_mse: 15.6946\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6295 - mse: 0.6295 - val_loss: 15.6896 - val_mse: 15.6896\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6295 - mse: 0.6295 - val_loss: 15.6847 - val_mse: 15.6847\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6295 - mse: 0.6295 - val_loss: 15.6803 - val_mse: 15.6803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.6752 - val_mse: 15.6752\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.6710 - val_mse: 15.6710\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.6656 - val_mse: 15.6656\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.6612 - val_mse: 15.6612\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.6567 - val_mse: 15.6567\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.6524 - val_mse: 15.6524\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.6479 - val_mse: 15.6479\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6295 - mse: 0.6295 - val_loss: 15.6437 - val_mse: 15.6437\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6294 - mse: 0.6294 - val_loss: 15.6394 - val_mse: 15.6394\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6295 - mse: 0.6295 - val_loss: 15.6351 - val_mse: 15.6351\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6295 - mse: 0.6295 - val_loss: 15.6304 - val_mse: 15.6304\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6295 - mse: 0.6295 - val_loss: 15.6260 - val_mse: 15.6260\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6295 - mse: 0.6295 - val_loss: 15.6216 - val_mse: 15.6216\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6293 - mse: 0.6293 - val_loss: 15.6164 - val_mse: 15.6164\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6294 - mse: 0.6294 - val_loss: 15.6124 - val_mse: 15.6124\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6293 - mse: 0.6293 - val_loss: 15.6077 - val_mse: 15.6077\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6293 - mse: 0.6293 - val_loss: 15.6030 - val_mse: 15.6030\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6293 - mse: 0.6293 - val_loss: 15.5985 - val_mse: 15.5985\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6293 - mse: 0.6293 - val_loss: 15.5939 - val_mse: 15.5939\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 15.5894 - val_mse: 15.5894\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5851 - val_mse: 15.5851\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5807 - val_mse: 15.5807\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5763 - val_mse: 15.5763\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 15.5720 - val_mse: 15.5720\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6305 - mse: 0.6305 - val_loss: 15.5677 - val_mse: 15.5677\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 15.5635 - val_mse: 15.5635\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5588 - val_mse: 15.5588\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5544 - val_mse: 15.5544\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5501 - val_mse: 15.5501\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5459 - val_mse: 15.5459\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 15.5416 - val_mse: 15.5416\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5372 - val_mse: 15.5372\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5326 - val_mse: 15.5326\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5284 - val_mse: 15.5284\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5239 - val_mse: 15.5239\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5195 - val_mse: 15.5195\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5153 - val_mse: 15.5153\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5112 - val_mse: 15.5112\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.5071 - val_mse: 15.5071\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6309 - mse: 0.6309 - val_loss: 15.5033 - val_mse: 15.5033\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.4993 - val_mse: 15.4993\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6307 - mse: 0.6307 - val_loss: 15.4949 - val_mse: 15.4949\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.4908 - val_mse: 15.4908\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - mse: 0.6306 - val_loss: 15.4861 - val_mse: 15.4861\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6309 - mse: 0.6309 - val_loss: 15.4818 - val_mse: 15.4818\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6311 - mse: 0.6311 - val_loss: 15.4773 - val_mse: 15.4773\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6308 - mse: 0.6308 - val_loss: 15.4729 - val_mse: 15.4729\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6311 - mse: 0.6311 - val_loss: 15.4687 - val_mse: 15.4687\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 15.4645 - val_mse: 15.4645\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 15.4602 - val_mse: 15.4602\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6309 - mse: 0.6309 - val_loss: 15.4560 - val_mse: 15.4560\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6309 - mse: 0.6309 - val_loss: 15.4516 - val_mse: 15.4516\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 15.4472 - val_mse: 15.4472\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6309 - mse: 0.6309 - val_loss: 15.4428 - val_mse: 15.4428\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 15.4385 - val_mse: 15.4385\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6310 - mse: 0.6310 - val_loss: 15.4346 - val_mse: 15.4346\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6309 - mse: 0.6309 - val_loss: 15.4302 - val_mse: 15.4302\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6308 - mse: 0.6308 - val_loss: 15.4263 - val_mse: 15.4263\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6309 - mse: 0.6309 - val_loss: 15.4218 - val_mse: 15.4218\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6309 - mse: 0.6309 - val_loss: 15.4181 - val_mse: 15.4181\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.4144 - val_mse: 15.4144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6296 - mse: 0.6296 - val_loss: 15.4103 - val_mse: 15.4103\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "shistory = smodel.fit(X_train,y_train,epochs=1000,validation_split=0.2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [3]]]\n",
      "Prediction: [2.187451  2.6195827 5.040031 ]\n",
      "Actual: [2 3 5]\n"
     ]
    }
   ],
   "source": [
    "#try on prediction\n",
    "print(X_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(smodel.predict(X_test))))\n",
    "print(\"Actual: \" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13e087ac0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT MSE from the 3 graphs\n",
    "pyplot.plot(history.history['mse'],label='simple')\n",
    "pyplot.plot(bihistory.history['mse'],label='bi')\n",
    "pyplot.plot(shistory.history['mse'],label='stack')\n",
    "pyplot.xlabel(\"epochs\")\n",
    "pyplot.ylabel(\"MSE\")\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgError(p, a):\n",
    "    z = zip(p,a)\n",
    "    e = []\n",
    "    for i, j in z:\n",
    "        e.append(abs(i-j))\n",
    "    return sum(e)/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Simple LSTM</th>\n",
       "      <th>Stacked LSTM</th>\n",
       "      <th>Bidirectional LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.212647</td>\n",
       "      <td>0.202633</td>\n",
       "      <td>0.234644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Simple LSTM  Stacked LSTM  Bidirectional LSTM\n",
       "0     0.212647      0.202633            0.234644"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SHOW ERROR TABLE\n",
    "err_arch = []\n",
    "for m in [model, smodel, bimodel]:\n",
    "    p = np.squeeze(m.predict(X_test))\n",
    "    e = getAvgError(p,y_test)\n",
    "    err_arch.append(e)\n",
    "\n",
    "pd.DataFrame([err_arch], columns=[\"Simple LSTM\", \"Stacked LSTM\", \"Bidirectional LSTM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [3]]]\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x145e68a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction: [2.3026855 2.8211935 4.988802 ]\n",
      "Actual: [2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# Simple LSTM - RELU Activation\n",
    "m=50\n",
    "model = Sequential()\n",
    "model.add(LSTM(m, activation='relu', input_shape=(fib_look, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse',metrics=['mse'])\n",
    "history = model.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=0)\n",
    "\n",
    "#try on prediction\n",
    "print(X_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(model.predict(X_test))))\n",
    "print(\"Actual: \" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [3]]]\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1441ac550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction: [11.73003  11.787016 11.944893]\n",
      "Actual: [2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# Simple LSTM - Sigmoid Activation\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(m, activation='sigmoid', input_shape=(fib_look, 1)))\n",
    "model2.add(Dense(1))\n",
    "model2.compile(optimizer='adam', loss='mse',metrics=['mse'])\n",
    "history2 = model2.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=0)\n",
    "\n",
    "#try on prediction\n",
    "print(X_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(model2.predict(X_test))))\n",
    "print(\"Actual: \" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [3]]]\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x13f741a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction: [2.6555212 3.0589612 5.1326575]\n",
      "Actual: [2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# Simple LSTM - Linear Activation\n",
    "m=50\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(m, activation='linear', input_shape=(fib_look, 1)))\n",
    "model3.add(Dense(1))\n",
    "model3.compile(optimizer='adam', loss='mse',metrics=['mse'])\n",
    "history3 = model3.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=0)\n",
    "\n",
    "#try on prediction\n",
    "print(X_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(model3.predict(X_test))))\n",
    "print(\"Actual: \" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [3]]]\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x13fbf00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction: [2.0673404 5.6786513 4.8972044]\n",
      "Actual: [2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# Simple LSTM - TANH Activation\n",
    "model4 = Sequential()\n",
    "model4.add(LSTM(m, activation='tanh', input_shape=(fib_look, 1)))\n",
    "model4.add(Dense(1))\n",
    "model4.compile(optimizer='adam', loss='mse',metrics=['mse'])\n",
    "history4 = model4.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=0)\n",
    "\n",
    "#try on prediction\n",
    "print(X_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(model4.predict(X_test))))\n",
    "print(\"Actual: \" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [3]]]\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14401c8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction: [2.3934603 3.4214082 4.9973035]\n",
      "Actual: [2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# Simple LSTM - Softplus Activation\n",
    "m = 50\n",
    "model5 = Sequential()\n",
    "model5.add(LSTM(m, activation='softplus', input_shape=(fib_look, 1)))\n",
    "model5.add(Dense(1))\n",
    "model5.compile(optimizer='adam', loss='mse',metrics=['mse'])\n",
    "history5 = model5.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=0)\n",
    "\n",
    "#try on prediction\n",
    "print(X_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(model5.predict(X_test))))\n",
    "print(\"Actual: \" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [3]]]\n",
      "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x144667040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction: [nan nan nan]\n",
      "Actual: [2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# Simple LSTM - exponential Activation\n",
    "model6 = Sequential()\n",
    "model6.add(LSTM(m, activation='exponential', input_shape=(fib_look, 1)))\n",
    "model6.add(Dense(1))\n",
    "model6.compile(optimizer='adam', loss='mse',metrics=['mse'])\n",
    "history6 = model6.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=0)\n",
    "\n",
    "#try on prediction\n",
    "print(X_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(model6.predict(X_test))))\n",
    "print(\"Actual: \" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [3]]]\n",
      "WARNING:tensorflow:10 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x146cb1700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction: [2.6727412 3.142156  5.1536427]\n",
      "Actual: [2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# Simple LSTM - ELU Activation\n",
    "model7 = Sequential()\n",
    "model7.add(LSTM(m, activation='elu', input_shape=(fib_look, 1)))\n",
    "model7.add(Dense(1))\n",
    "model7.compile(optimizer='adam', loss='mse',metrics=['mse'])\n",
    "history7 = model7.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=0)\n",
    "\n",
    "#try on prediction\n",
    "print(X_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(model7.predict(X_test))))\n",
    "print(\"Actual: \" + str(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x146c675b0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#compare all of them together\n",
    "#PLOT MSE from the 7 graphs\n",
    "pyplot.plot(history.history['mse'],label='relu')\n",
    "pyplot.plot(history2.history['mse'],label='sigmoid')\n",
    "pyplot.plot(history3.history['mse'],label='linear')\n",
    "pyplot.plot(history4.history['mse'],label='tanh')\n",
    "pyplot.plot(history5.history['mse'],label='softplus')\n",
    "pyplot.plot(history6.history['mse'],label='exponential')\n",
    "pyplot.plot(history7.history['mse'],label='elu')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x144991d60>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print original and best\n",
    "pyplot.plot(history.history['mse'],label='relu')\n",
    "pyplot.plot(history3.history['mse'],label='linear')\n",
    "pyplot.plot(history5.history['mse'],label='softplus')\n",
    "pyplot.xlabel(\"epochs\")\n",
    "pyplot.ylabel(\"MSE\")\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReLU</th>\n",
       "      <th>Linear</th>\n",
       "      <th>Softplus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16423</td>\n",
       "      <td>0.28238</td>\n",
       "      <td>0.272522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ReLU   Linear  Softplus\n",
       "0  0.16423  0.28238  0.272522"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SHOW ERROR TABLE\n",
    "err_arch = []\n",
    "for m in [model, model3, model5]:\n",
    "    p = np.squeeze(m.predict(X_test))\n",
    "    e = getAvgError(p,y_test)\n",
    "    err_arch.append(e)\n",
    "\n",
    "pd.DataFrame([err_arch], columns=[\"ReLU\", \"Linear\", \"Softplus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn from one example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bot needs to be able to approximate a recursive function that changes from sequence to sequence. Therefore, we need to come up with a network capable of learning and validating itself from one example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive (Fibonacci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 610]\n",
      "  [ 987]]\n",
      "\n",
      " [[ 987]\n",
      "  [1597]]]\n",
      "[[1597]\n",
      " [2584]]\n",
      "4181\n"
     ]
    }
   ],
   "source": [
    "#length of input sequence is 4 numbers, guess the 5th\n",
    "print(X_train[:2])\n",
    "print(X_train[2])\n",
    "print(y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[34]\n",
      "  [55]]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x13e8933a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction: 91.160446\n",
      "Actual: 89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Recursion Test')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple LSTM - softplus Activation\n",
    "fib_model = Sequential()\n",
    "fib_model.add(LSTM(50, activation='softplus', input_shape=(fib_look, 1)))\n",
    "fib_model.add(Dense(1))\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "fib_model.compile(optimizer=adam, loss='mse',metrics=['mse'])\n",
    "fib_history = fib_model.fit([X_train[:2]], [y_train[:2]], epochs=500, verbose=0)\n",
    "\n",
    "#predict on unseen\n",
    "xt = np.expand_dims(X_train[3],0)\n",
    "print(xt)\n",
    "print(\"Prediction: \" + str(np.squeeze(fib_model.predict(xt))))\n",
    "print(\"Actual: \" + str(y_train[3]))\n",
    "\n",
    "#plot error\n",
    "pyplot.plot(fib_history.history['mse'])\n",
    "pyplot.xlabel(\"epochs\")\n",
    "pyplot.ylabel(\"MSE\")\n",
    "pyplot.title(\"Recursion Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataset (input => index of number, output => value)\n",
    "# function: A_n = (2n-1)*2n\n",
    "ind_train = [[[1]],[[2]],[[3]]]     \n",
    "ans_train = [[[2]],[[12]],[[30]]]\n",
    "ind_test = [[[4]]]\n",
    "ans_test = [[[56]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4]]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1478320d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction: 53.105938\n",
      "Actual: [[[56]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Index Test')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple LSTM - softplus Activation\n",
    "index_model = Sequential()\n",
    "index_model.add(LSTM(50, activation='softplus', input_shape=(1, 1)))\n",
    "index_model.add(Dense(1))\n",
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "index_model.compile(optimizer=adam, loss='mae',metrics=['mae'])\n",
    "index_history = index_model.fit(ind_train, ans_train, epochs=500, validation_split=0.0, verbose=0)\n",
    "\n",
    "#predict on unseen\n",
    "print(ind_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(index_model.predict(ind_test))))\n",
    "print(\"Actual: \" + str(ans_test))\n",
    "\n",
    "#plot error\n",
    "pyplot.plot(index_history.history['mae'])\n",
    "pyplot.xlabel(\"epochs\")\n",
    "pyplot.ylabel(\"MAE\")\n",
    "pyplot.title(\"Index Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive and index hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2, 1)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "#remember format of fibonacci data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataset (input => [value (n), index of number], output => value (n+1))\n",
    "# function: A_{n+1} = 2^{5-n} - A_n\n",
    "n_train = [[[7],[1]],[[9],[2]],[[-1],[3]]]     \n",
    "nplus_train = [[[9]],[[-1]],[[5]]]\n",
    "n_test = [[[5],[4]]]\n",
    "nplus_test = [[[-3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5], [4]]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1450e6f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction: -2.0652003\n",
      "Actual: [[[-3]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Hybrid Test')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simple LSTM - softplus Activation\n",
    "hybrid_model = Sequential()\n",
    "hybrid_model.add(LSTM(50, activation='softplus', input_shape=(2, 1)))\n",
    "hybrid_model.add(Dense(1))\n",
    "adam = keras.optimizers.Adam(lr=0.1)\n",
    "hybrid_model.compile(optimizer=adam, loss='mae',metrics=['mae'])\n",
    "hybrid_history = hybrid_model.fit(n_train, nplus_train, epochs=1000, validation_split=0.0, verbose=0)\n",
    "\n",
    "#predict on unseen\n",
    "print(n_test)\n",
    "print(\"Prediction: \" + str(np.squeeze(hybrid_model.predict(n_test))))\n",
    "print(\"Actual: \" + str(nplus_test))\n",
    "\n",
    "#plot error\n",
    "pyplot.plot(hybrid_history.history['mae'])\n",
    "pyplot.xlabel(\"epochs\")\n",
    "pyplot.ylabel(\"MAE\")\n",
    "pyplot.title(\"Hybrid Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgError(p, a):\n",
    "    z = zip(p,a)\n",
    "    e = []\n",
    "    for i, j in z:\n",
    "        e.append(abs(i-j))\n",
    "    return np.average(np.array(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECURSION: 1/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14b0071f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RECURSION: 2/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14b92d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RECURSION: 3/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x149018ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RECURSION: 4/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x147757ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RECURSION: 5/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x144f54ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RECURSION: 6/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14b48d820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RECURSION: 7/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x148fe29d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RECURSION: 8/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1493a2f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RECURSION: 9/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14a960dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RECURSION: 10/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x149d11e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "INDEX: 1/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14c4394c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX: 2/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14ad0b430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INDEX: 3/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14c75b5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INDEX: 4/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14c9a0af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INDEX: 5/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x149339b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INDEX: 6/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x13f2c4670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INDEX: 7/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14b9d7430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INDEX: 8/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x149baa0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INDEX: 9/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14a882280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INDEX: 10/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x148fe2ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "HYBRID: 1/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14b60c8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "HYBRID: 2/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14bbda280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYBRID: 3/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14b92d8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "HYBRID: 4/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14a68ff70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "HYBRID: 5/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14be60310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "HYBRID: 6/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1490180d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "HYBRID: 7/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14b3c00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "HYBRID: 8/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1486de310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "HYBRID: 9/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14ad0b5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "HYBRID: 10/10\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14bbda5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# REPEATED TRIALS OF THE THREE MODELS\n",
    "trials = 10\n",
    "\n",
    "#recursion model\n",
    "rec_model_acc = []\n",
    "for i in range(trials):\n",
    "    print(\"RECURSION: \" + str(i+1) + \"/\" + str(trials))\n",
    "    # Simple LSTM - softplus Activation - recursion data\n",
    "    fib_model = Sequential()\n",
    "    fib_model.add(LSTM(50, activation='softplus', input_shape=(fib_look, 1)))\n",
    "    fib_model.add(Dense(1))\n",
    "    adam = keras.optimizers.Adam(lr=0.01)\n",
    "    fib_model.compile(optimizer=adam, loss='mse',metrics=['mse'])\n",
    "    fib_history = fib_model.fit([X_train[:2]], [y_train[:2]], epochs=500, verbose=0)\n",
    "\n",
    "    #predict on unseen\n",
    "    xt = np.expand_dims(X_train[3],0)\n",
    "    p = fib_model.predict(xt)\n",
    "    a = y_train[3]\n",
    "    rec_model_acc.append(getAvgError([p],[a]))\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "#index model\n",
    "ind_model_acc = []\n",
    "for i in range(trials):\n",
    "    print(\"INDEX: \" + str(i+1) + \"/\" + str(trials))\n",
    "    # Simple LSTM - softplus Activation - index data\n",
    "    index_model = Sequential()\n",
    "    index_model.add(LSTM(50, activation='softplus', input_shape=(1, 1)))\n",
    "    index_model.add(Dense(1))\n",
    "    adam = keras.optimizers.Adam(lr=0.01)\n",
    "    index_model.compile(optimizer=adam, loss='mae',metrics=['mae'])\n",
    "    index_history = index_model.fit(ind_train, ans_train, epochs=500, validation_split=0.0, verbose=0)\n",
    "\n",
    "    #predict on unseen\n",
    "    p = index_model.predict(ind_test)\n",
    "    a = ans_test\n",
    "    ind_model_acc.append(getAvgError([p],[a]))\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "#hybrid model\n",
    "hyb_model_acc = []\n",
    "for i in range(trials):\n",
    "    print(\"HYBRID: \" + str(i+1) + \"/\" + str(trials))\n",
    "        \n",
    "    # simple LSTM - softplus Activation - hybrid data\n",
    "    hybrid_model = Sequential()\n",
    "    hybrid_model.add(LSTM(50, activation='softplus', input_shape=(2, 1)))\n",
    "    hybrid_model.add(Dense(1))\n",
    "    adam = keras.optimizers.Adam(lr=0.1)\n",
    "    hybrid_model.compile(optimizer=adam, loss='mae',metrics=['mae'])\n",
    "    hybrid_history = hybrid_model.fit(n_train, nplus_train, epochs=1000, validation_split=0.0, verbose=0)\n",
    "\n",
    "    #predict on unseen\n",
    "    p = hybrid_model.predict(n_test)\n",
    "    a = nplus_test\n",
    "    hyb_model_acc.append(getAvgError([p],[a]))\n",
    "    \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_model_acc = rec_model_acc[:10]\n",
    "ind_model_acc = ind_model_acc[:10]\n",
    "hyb_model_acc = hyb_model_acc[:10]\n",
    "\n",
    "rec_avg = np.average(np.squeeze(rec_model_acc))\n",
    "ind_avg = np.average(np.squeeze(ind_model_acc))\n",
    "hyb_avg = np.average(np.squeeze(hyb_model_acc))\n",
    "\n",
    "rec_model_acc.append(rec_avg)\n",
    "ind_model_acc.append(ind_avg)\n",
    "hyb_model_acc.append(hyb_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     recursion model  index model  hybrid model\n",
      "1           3.055908     3.662125      1.063082\n",
      "2           3.886070     2.273430      0.956628\n",
      "3          11.684509     3.068684      1.996088\n",
      "4          10.034424     2.110504      1.237063\n",
      "5           1.692841     3.506943      1.011627\n",
      "6           3.921051     2.922745      1.504955\n",
      "7           1.496468     2.822330      1.231271\n",
      "8           5.415962     3.196648      1.048980\n",
      "9           5.132637     2.757069      1.202277\n",
      "10          1.385681     2.768295      1.970452\n",
      "avg         4.770555     2.908877      1.322242\n"
     ]
    }
   ],
   "source": [
    "#rec_model_acc = list(map(lambda x: x.value, rec_model_acc))\n",
    "#ind_model_acc = list(map(lambda x: x.value, ind_model_acc))\n",
    "#hyb_model_acc = list(map(lambda x: x.value, hyb_model_acc))\n",
    "\n",
    "i = list(range(1,11))\n",
    "i.append(\"avg\")\n",
    "\n",
    "trial_dat = pd.DataFrame({\"recursion model\":rec_model_acc, \n",
    "                          \"index model\": ind_model_acc, \n",
    "                          \"hybrid model\": hyb_model_acc},\n",
    "                         index=i)\n",
    "print(trial_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Needs to have better accuracy (-1.8 is too close to -1 (a possible answer choice) rather than -3 (the actual answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
